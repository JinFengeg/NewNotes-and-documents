# Linux内核设计与实现

Linux Kernel Development

---

三刷Linux内核了，本次因为学完ARM体系结构，主要关注点在于处理器工作模式以及内存管理上，所以第三章进程管理、中断和中断处理、下半部、内存管理、虚拟文件系统、进程地址空间、页高速缓存和页回写需要重新看一遍。

---

## 个人感悟

从当初的操作系统学习开始就对操作系统充满了向往，当时好奇实例化的操作系统又是怎样的风采，后来的学习Linux驱动、Linux应用、乃至于LinuxQt都是属于在外围远观Linux内核，这一次从11/25-1/10，给自己留出了45 * 4=180个小时的时间，并没有打算一遍就能把此书看懂，这也将是我面向就业前最后一本连续大段时间看的书了，希望可以借此了解下内核机制，整体上有一个认识，到时候复习操作系统、Linux驱动/应用的时候可以从内核的角度重新梳理下。

本书基于Linux2.6.34内核详细介绍Linux内核系统，覆盖了从核心内核系统的应用到内核设计与实现等各方面的内容。本书主要内容包括：进程调度、进程管理、时间管理和定时器、系统调用接口、内存寻址、内存管理和页缓存、VFS、内核同步以及调试技术。王峰师兄的开题中其实就是写到了这些，但能看出来人家还是比较水的。

同时本书也涵盖了Linux2.6内核中颇具特色的内容，包括CFS调度程序、抢占式内核、块I/O层以及I/O调度程序。本书采用理论与实践相结合的路线，能够带领读者快速走进Linux内核世界，真正开发内核代码。

## 译者序

个人感觉译者是把自身经历情感表达出来了，所以需要学习下。

“不知不觉涉足Linux内核已经十多年了，与其他有志于此的朋友一样，我们也经历了学习-实用-追踪-再学习的过程。也就是说，我们也是从漫无边际到茫然无措，再到初窥门路，转而觉得心有戚戚这一路走下来，其中甘苦，悠然在心

Linux最为人称道的莫过于他的自由精神，所有源代码唾手可得。侯捷先生云：源码在前，了无秘密。是的，但是我们在面对他的时候，为什么却总是因为这种规模和层面所造就的陡峭学习曲线陷入困顿呢，很多朋友就此倒下，纵然Linux世界繁华似锦，纵然内核天空无边广阔，但是眼前的迷雾重重，心中的阴霾又怎能被阳光驱散呢，纵有雄心壮志，拔剑四顾心茫然，脚下的路在何方。

Linux内核入门是不容易的，他之所以难学，在于庞大的规模和复杂的层面。规模一大，就不容易现出本来面目，浑然一体，自然不容易找到着手之处；层面一多，就会让人眼花缭乱，盘根错节，怎能让人提纲指令？

如果有这样一本书，既能提纲指令，为我理顺思绪，指引方向，同时又能照顾小节，阐述细微，帮助我们更好更快地理解STL源码，那该有多好。孟岩先生如此说，虽然针对的是C++，但道出的是研习源码的人们共同的心声。然而Linux源码研究的方法却不太相同。这还是由于规模和层面决定的。比如说，在语言学习中，我们可以采取小步快跑的方法，通过一个个小程序和小尝试，就可以取得渐进的成果，就能从新技术中有所收获。而掌握Linux呢，如果没有对整体的把握，即使你对某个局部的算法、技术或是代码再熟悉，也无法将其融入实用。其实向内核这样的大规模的软件，正是编程技术施展身手的舞台。

那么，我们能不能做点什么，让Linux的内核学习过程更符合程序员的习惯呢

Robert Love（这是本书的原文作者）回答了这个问题，Robert Love是一个狂热的内核爱好者，所以他的想法自然贴近程序员。是的，我们注定要在对所有核心的子系统有了全面认识之后，才能开始自己的实践 ，但却完全可以舍弃细枝末节，将行李压缩到最小，自然可以轻装快走，迅速进入动手阶段。

因此，相对于Daniel.P.Bovet和Marco Cesati的内核巨著《Understanding the Linux Kernel》，它少了五分细节；相对于实践经典《Linux Device Drivers》（这个就是Linux设备驱动），多了五分说理。可以说，本书填补了Linux内核理论和实践之间的鸿沟，真可谓，一桥飞架南北，天堑变通途。

就我们的经验，内核初学者（不是编程初学者）可以从本书着手，对内核各个核心子系统有个整体把握，包括他们提供什么样的服务，为什么要提供这样的服务，又是怎么实现的。而且，本书还包含了Linux内核开发者在开发时需要用到的很多信息，包括调试技术、编程风格、注意事项等。在消化本书的基础上，如果侧重于了解内核，可以进一步研究《Understanding the Linux Kernel》和源代码本身；如果侧重于实际编程，可以研读《Linux Device Drivers》，直接开始动手工作；如果你想有一个轻松的内核学习和实践环节学这本书吧。

Linux内核是一艘永不停息的轮船，它将驶向何方我们并不知晓，但在这些变化的背后，总有一些原理是恒定不变的，总有一些变化是我们想知晓的，比如调度程序的大幅度改进，内核性能的不断提升，本书第三版虽然针对的是较新的2.6.34Linux内核版本，但在旧版本上积累的知识和经验依旧有效，而新增内容将使读者在应对变化了的内核代码时更加从容。

感谢牛涛和武特，他们在第二版和第三版差异的校对中花费了大量精力。感谢素不相识的网友Cheng Renquan，他主动承担了其中一章的修订。还要感谢苏锦秀、黄伟、王泽宇、赵格娟、刘周平、周永飞、曹江峰、陈白虎和孟阿龙，他们参与了后期的校对和查缺补漏。

最后特别感谢我的合作者康华，从十年前一块分析Linux内核代码到今天，他对技术孜孜不倦的追求不但在业界赢得声誉，也使我们在翻译过程中所遇到的技术难点和晦涩句子被一一迎刃而解，感谢合作者张波，他流畅有趣的文笔让本书少了份枯燥，多了份趣味

​                                                                                                                陈莉君   ”

## 序言

随着Linux内核和Linux应用程序越来越成熟，越来越多的系统软件工程师涉足Linux开发和维护领域。他们中有些人纯粹是出于个人爱好，有些人是为Linux公司工作，有些人是为硬件厂商做开发，还有一些是为内部项目工作的。

但是所有人都必须直面一个问题：内核的学习曲线变得越来越长，也越来越陡峭。系统规模不断扩大，复杂程度不断提高。虽然现在的内核开发者对内核的掌握越发炉火纯青，但新手却无法跟上内核发展的步伐，长此以往将出现青黄不接的断层。

我认为这种新老鸿沟已经成为内核质量的一个隐患，而且问题将继续恶化。所以那些真正关心内核的人已经开始致力于扩大内核开发群体。我就是被扩大的群体中的一员。

解决上述问题的一个方法是尽量保证代码简洁：接口定义合理，代码风格一致，“一次做一件事，做到完美”，这也是Linus Torvalds倡导的解决办法。

我提倡的解决办法是对代码慷慨地加上注释，即能够让读者里克尔了解代码开发者意图的文字，（识别意图和实现之间差异的工作称为调试。如果意图和实现没有差距就不需要调试了，如果意图不明确显然调试也难以进行）。

可是，即使有注解，也没办法清楚地展现内核的各个主要子系统的全景，说明他们到底要做什么。那么，这些开发者又该从何下手呢？

由文字材料来说明这些还在起步阶段就该理解的材料，其实是最合适的。

Robert Love的贡献就在于此，有经验的开发者可以通过本书全面了解内核子系统提高的服务，同时还可以了解这些服务是怎么实现的。对于不少人来说，这些知识已经足够了：那些好奇的人，那些应用程序开发者，那些想对内核设计品头论足一番的人，都有足够的谈资了。这句话说的比较深刻了，译者认为应用开发过于肤浅，些微好奇难成大事，我是Linux驱动开发者，但也不知道自己可以走多远。

但学习本书同样可以作为那些有抱负的内核开发者更上一层楼的契机，可以帮他们更改内核代码以达到预定达到目标。我建议有抱负的开发者能够亲身实践；理解内核某部分的捷径就是对他做些修改，这样能为开发者揭示仅仅通过看内核代码无法看到的深层机理。

严肃认真的内核开发者应该加入开发邮件列表，不断的和其他开发者交流。这是内核开发者相互切磋和并肩前进的最好方法。而Robert在书中对内核生活中至关重要的文化和技巧都做了精彩介绍。

请学习和欣赏Robert的书吧，相比你也希望能精益求精，继续探索，成为内核开发社区中的一员，那么首先你要清楚的是：社区欢迎你。我们评价和衡量一个人是根据他所作的贡献，当你投身于Linux时，你要明白：虽然你仅仅贡献了一小份力，但马上就会有数千万上亿人受益。这是我们的欢乐之源，也是我们的责任之本。

## 前言

在我刚开始有把自己的内核开发经验集结成册，撰写一本书的念头时，我其实也觉得有点头绪繁多，，不知道该从何下手。我实在不想落入传统内核书籍的臼槽，照猫画虎地再写这么一本。不错，前人著述备矣，但我终归要写出点与众不同的东西来，我的书该如何定位，这确实让人颇费思量。

后来灵感终于浮现出来，我意识到自己可以从一个全新的视角看待这个主题。开发内核是我的工作，同时也是我的嗜好，内核就是我的挚爱。这些年来，我不断搜集与内核有关的奇闻异事，不断积攒关键的开发诀窍，依靠这些日积月累的材料，我可以写一本关于开发内核该做什么--更重要的是--不该做什么！的书籍。从本质上说，这本书依旧是描述Linux内核是如何设计和实现的，但是写法却另辟溪径，所提供的信息更倾向于实用。通过本书，你就可以做一些内核开发的工作了，并且是使用正确的方法去做。我是一个注重实效的人，因此，这是一本实践的书，它应当有趣、易读且有用。

我希望读者可以从这本书中领略到更多Linux内核的精妙之处，也希望读者敢于从阅读本书和读内核代码开始跨越到开始尝试开发可用、可靠且清晰的内核代码。当然如果你仅仅是兴致所至，读书自娱，那也希望你能从中找到乐趣。

从第一版到现在，又过了一段时间，我们再次回到本书，修补遗憾。本版比第一版和第二版内容更丰富：修订补充并增加了新的内容和章节，使其更加完善。本版融合了第二版以来内核的各种变化。更值得一提的是，Linux内核联盟做出决定，近期内不进行2.7版内核的开发，于是，内核开发者打算继续开发并稳定的2.6版本。这个决定意味深长，而本书从中的最大受益就是在2.6版上可以稳定相当长时间。随着内核的成熟，内核快照才有机会能维持得更久远一些。本书可以作为内核开发的规范文档，既认识内核的过去，也着眼于内核的未来。

### 使用这本书

开发Linux内核不需要天赋异禀，不需要什么魔法，连Unix开发者普遍长着络腮胡子都不一定有。内核虽然有一些有趣并且独特的规则和要求，但是它和其他大型软件项目相比，并没有太大差别。像所有的大型软件开发一样，要学的东西确实不少，但是不同之处在于数量上的积累，而非本质上的区别。

认真阅读源码非常有必要，Linux系统代码的开放性其实是弥足珍贵的，不要无动于衷地将它搁置一边，浪费了大好资源。实际上就是读了代码还远远不够，应该钻研并尝试着动手改动一些代码。寻找一个bug然后去修改它，改进你的硬件设备等驱动程序。一下子@我了。增加新功能，即使看起来微不足道，寻找痛痒之处并解决。只有动手写代码才能真正融会贯通。

### 内核版本

本书基于Linux2.6内核系列。它并不涵盖早期的版本，当然也有一些例外。比如，我们会讨论2.4系列内核中的一些子系统是如何实现的，这是因为简单的实现有助于传授知识。特别说明的是，本书介绍的是最新的Linux2.6.34内核版本.尽管内核总在不断更新，任何努力也难以捕获这样一只永不停息的猛兽，但是本书力图适合于新旧内核的开发者和用户。

虽然本书讨论的是2.6.34内核，但我也确保了它同样适用于2.6.32内核。后一个版本往往被各个Linux发行版本奉为企业版内核，所以我们可以在各种产品线上见到其身影。该版本确实已经开发了数年。

### 读者范围

本书是写给那些有志于理解Linux内核的软件开发者的。本书并不逐行逐字地注解内核源代码，也不是指导开发驱动程序或是内核API的参考手册。

本书的初衷是提供足够多的关于Linux内核设计和实现的信息，希望读过本书的程序员能够拥有较为完备的知识，可以真正开始开发内核代码。无论开发内核是为了兴趣还是为了赚钱，我都希望能够带领读者快速走进Linux内核世界。本书不但介绍了理论而且也讨论了具体应用，可以满足不同读者的需要。全书紧紧围绕着理论联系实践，并非一味强调理论或是实践。无论你研究Linux内核的动机是什么，我都希望这本书都能将内核的设计和实现分析清楚，起到抛砖引玉的作用。

因此，本书覆盖了从核心内核系统的应用到内核设计与实现等各方面的内容。我认为这一点很重要，值得花功夫讨论。例如，第八章讨论的是所谓的下半部机制。本章分别讨论了内核下半部机制的设计和实现（核心内核开发者或者学者会感兴趣），随即便介绍了如何使用内核提供的接口实现你自己的下半部（这对设备驱动开发者可能很有用处，确实很有用处，下半部就是使用内核提供接口实现）。其实，我认为上述两部分内容是相得益彰的，虽然核心内核开发者主要关注的问题是内核内部如何工作，但是也应该清楚如何使用接口；同样，如果设备驱动开发者了解了接口背后的实现机制，也会受益匪浅。这句话说明了我们驱动开发者学习内核的必要性。

这好比学习某些库的API函数与研究该库的具体实现。初看，好像应用开发者仅仅需要理解API--沃恩被灌输的思想是，应该想看待黑盒子一样看待接口。另外，库的开发者也只关心库的设计与实现，但是双方都应该花时间互相学习。能够深刻了解操作系统本质的应用程序开发者无疑可以更好的利用它。同样库开发者也绝不应该脱离于此库的应用程序，埋头开发。因此本书既讨论了内核子系统的设计，也讨论了他的用法，希望本书能对核心开发者和应用开发者都有用。

本书假设读者已经掌握了C语言，而且对Linux比较熟悉。如果读者还具有操作系统设计相关的经验和其他计算机科学的概念就更好了。当然，我也会尽可能多的解释这些概念，但如果还是不能理解这些知识的话，看本书最后参考资料中给出的一些关于操作系统设计方面的经典书籍。研究生可以直接使用本书作为教材。

### 作者介绍

略

## 第一章Linux内核简介

第一章将带我们从Unix的历史视角来认识Linux内核与Linux操作系统的前世今生。今天Unix系统已演变成一个具有相似应用程序编程接口API，并且基于相似设计理念的操作系统家族。但他又是一个别具特色的操作系统，从萌芽到现在已经有40余年的历史。若要了解Linux，必须首先认识Unix系统。

### 1.1Unix的历史

Unix虽然已经使用了40年，到现在应该五十多年了吧，但计算机科学家仍然认为它是现存操作系统中最强大和最优秀的系统。从1969年诞生以来，由Dennis Ritchie和Ken Thompson的灵感火花点亮的这个Unix产物已经成为一种传奇，历经了时间的考验依然名声不坠。

Unix是从贝尔实验室的一个失败的多用户操作系统Multics中涅槃而生的。Multics项目被终止后，贝尔实验室计算科学研究中心的人们发现自己处于一个没有交互式操作系统可用的境地。在这种情况下，1969年的夏天，贝尔实验室的程序员们设计了文件系统原型，而这个原型最终发展演化成了Unix。Thompson首先在一台无人问津的PDP-7型机上实现了这个全新的操作系统。1971年，Unix被移植到PDP-11型机中。1973年，整个Unix操作系统用C语言进行了重写，看得出来大型软件工程重写是有很大帮助的，正是这个当时并不太引人注目的举动，给后来Unix系统的广泛移植铺平了道路。第一个在贝尔实验室以外被广泛使用的Unix版本是第六版，称为V6.

许多其他的公司也把Unix移植到新的机型上。伴随着这些移植，开发者们按照自己的方式不断地增强系统的功能，并由此产生了若干变体。1977年，贝尔实验室综合各种变体推出了Unix system III；1983年AT&T推出了SystemV。

由于Unix系统设计简洁并且在发布时提供源代码，所以许多其他组织和团体都对他进行了进一步的开发。加州大学伯克利分校便是其中影响最大的一个。他们推出的变体叫Berkeley software Distributions(BSD)，伯克利的第一个Unix演化版是1977年推出的1BSD系统，他的实现基于贝尔实验室的Unix版本，不但在其上加入了许多修正补丁，而且还集成了不少额外的软件；1978年伯克利继续推出了2BSD系统，其中包含我们如今仍在使用的csh、vi等应用软件，原来我用的vi是1978年伯克利推出的，贝尔实验室是退出历史舞台了吗。而伯克利真正独立开发的Unix系统是于1979年推出的3BSD系统，该系统引入了一系列令人振奋的新特性，支持虚拟内存便是其一大亮点。这个可是导致了内存小也可以运行操作系统。在3BSD以后，伯克利又相继推出了4BSD系列，包括4.0BSD/4.1/4.2/4.3BSD等众多分支。这些Unix演化版实现了任务管理、换页机制、TCP/IP等新的特性。最终伯克利大学在1994年重写了虚拟内存子系统VM，并推出了伯克利Unix系统的最终官方版，即我们熟知的4.4BSD，现在，多亏了BSD的开放性许可。BSD的开发才得以由Darwin/FreeBSD/NetBSD和OpenBSD继续。

20世纪80和90年代，许多工作站和服务器厂商推出了他们自己的Unix，这些Unix大部分是在AT&T这是机构名称或伯克利发行版的基础上加上一些满足他们特定体系结构需要的特性。这其中就包括Digtal的Tru64/HP的HP-UX、IBM的AIX、sequent的DYNIX/ptx、SGI的IRIX和Sun的Solaris和SunOS。

上面这些我都没看过。

由于最初一流的设计和以后多年的创新与逐步提高，Unix系统成为一个强大、健壮和稳定的操作系统。下面的几个特点是使unix强大的根本原因。首先Unix很简洁：不像其他动则提供数千个系统调用并且设计目的不明确的系统，unix仅仅提供几百个系统调用并购且有一个非常明确的设计目的。第二，在Unix中，所有的东西都被当做文件对待，后来的Linux也继承了这个特点。这种抽象使对数据和对设备操作是通过一套相同的系统调用接口来进行的；open()/read()/write()/lseek()/close()。第三，Unix的内核和相关的系统工具软件是用C语言编写而成的，正是这个特点使得Unix在各种硬件体系架构面前都具有令人惊异的移植能力，并且使广大的开发人员很容易就能接受它。第四，Unix的进程创建非常迅速，并且有一个非常独特的fork()系统调用，这个我记得是创建子进程的函数。最后，Unix提供了一套非常简单但又稳定的进程间通信原语，快速简洁的进程创建过程使Unix的程序把目标放在一次执行保质保量地完成任务上，而简单稳定的进程间通信机制又可以保证这些单一目的的简单程序可以方便地组合在一起，去解决现实中变得越来越复杂的任务。正是由于这种策略和机制分离的设计理念，确保了Unix系统具备清晰的层次化结构。

今天，Unix已经发展成为一个支持抢占式多任务、多线程、虚拟内存、换页、动态链接和TCP/IP网络的现代化操作系统。除了最后的TCP/IP之外的理论都学过，Unix的不同变体被应用在大到数百个CPU的集群，小到嵌入式设备的各种系统上。。尽管Unix已经不再备认为是一个实验室项目了，但他仍然伴随着操作系统设计技术的进步而继续成长，人们仍然可以把它作为一个通用的操作系统来使用。

unix的成功归功于其简洁和一流的设计。他能拥有今天的能力和成就应该归功于Dennis Ritchie/Ken Thompson和其他早期设计人员的最初决策，同时也要归功于那些永不妥协于成见，从而赋予Unix无穷活力的设计决择，指的就是开源。

### 1.2追寻Linus足迹：Linux简介

1991年，linus Torvalds为当时新推出的、使用Interl 80386微处理器的计算机开发了一款全新的操作系统，Linux由此诞生。那时，作为芬兰赫尔辛基大学的一名学生的Linus，正为不能随心所欲使用强大而自由的unix系统而苦恼。对于Torvalds而言，使用当时流行的Microsoft的DOS系统，除了玩波斯王子游戏之外别无他用。Linus热衷使用于Minix,一种教学用的廉价Unix，但是，他不能轻易修改和发布该系统的源代码，由于Minix的许可证，也不能对Minix开发者所作的设计轻举妄动，这让他耿耿于怀并由此对作者的设计理念感到失望。

Linus像任何一名生机勃勃的大学生一样决心走出这种困境：开发自己的操作系统。他开始写了一个简单的终端仿真程序，用于连接到本校的大型Unix系统上。他的终端仿真程序经过一学年的研发，不断改进和完善。不久Linus手上就有了虽不成熟但五脏俱全的Unix，1991年年底，他在Internet上发布了早期版本。

从此Linux便启航了，最初的Linux发布很快赢得了众多用户。而实际上，他成功的重要因素是开源。

到现在，Linux早已羽翼丰满，它被广泛移植到Alpha、ARM、PowerPC、SPARC/x86-68等许多其他体系结构上。这里的Alpha并不是正点原子的IMXm6ull阿尔法系列。那个是ARM的Cortex-A7的。如今Linux即被安装在最轻小的消费电子设备上，比如手表，同时也在服务规模最庞大的服务数据中心上，如超级计算机集群。今天的Linux的商业前景也越来越被看好，不管是新成立的linux专业公司Red Hat这个是红帽，和华为联合推出证书的公司还是闻名遐迩的计算巨头IBM，都提供林林总总的解决方案，从嵌入式系统、桌面环境一直到服务器。

Linux是类Unix系统，但它并不是Unix，需要说明的是，尽管Linux借鉴了Unix的许多设计并且实现了Unix的API但Linux没有向其他Unix变种那样直接使用Unix的源代码。必要时可，他的实现可能和其他各种Unix的实现不一样，但他没有抛弃Unix的设计目标并且保证了应用程序编程接口的一致。

Linux是一个非商业化的产品，这是他最让人感兴趣的特征。实际上Linux是一个互联网上的协作开发项目。尽管linus被认为是Linux之父，并且现在依然是一个内核维护者，但开发工作其实是由一个结构松散的工作组协力完成的。事实上，任何人都可以开发内核。和该系统的大部分一样，Linux内核也是自由公开软件。当然，也不是无限自由的，它使用GNU的GPL（General Public License）第二版作为限制条款这里接触过，之前的ad7606官方的驱动源码就写了GPL2.这样做的结果是，你可以自由地获取内核代码并随意修改它，但如果你希望发布你修改过的内核，你也得保证让得到你的内核的人同时享有你曾经享受过的所有权利，包括全部的源代码。

Linux用途广泛，包含的东西也名目繁多。Linux系统的基础是内核、C库、工具集和系统的基本工具，如登录程序和Shell。Linux系统也支持现代的X Windows系统，这样就可以使用完整的图形用户桌面环境，比如GNOME。可以在Linux上使用的商业和自由软件数以千计。在这本书以后的部分，当使用Linux这个词时，说的是Linux，在容易引起混淆的地方，会说明指的到底是整个系统还是内核。

```c
MODULE_LICENSE("GPL v2");//ad7606驱动模块中的许可证
```

### 1.3操作系统和内核简介

由于一些现行商业操作系统日趋庞杂及其设计上的缺陷，操作系统的精确定义并没有一个统一的标准。许多用户把他们在显示器屏幕上看到的东西理所当然地任务就是操作系统。通常，操作系统是指在整个系统中负责完成最基础功能和系统管理的那些部分。这些部分应该包括内核、设备驱动程序、启动引导程序uboot、命令行Shell或者其他种类的用户界面、基本的文本管理工具和系统工具。这些都是必不可少的东西。

本书的主题是内核。用户界面是操作系统的外在表象，内核才是操作系统的内在核心。系统其他部分必须依靠啮合这部分软件提供的服务，像管理硬件设备、分配系统资源等。内核有时候被称为管理者或者操作系统核心。通常一个内核由负责响应中断的中断服务程序，负责管理多个进程从而分享处理器时间的调度程序（就是如何实现并发功能的程序），负责管理进程地址空间的内存管理程序和网络、进程间通信等系统服务程序共同组成。对于提供保护机制的现代系统来说，内核独立于普通应用程序，他一般处于系统态，其实就是内核态，拥有受保护的内存空间和访问硬件设备的所有权限。这种系统态和被保护起来的内存空间，统称为内核空间。相对的，**应用程序在用户空间执行，他们只能看到允许他们使用的部分系统资源，并且只使用某些特定的系统功能，不能直接访问硬件，也不能访问内核划给别人的内存范围**（这里有什么实例么，驱动模块是允许在内核态的应该不算这里的），还有其他的使用限制。**当内核运行的时候，系统以内核态进入内核空间执行。而执行一个普通用户程序时，系统将以用户态进入以用户空间执行**。

在系统中运行的应用程序**通过系统调用来与内核通信**。即使是标准C库函数其实底层也是调用系统调用最终都是为了和内核通信。让内核代其完成各种不同任务，这么说的话其实malloc、new函数（这是关键字、运算符，C++中大于128B让malloc分配，小于就用内存池技术了）最终也是内核代为分配的内存。**一些库调用提供了系统调用不具备的许多功能**，确实比如自动创建一个应用**缓冲区**（正是因为有了缓冲区，库函数比系统调用节约资源），在那些较为复杂的函数中，调用内核的操作通常只是整个工作的一个步骤而已。看起来学习Linux应用真的很有用，比如printf()函数来说，他提供了数据丶缓存和格式化等操作，而调用write函数将数据写道控制台上只不过是其中的一个动作罢了，确实，**printf函数会自动创建应用缓冲区**等到缓冲区满足要求之后会刷新缓冲区将数据刷新到内存缓冲区中，然后依旧是等待满足条件才把数据刷新到硬件上。并且这个操作是默认写入标准输出，write确实只是其中一个行为而已。

也有一些库函数和系统**调用就是一一对应的关系**，比如open()库函数除了调用open()系统调用之外，几乎什么也不做（打开文件会创建文件描述符fd。）。当一个应用程序执行一条系统调用，说内核正在代其执行。如果进一步解释，在这种情况下，**应用程序被称为通过系统调用在内核空间运行**，这句话能理解，而内核被称为运行于**进程上下文**中。（一般说上下文是周围环境，这进程上下文包括系统、用户、寄存器上下文）。这种交互关系--应用程序通过系统调用界面陷入内核--是应用程序完成其工作的基本行为方式。

内核还要负责管理系统的硬件设备。现有的几乎所有体系结构，包括全部Linux支持的体系结构，都提供了中断机制。当硬件设备想和系统通信的时候，首先要发出一个**异步的中断信号去打断处理器的执行**，继而打断内核的执行。这是因为外部IO设备与CPU之间的频率差距过大，只有这种异步通知方式才可以在不影响CPU速度的情况下服务外部IO。

中断通常对应着一个中断号，内核通过这个中断号查找响应的中断服务程序，确实中断号和中断服务函数一起被申请中断的使用。并调用这个程序响应和处理中断。许多操作系统的中断服务程序，包括Linux的，都不在进程上下文中执行，他们在一个与所有进程都无关的、专门的中断上下文中运行。（中断上下文中中断服务程序不会运行在用户空间）。所以需要保存，这样可以继续执行该指令的上下。之所以存在这样一个专门的执行环境，就是为了保证中断服务程序能够在第一时间响应和处理中断请求，然后快速地退出。

这些上下文代表着内核活动的范围。实际上我们可以将每个处理器在任何指定时间点上的活动必然概括为以下三者之一。

![1669551479253](C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1669551479253.png)

这张图很好，表示了应用程序运行于用户空间，通过执行标准库函数或者系统调用接口陷入内核空间，而内核空间中的内核子系统与设备驱动程序之间关系比较复杂，想要直接访问设备驱动程序（这里要是说例子的话连字符设备驱动都不使用，什么框架都不用gpio子系统和pinctrl子系统也不使用，虚实转换自己写的话即使如此也要通过虚拟文件系统sysfs，或者访问设备节点吗，但这些确实不是内核子系统吧，，，），通过内核子系统访问驱动也是可以的。而设备驱动程序完全覆盖硬件。而且设备驱动不完全覆盖硬件，比如spi总线会帮助我们初始化一部分的硬件，严格来说还要考虑总线驱动程序。（下面指的是为什么会有上下文概念的产生，因为操作系统会运行于两种状态，执行系统调用进入内核空间 ，但最后还要回到用户空间继续执行，所以需要保护当前环境），原来在这里已经解释了上下文。

- 运行于用户空间，执行用户进程。
- 运行于内核空间，处于进程上下文，代表某个特定的进程执行。
- 运行于内核空间，处于中断上下文，与任何进程无关，处理某个特定的中断。

以上所列几乎包括所有情况。

### 1.4Linux内核和传统Unix内核的比较

由于所有的Unix内核都同宗同源，并且提供相同的API，现代的Unix内核存在许多设计上的相似之处。unix内核几乎毫无例外的都是一个不可分割的静态可执行库。也就是说，他们必须以巨大、单独的可执行块的形式在一个单独的地址空间中运行。这里的静态指的是编译期间划分了地址空间，不再改变。

Unix内核通常需要硬件系统提供页机制MMU也就是内存管理单元，以管理内存。这种页机制可以**加强对内存空间的保护**，（这里指的是当你想要把虚拟地址转换成物理地址的时候会阻止你，这样就起到对内存空间的保护了），并保证每个进程都可以运行于不同的**虚地址空间上**，（这里指的就是进程地址空间了）（每个进程间的地址不重合，只能通过IPC方式来获取其他进程的数据，这是进程地址空间不共享导致的，并不是虚拟地址的问题）初期的Linux系统也需要MMU支持，但有一些特殊版本并不依赖于此。这无疑是一个简洁的设计，可以使得Linux系统运行在没有MMU的小型嵌入式设备上。明明最开始的时候MMU是设计了逻辑上的存储空间，导致可以运行操作系统。怎么现在成了是因为保护内存空间了呢，也许是不同学科的角度不同吧，本书，将重点关注支持MMU的Linux系统。

#### 单内核与微内核设计之比较

操作系统内核可以分为两大阵营：单内核和微内核（第三阵营是外内核，主要用在科研系统中）。

单内核是两大阵营中一种比较简单的设计，在1980年之前，所有的内核都设计成单内核，所谓**单内核就是把它从整体上作为一个单独的大过程来实现，同时也运行在一个单独的地址空间上。**（内核空间，32位系统最多给进程地址空间分配4GB，1GB是内核空间，所有进程共享的）因此，这样的内核通常以**单个静态二进制文件的形式存放于磁盘中**。（这里指的是内核镜像uImage，这是uboot启动需要的类型，和内核源码无关了，存放在根文件系统中。）所有内核服务都在这样的**一个大内核地址空间上运行**。（内核线程没有物理内存，只有内核栈）内核之间的通信是微不足道的，当成了一个大型的进程，可以自由的访问进程地址范围。因为大家都运行在内核态，并身处同一地址空间：内核可以直接调用函数，这与用户空间应用程序没有什么区别。

这种模式的支持者认为**单模块具有简单和性能高的特点**。大多数Unix系统都设计为单模块。

另一方面，微内核并不作为一个单独的大过程来实现。相反，微内核的功能被划分为多个独立的过程，每个过程叫做一个服务器（怎么在这里突然提到服务器）。理想情况下，只有强烈请求特权服务的服务器才运行在**特权模式下**（ARM的工作模式）

所有的服务器都保持独立并运行在**各自的地址空间**上。因此，就不可能像单模块内核直接调用函数，而是通过**消息传递处理微内核通信**：不能直接把函数、数据拿来直接用了，系统采用了进程间通信IPC机制，因此各个服务器之间通过IPC机制互通消息，互换服务。服务器的各自独立有效地避免了一个服务器的失效祸及另一个服务器。毕竟地址独立，出错了也影响不到其他服务器上。

因为IPC机制的开销多于函数调用，又因为会涉及内核空间与用户空间的上下文切换，，因此，消息传递需要一定的周期，而单内核中简单的函数调用没有这些开销。结果，所有实际应用的基于微内核的系统，都让大部分或者全部服务器位于内核，这样就可以直接调用函数，消除频繁的上下文切换。这不就是把微内核变为了单内核了吗。

WindowsNT内核和Mach是微内核的典型实例。但这两个都不让任何微内核服务器运行在用户空间，这违背了微内核设计的初衷

Linux是一个单内核，举例就是我可以直接在驱动模块中使用内核的函数。也就是说Linux内核运行在单独的内核地址空间上。不过Linux吸取了微内核的精华：其引以为豪的是模块化设计、抢占式内核、支持内核线程以及动态装载内核模块的能力。确实要说的话驱动模块可以编译进内核也可以手动在用户空间加载到内核上。

不仅如此，Linux还避其微内核设计上性能损失的缺陷，让所有事情都运行在内核态，直接调用函数无需消息传递。至今，Linux是模块化的、多线程的以及内核本身可调度的操作系统，实用主义占据上风。

当linus和其他内核开发者设计Linux内核时，他们并没有完全彻底地与Unix诀别。他们充分认识到，不能忽视Unix的底蕴，由于Linux并没有基于某种特定的Unix，linus和他的伙伴们对每个特定的问题都选择了已知最理想的解决方案，在有些时候，也可以创造一些新的方案。Linux内核与传统的Unix系统之间存在一些显著的差异：

- Linux支持**动态加载和卸载内核模块**。尽管Linux内核也是单内核，可是允许在需要的时候动态地卸载和加载部分内核代码。指的就是驱动模块，其实确实难以想象，毕竟单模块内核是一个静态的二进制文件在磁盘中，如何去掉其中一部分呢。
- Linux支持**对称多处理SMP机制**（就是多处理器模式），尽管许多Unix的变体也支持SMP，但传统的Unix并不支持这种机制，没用过。
- Linux**内核可以抢占**。与传统的Unix变体不同，Linux内核具有允许在内核运行的任务优先执行的能力。好像是听过这个，在内核运行的进程优先级高。
- Linux对线程支持的实现比较有意思：内核并**不区分线程和其他的一般进程**。对于内核来说，所有的进程都一样，只不过其中的一些共享资源而已。
- Linux提供**具有设备类的面向对象的设备模型**、热插拔事件，以及用户空间的设备文件系统sysfs，这里太熟悉了，热插拔就是插上设备可以检测到然后自动加载相应的模块，设备模型就是驱动框架，sysfs虚拟文件系统把设备的属性暴漏给用户空间。
- Linux忽略了被认为设计得很拙劣的Unix特性。
- Linux**体现了自由的精髓**，摒弃了一些处于市场宣传目的或者没有普遍意义的特性。

### 1.5Linux内核版本

Linux内核有两种：稳定的和处于开发中的。稳定的内核具有工业级的强度，可以广泛地应用和部署。新推出的稳定内核大部分都只是修正了一些bug或者加入了一些新的设备驱动程序。另一方面处于开发中的内核中许多东西变化的很快。而且由于开发者不断试验新的解决方案，内核常常发生剧烈的变化。

Linux通过一个简单的命名机制来区分稳定的和处于开发中的内核。这种机制使用三个或者四个“.”分隔的数字来代表不同内核版本。**第一个数字是主版本号**，第二个数字是**从版本号**，第三个数字是**修订版本号**，第四个可选的数字为**稳定版本号**。5.4.31，从副版本号可以反映出该内核是一个稳定版本还是一个处于开发中的版本；如果是**偶数**，此内核就是**稳定版**；如果是奇数就是开发版。那么这里的5.4.31就是其中的4就是稳定版。主版本号是5，从版本号是4，修订版本号是31，没有稳定版本号。此时属于稳定版本但是没有稳定版本号，正常吗？

头两个数字在一起描述了内核系列，这是一个5.4版内核系列。

处于开发中的内核一般要经历几个阶段。最开始，内核开发者们开始试验新的特性，这时候出现错误和混乱是在所难免的，经过一段时间，系统渐渐成熟，最终会有一个特性审定的声明。这时候，linus就不再接受新的特性了，而对已有特性所进行的后续工作会继续进行。当linus认为这个新内核确实是趋于稳定后，开始审定代码。这以后，就只允许再向其中加入修改bug的代码了。在经过一个短暂的准备期之后，linus会将这个内核作为一个新的稳定版退出。例如1.3系列的开发版稳定在2.0，而2.5稳定在2.6

在一个特定的系列下，linus会定期发布新内核。每个新内核都是一个新的修订版本。比如2.6内核系列的第一个版本是2.6.0，第二个版本是2.6.1，这些修订版包含了BUG修复、新的驱动和一些新特性。修订版本之间区别很微小。

为了解决版本发布周期变长的副作用，内核开发者们引入了上面提出的稳定版本号。这个稳定版本号包含了关键性bug的修改，并且常会向前移植处于开发版内核的重要修改。这里指的就是2.6.32.8包含了关键性bug的修改，并且还会移植2.6.33的重要修改，保证2.6.32在保证稳定性的同时与时俱进。一般只提前三个数字，最后的稳定版本号没人提。

### 1.6Linux内核开发者社区

当你开始开发内核代码时，你就成为全球内核开发社区的一份子了，这个社区最重要的论坛是linux kernel mailing list常缩写为lkml

### 1.7小结

这是一本关于Linux内核的书：内核的目标，为达到目标所进行的设计以及设计的实现。本书侧重实用，同时在讲述工作原理时会结合理论联系实践。从业内人士的视角来欣赏和理解Linux内核的设计和实现之美。力求以一种有趣的方式引导我走过跌跌撞撞的起步阶段，无论是立志于开发内核代码还是进行驱动开发，甚至只希望能更好地了解Linux操作系统，你都能受益。

当阅读本书时，希望有一台装有Linux的及其，能够看到内核代码，我也许算是这类人吧，只不过需要一份结构图以便对整个经脉有个总体把握。

最重要的是，在其中寻找快乐吧。

### 总结

第一章更多的是普及下Linux与Unix的区别，以及各自的历史。微内核，单内核概念，以及内核版本号的说明。微内核与单内核区别，Linux与Unix的区别

## 第二章从内核出发

在这一章，我们将介绍Linux内核的一些基本常识：从何处获取源码，如何编译他，如何安装新内核。那么，考察下内核程序与用户空间程序的差异，以及内核中所使用的通用编程结构。虽然内核在很多方面有其独特性，但从现在来看他和其他大型软件项目没有多大差距。

### 2.1获取内核源码

登录Linux内核官方网站http://www.kernel.org，可以随时获取当前版本的Linux源代码，可以是完整的压缩形式（使用tar命令创建的一个压缩文件），也可以是增量补丁形式。

除特殊情况下需要Linux源码的旧版本外，一般都希望拥有最新的代码。kernel.org是源码的库存，那些领导潮流的内核开发者所发布的增量补丁也放在这里。

其实这个操作之前做过了。

#### 2.1.1使用Git

在过去的几年中，linus和他领导的内核开发者们开始使用一个新版本的控制系统来管理Linux内核源diamagnetic。linus创造的这个系统成为Git。与CSV这样的传统版本控制系统不同，Git是分布式的，指的是内容保存在世界各地么，他的用法和工作流程对许多开发者来说都很陌生。我强烈建议使用Git来下载和管理linux内核源代码。

可以使用Git来获取最新提交到Linus版本树的一个副本

当下载代码后，可以更新我的分支到Linus的最新分支

#### 2.1.2安装内核源代码

内核压缩以GNUzip(gzip)和bzip2两种形式发布。bzip2是默认和首选形式，因为他在压缩上比gzip更有优势。以bzip2形式发布的Linux内核叫做linux-x.y.z.tar.bz2，这里的x.y.z是内核源码的具体版本。下载了源代码之后，就可以轻而易举地对其解压。如果压缩形式是bzip2就运行：

```
tar xvjf linux-x.y.z.tar.bz2
tar xvzf linux-x.y.z.tar.gz
```

##### 何处安装并触及源码

内核源码一般安装在/usr/src/linux目录下，但请注意，不要把这个源码树用于开发，因为编译标准C库所用的内核版本就链接到这棵树。此外，不要以root身份对内核进行修改，应当是建立自己的主目录，仅以root身份安装新内核。

#### 2.1.3使用补丁

在Linux内核社区中，补丁是通用语。可以以补丁的形式发布对代码的修改，也可以以补丁的形式接收其他人所作的修改。增量补丁可以作为版本转移的桥梁。不再需要下载庞大的内核源码的全部压缩，而仅需给旧版本打上一个增量补丁，不仅解决了带宽，还生了时间，要应用增量补丁，从内部源码树开始，只需运行：

```
patch -p1 < ../patch-x.y.z
```

一般来说，一个给定版本的内核补丁总是打在前一个版本上。

这里直接使用之前项目用的5.4.31版本了

### 2.2内核源码树

内核源码树由很多目录组成，而大多数目录又包含更多的子目录，源码树的根目录以及子目录如下：

当初学的时候uboot、kernel都是一大堆的目录。

| 目录          | 描述                                                         |
| ------------- | ------------------------------------------------------------ |
| arch          | 特定体系结构的源码                                           |
| block         | 块设备I/O层                                                  |
| crypto        | 加密API                                                      |
| Documentation | 内核源码文档                                                 |
| drivers       | 设备驱动程序，其实这里放的都是内核驱动，编译进内核了         |
| firmware      | 使用某些驱动程序而需要的设备固件，我记得USB需要添加          |
| fs            | VFS和各种文件系统                                            |
| include       | 内核头文件，驱动模块包含的头文件在这里                       |
| init          | 内核引导和初始化，比如挂载根文件系统之后，系统引导程序会执行rcS初始化脚本 |
| ipc           | 进程间通信代码                                               |
| kernel        | 像调度程序这样的核心子系统，iio这种驱动模块使用的子系统应该不在这里 |
| lib           | 通用内核函数，这里存放的是函数，complete函数应该在这里       |
| mm            | 内存管理子系统和VM，mm_struct就是物理进程空间，vm_area_struct就是虚拟的进程地址空间。这两个属于系统级上下文中需要保存的内存管理信息。 |
| net           | 网络子系统，这个与我无关                                     |
| samples       | 示例，示范代码                                               |
| scripts       | 编译内核所用的脚本，其实我们自己也写过.sh文件                |
| security      | Linux安全模块                                                |
| sound         | 语音子系统                                                   |
| usr           | 早期用户空间代码                                             |
| tools         | 在Linux开发中有用的工具，之前学习IIO的时候就是tools中有调试工具，确实有，明天调试的时候研究研究 |
| virt          | 虚拟化基础结构                                               |

在源码树根目录中的很多文件值得提及。COPYING文件是内核许可证（GNU GPL v2）。驱动模块都要加上这个许可证，CREDITS是开发了很多内核代码的开发者列表。MAINTAINERS是维护者列表，他们负责维护内核子系统和驱动程序，Makefile是顶层Makefile。

### 2.3编译内核

编译内核很简单。

#### 2.3.1配置内核

因为Linux源码唾手可得，耐久意味着在编译它之前可以配置和定制。，可以把自己的特定功能和驱动程序编译进内核，在编译内核之前，首先你必须配置它。由于内核提供了数不胜数的功能，支持了难以计数的硬件，因而有许多东西需要配置。可以配置的各种选项，以CONFIG_FREATURE形式表示，其前缀为CONFIG，例如对称多处理器SMP的配置选项为CONFIG_SMP。如果设置了该选项，选择SMP启用，我们把驱动模块添加到drivers目录中，但是光如此还不够，还需要添加宏在相应的子目录中，配置选项既可以用来决定那些文件编译进内核，也可以通过预处理命令处理代码，像我们使用的Kconfig图像化配置其实也是修改这里。

内核提供了各种不同的工具来简化内核配置。

```
make config//会逐一遍历所有配置项，要求用户选择yes/no或是module，由于这个过程往往要耗费掉很长时间，所以，除非你的工作是按小时计费的，否则应该多利用基于ncurse库编制的图形界面工具：
make menuconfig//或者使用基于gtk+的图形工具
make gconfig
```

这三种工具将所有配置项分门别类放置，比如按处理器类型和特点。

这些配置项会存放在内核diamagnetic树根目录下的.config文件中。之前都用熟悉了。

#### 2.3.2减少编译的垃圾信息

如果想尽量少地看到垃圾信息，却又不希望错过错误报告和警告信息的话，你可以用以下命令来对输出进行重定向：

```c
make > ../detritus//这是把标准输出的设备文件换成了detritus文件内容会输出到这里
make > /dev/null，把无用的输出信息重定向到用无返回值的黑洞/dev/null，这里的>表示覆盖，/dev/null表示文件黑洞，用于销毁文件
```

#### 2.3.3衍生多个编译作业

make程序能把编译过程拆分成多个并行的作业，有助于极大的加快多处理器系统上的编译过程，也有利于改善处理器的利用率，因为编译大型源代码树也包括I/O等待所花费的时间。

make -jn这里的n要衍生出的作业数。在实际中，每个处理器上一般衍生出一个或两个作业。

#### 2.3.4安装新内核

内核在编译好之后，你还需要安装他，怎么安装就和体系结构以及启动引导工具bootloader，后来演变出uboot，这里需要查阅启动引导工具的说明，按照他的直到将内核映像拷贝到合适的位置，并且按照启动要求安装他，这里其实就是需要编写uboot中的bootcmd，这是会传递给内核的一组命令，然后内核的设备树就可以根据其中的信息匹配内核了。

### 2.4内核开发的特点

相对于用户空间内应用程序的卡法，内核开发有些独特之处。尽管这些差异并不会使开发内核代码的难度超过开发用户代码，但他们依然有很大不同。

这些特点使内核成了一只性格不同的猛兽，一些常用的准则被颠覆了，而又必须建立许多全新的准则。尽管有许多差异包括以下几种：

- 内核编程时**既不能访问C库也不能访问标准的C头文件**。之前交流群中有个老哥很搞笑，问我们为啥不能再驱动模块中return语句修改成exit()。这是标准C库的函数
- 内核编程时**必须使用GNU C。自由软件的C库**
- 内核编程时**缺乏像用户空间那样的内存保护机制**，指的是不能访问内核空间吧，这是由MMU实现的，在内核中不起作用是因为没有虚实转换的过程吗。
- 内核编程时**难以执行浮点运算**，这个驱动模块中也提到过，都是把数据传到用户层进行浮点运算。
- 内核给每个进程**只有一个很小的定长堆栈**（这里指的就是内核栈了）。也就是说每个进程自动创建和手动创建的空间有限。
- 由于内核支持异步中断、抢占和SMP，因此必须时刻注意同步和并发。抢占式内核指的是内核线程运行时被另一个内核线程抢占。
- 要考虑可移植性的重要性，这里指的就是跨平台特性了，这就导致内核中需要大量的宏判定当前平台是什么，有哪些可以执行，跳过哪些过程。

#### 2.4.1无libc库抑或无标准头文件

与用户空间的应用程序不同，**内核不能链接使用标准C函数库**（库文件在根文件系统中），或者其他的那些库也不行。造成这种情况的原因有很多，其中就包括先有鸡还是先有蛋这个悖论确实，内核之前就是uboot，最重要的是速度和大小。对内核来说，完整的C库-哪怕是他的一个子集，都太大且太低效了。

大部分常用的C库函数在内核中得到了实现。我是不是可以理解完整C库不能用，但是把其中一部分函数拿出来放到内核可以。比如操作字符串的函数组就位于lib/string.c文件中。只要包含<linux/string.h>头文件，就可以使用他们。

##### 头文件

本书中提及头文件时，指的是组成内核源代码树的内核头文件，总是提到内核源代码树这个概念，之前的所谓树内模块指的也是这个树吧，其实就是内核源码。**内核源代码文件不可以包含外部头文件**，就像他们不能用外部库一样。

基本的头文件位域内核源代码树顶级目录下的include目录中。例如头文件<linux/inotify.h>对应内核源代码树的include/linux/inotify.h，之前驱动模块中都是用了<linux/>这个类型的头文件，这表示是内核源码头文件。在include/linux/下确实找到了completion.h文件。

**体系结构相关**的头文件集位于内核源代码树的arch/<architecture>/include/asm目录下。如果编译的是x86体系结构，则体系结构相关的头文件就是arch/x86/include/asm。内核代码通过以**asm/为前缀的方式包含这些头文件**。例如<asm/ioctl.h>，确实驱动模块中都是这么添加的头文件类型。这里体系结构比如说中断的头文件。

在所有没有实现的函数中，最著名的就数printf()函数了，内核代码虽然无法调用printf()，但他提供的printk()函数几乎与printf函数相同，printk()函数负责把格式化好的字符串拷贝到内核日志缓冲区上，对内核缓冲区，**等待缓冲区满之后再输出到硬件上**，也就是标准输出设备上 。这样，syslog程序就可以通过读取该缓冲区来获取内核信息了。printk()的用法很想printf()

两者之间的一个显著区别在于，**printk()允许通过指定一个标志来设置优先级**。syslogd会根据这个优先级标志来决定在什么地方显示这条系统消息。我之前写在模块中的printk就打印不出来，修改proc/debug也不好使，**只能在dmesg中看**。下面是一个使用这种优先级标志的例子：

```c
printk(KERN_ERR "this is an error\n");
```

优先级标志是预处理程序定义的一个描述性字符串，在编译时优先级标志就与要打印的消息绑在一起处理。

#### 2.4.2GNU C

像所有自视清高的Unix内核一样，Linux内核是用C语言编写的。让人略感惊讶的是，内核并不完全符合ANSI C标准。实际上，只要有可能，内核开发者总是要用到**gcc提供的许多语言的扩展部分。**（这里的gcc是多种GNU编译器的集合，包含的C编译器既可以编译内核，也可以编译Linux系统上用C语言写的其他代码）。

我不太清楚的是这个GNU是啥 GNU”是“GNU's Not Unix ,GNU是一个操作系统，这里的GNU应该是一种类型了，GNU C表示一种类型的C库。

内核开发者使用的C语言涵盖了ISO **C99标准和GNU C扩展特性**。这其中的种种变化把Linux内核推向了gcc的怀抱，尽管目前出现了一些新的编译器比如Intel C，已经支持足够多的gcc扩展特性，完全可以用来编译Linux内核了，最早支持gcc的版本是3.2，但是推荐使用gcc4.4或者之后的版本。Linux内核用到的ISO C99标准的扩展没有什么特别之处，而且C99作为C语言官方标准的修订本，不可能有大的或者激进的变化，让人感兴趣的，与标准C语言有区别的是GNU C上。就让我们研究下内核代码中所使用到的C语言扩展中让人感兴趣的那部分吧，这些变化使内核代码有别于标准C的项目。

##### 1.内联（inline）函数

这个函数之前就一直不理解，一般在底层代码上能看到。C99和GNU C均支持内联函数。inline这个名称就可以反映出他的工作模式，**函数会在它所调用的位置上展开**。这么做可以**消除函数调用和返回所带来的开销**(指的是栈帧结构)也就是寄存器的存储和恢复。我猜测下，当指令执行到正常函数的时候，是需要跳转到函数所定义的部分，类似于中断来了的时候需要保存当前进程上下文去执行 中断的指令。这样寄存器就需要存储和恢复了。使用内联函数的话，就会将调用的函数与他的定义一起被添加到相应的指令中。类似于将这个函数的定义放到被调用的地方。

由于编译器会把调用函数的代码和函数本身放在一起进行优化。所以也有进一步优化代码的可能。不过这么做的话代码确实会变长，意味着占用更多的内存空间或者指令缓存，（果然涉及到了指令这一层），内核开发者通常会把那些对时间眼球比较高，而本身长度比较短的函数定义成内联函数。确实，省却了跳转的时间。如果一个函数比较大，会被反复调用，且没有特别的时间上限值，不应该变成内联函数。学到了，这本书更像是给我解惑。理论功底学不到啥了。内联就是在字里行间展开函数。

定义一个内联函数的时候，需要使用**static关键字**，用inline限制(static设置静态变量，保存在数据段，只初始化一次，内部链接)（静态函数不存在调用函数时的出栈入栈动作，函数体就是栈帧结构，栈帧结构是存放在一直使用的存储区中）

```c
static inline void wolf(unsigned long tail_size)
```

内联函数必须在使用之前就定义好，否则编译器就没法把这个函数展开。实践中一般在头文件中定义内联函数。由于使用了static限制，**编译时不会为内联函数单独建立一个函数体**。如果一个内联函数仅仅在某个源文件中使用，那么也可以把它定义在该文件开始的地方。这里对于static确实没有深刻理解，只知道限制以后其他文件就不可以调用了。看起来和单独创建一个函数体有关。（函数存放在代码段，数据存放在栈中，函数体指的是什么，栈帧结构吗）

在内核中，为了类型安全和易读性，优先使用内联函数而不是复杂的宏。

##### 2.内联汇编

gcc编译器支持在C函数中嵌入汇编指令。当然，在内核编程的时候，只有知道对应的体系结构，才能使用这个功能。确实毕竟汇编由于直接操作寄存器距离硬件太近了，有ARM汇编这样的，受到体系结构的影响非常大。

通常使用asm()指令嵌入汇编代码。

```c
unsigned int low,high;
asm volatile("rdtsc" : "=a" (low), "=d" (high));
```

Linux的内核混合使用了C语言和汇编语言。在偏近体系结构的底层或者对执行时间要求严格的地方，一般使用的是汇编语言。而内核其他部分的大部分代码是用C语言编写的。

随着rust加入内核之后就是GNU C、汇编语言、rust了。

##### 3.分支声明

对于条件选择语句，gcc内建了一条指令用于优化，在一个条件经常出现，或者该条件很少出现的时候，编译器可以根据这条指令对条件分支选择进行优化。内核把这条指令封装成了宏，比如likely()和unlikely()。这里我有印象，因为条件选择语句需要等待判定浪费时间，与其这样倒不如我先跟着可能性大的哪个分支继续执行，然后回头判定下，如果对就可以直接走了，不对就回头执行另一个，因为概率很大所以宏观上确实减少了运行时间。这个在指南中没有提到。

在想要对某个条件选择语句进行优化之前，一定要搞清楚其中是不是存在这么一个条件，在绝大多数情况下都会成立。这点十分重要：如果判断对了，确实这个条件占压倒性的地位，那么性能会得到提升；如果判断错了，性能反而会下降。unlikely在内核中会得到更广泛的使用，因为if语句往往判断一种特殊情况。

#### 2.4.3没有内存保护机制

如果一个用户程序试图进行一次非法的内存访问，**内核就会发现这个错误，发送SIGSEGV信号，并结束整个进程。**（这里其实也属于IPC的一种，管道(pipe和fifo有名管道)/信号量/信号、共享内存、套接字、消息队列）、（内核与用户空间交互：netlink/proc/sysfs+kobject/信号/文件、mmap共享内存、API）然而，如果是内核自己非法访问了内存，后果就很难控制了，毕竟没有人为内核负责。内核中发生的内存错误会导致oops，这是内核中出现的最常见的一类错误。在内核中，不应该去做访问非法的内存地址，引用空指针之类的事情，可能会死掉但不跟你说。在内核里，风险常常会比外面大一些，我之前做驱动模块的实验就会出现运行驱动之后直接死掉重启了。就是因为哪里引用了空指针。(内核空间和用户空间的区分是为了保护内核，用户空间无法访问内核空间从而保护，用户空间函数调用频繁)

#### 2.4.4不要轻易在内核中使用浮点数

在用户空间的进程内进行浮点操作的时候，内核会完成从整数操作到浮点数操作的模式转换这里的模式转换。在执行浮点指令的时候到底会做什么？这确实是一个问题，因为体系结构不同，内核的选择不同，但是内核通常给捕获陷阱并着手于整数到浮点方式的转变。这句话没理解。是不是说在用户空间的进程内进行浮点操作其实下方到内核里就变成了整数了，在内核中进行整数的计算然后返回给用户空间的时候再变成浮点数。

与用户空间进程不同，**内核并不能完美地支持浮点操作**，在内核中使用浮点数时，除了要**人工保存和恢复浮点寄存器**，还有其他一些琐碎的事情要做。所以不要再内核中使用浮点操作。(感觉像是ARM的工作模式中并不支持在内核执行浮点操作，而是支持用户空间执行浮点操作)

#### 2.4.5容积小而固定的栈

用户空间的程序可以从栈上分配大量的空间来存放变量，甚至巨大的结构体或者是包含数以千计得数据项的数组都没有问题，之所以可以这么做是因为用户空间的栈本身比较大，而且还能动态地增长（指的是堆栈，堆向上增长、栈向下增长）。DOS操作系统即使在用户空间也只有固定大小的栈。

内核栈的准确大小随体系结构而变。在x86上，栈的大小在编译内核时配置，**32位机的内核栈是8KB**，而64位机是14KB。这是固定不变的。

#### 2.4.6同步和并发

内核很容易产生竞争条件，和单线程的用户空间程序不同，内核的许多特性都要求能够并发地访问共享数据，这就要求有**同步机制以保证不出现竞争条件**，比如IIO子系统中，有可能出现触发缓冲访问和sysfs目录项访问，这是就需要用到complete函数了。

针对以下：

- Linux是抢占多任务操作系统。内核的进程调度程序对进程进行调度和重新调度。内核必须和这些任务同步。
- Linux内核支持对称多处理器系统SMP。所以，如果没有适当的保护，同时在两个或两个以上的处理器上执行的内核代码很可能会同步访问共享的同一个资源。
- 中断是异步到来的，完全不顾及当前正在执行的代码。也就是说如果不加以适当的保护，中断完全有可能在代码访问资源的时候到来。这就是我之前提到的IIO子系统了。
- Linux内核可以抢占。所以，如果不加以适当的保护，内核中一段正在执行的代码可能会被另外一段代码抢占，从而有可能导致几段代码同时访问相同的资源。

常用的解决竞争的办法是自旋锁和信号量。BLK大内核锁、顺序锁、完全变量、

#### 2.4.7可移植的重要性

尽管用户空间的应用程序不太注意移植问题，然而Linux却是一个可移植的操作系统，并且要一直保持这种特点。也就是说，大部分C代码和体系结构无关，在许多不同体系结构的计算机上都能编译和执行，因此必须把与体系结构相关的代码从内核代码树的特定目录中适当地分离出来。

诸如保持字节序这里指的是BE和LE，还有**CPU字节序**，(判断大小端其实利用了联合体成员从低地址、指针截断保留低地址)在iio通道的scan_type结构体中涉及到了这个，大小端问题。64位对其、不假定字长和页面长度等一系列准则都有助于移植性。

### 2.5小结

内核有独一无二的特质。实施自己的规则和奖赏措施，拥有整个系统的最高管理权。当然，Linux内核的复杂性和高门槛与其他大型软件项目并无差异。在内核开发之路上最重要的步骤是意识到内核并没有那么可怕。

本章和以前的章节为贯穿本书剩余章节所讨论的主题奠定了基础。在后续的每一章中，都会涵盖内核的一个具体概念或子系统。在探索的征途中，最重要的是阅读和修改内核源代码，只有通过实际的阅读和实践才会理解内核。

### 总结

本章其实总结了下内核开发与用户空间开发的区别。GNU C+汇编语言的语言环境，只支持大部分的GNU C库函数，不支持标准C库。头文件只能是<linux/>和<asm/>这两种，都是内核源码的头文件，后者是体系结构相关的头文件。以及同步问题，不能轻易使用浮点数，没有内存保护机制、可移植性问题，这些在我之前写驱动代码时都遇到并考虑过的问题了，看起来还不算陌生。不止一次的体会到Linux驱动/应用、计组/操作系统这些认真学习的好处。概念理解上比较轻松。

## 第三章进程管理

本章开始才是正是的内核学习，从学习内核的核心子系统开始。

本章给引入进程的概念，进程是Unix操作系统抽象概念中最基本的一种，其中涉及到进程的定义以及相关的概念。比如线程；然后讨论Linux内核如何管理每个进程：他们在内核中如何被列举，如何创建，最终又如何消亡。我们拥有操作系统就是为了运行用户程序，因此进程管理就是所有操作系统的心脏所在，Linux也不例外。

### 3.1进程

进程就是**处于执行期的程序**（目标码存放在某种存储介质上），其实进程就是**程序的动态形式** ，目标码就是程序存放在磁盘中，只有被调用到内核中运行的时候变成进程，并且会**提供一个PCB进程控制块**用来管理进程。进程并不仅仅局限于一段可执行程序代码（Unix称之为代码段，text section，这个称呼让我想起代码段、数据段这个对进程内部的分类），通常进程还要包含其他资源，像打开的文件，挂起的信号，内核内部数据，处理器状态，一个或多个具有内存映射的内存地址空间以及一个或多个执行线程（thread of execution），当然还包括用来存放全局变量的数据段等。这是从不同角度观察的进程。实际上，进程就是正在执行的程序代码的实时结果。因为它是动态的所以是实时的。随时可能死掉，内核需要有效而又透明地管理所有细节，要时刻知道当前进程的状态以及所掌握的资源。进程是系统分配资源的基本单位。

执行线程，简称线程（thread）（线程是执行调用的单位），是**在进程中活动的对象**。每个线程都拥有一个独立的**程序计数器**、**进程栈**、和**一组进程寄存器**，这里是学应用的时候讲过，每个线程都拥有自己的线程栈、**共享一个进程栈**(线程是共享其他进程地址空间的进程)。**内核调度的对象是线程**，而不是进程。当**只有一个线程就是单线程进程**，所以本质上调动的还是线程没毛病。在传统的Unix系统中，一个进程只包含一个线程，这个线程就是主线程，但是现在的系统中，包含多个线程的多线程程序司空见惯。Linux系统的线程实现非常特别：**它对线程和进程并不特别区分**。对Linux而言，线程只不过是一种特殊的进程罢了。（进程地址空间其实理解的不太好，进程与线程的理解也不太好，之前的ARM架构还没设计到多线程，我理解是用户堆栈里的进程栈内部再次细分为各个线程栈，这样他们有各自的线程栈，也可以访问父进程的进程栈）

在现代操作系统中，进程提供两种虚拟机制：**虚拟处理器**（这里指的是调度器吧，时间片模式）和虚拟内存。虽然实际上可能是许多进程正在分享一个处理器，但虚拟处理器给进程一个假象，让这些进程觉得自己在独享处理器。这个之前学过，就是时间片轮询和逻辑存储系统，而虚拟内存让进程在分配和管理内存时觉得自己拥有整个系统的所有内存资源。线程之间可以共享虚拟内存，就是共享逻辑地址，但每个都拥有各自的虚拟处理器。这里指的是同一进程中的线程，多线程实际上指的是在同一进程下的概念，**线程号都不具有唯一性只有在同一进程下有效**。

程序本身并不是进程，进程是处于执行期的程序以及相关的资源的总称。实际上，完全可能存在两个或多个不同的进程执行的同一个程序，这里指的是子进程吧，子进程共享父进程的文件描述符。

进程在创建它的时刻开始存活。在Linux系统中，这通常是调用fork()系统的结果，该系统调用通过复制一个现有进进程来创建一个全新的进程，子进程继承父进程的绝大部分资源，调用fork()的进程成为父进程，新产生的进程成为子进程。在该调用结束时，在返回点这个相同位置上，父进程恢复执行，子进程开始执行。其实就是执行fork函数返回之后。

fork()系统调用从内核返回两次：**一次返回父进程，另一次返回新产生的子进程**。通常，创建新的进程都是为了立即执行新的、不同的程序，而接着调用exec()这组函数就可以创建新的地址空间，这里提到的exec族其实就是让子进程可以执行不同的函数。将新的程序载入到子进程中。在现代Linux内核中，fork()实际上是由**clone()系统调用**实现的。

最终，程序通过**exit()系统调用退出执行**。这个函数会终结进程并将其占用的资源释放掉。父进程可以通过**wait()系统调用查询子进程是否终结**，如果没终结就阻塞父进程，如果终结了就给其收尸。这使得进程拥有了等待特定进程执行完毕的能力，**进程退出执行后被设置为僵死状态**，直到父进程调用wait()或者waitpid()为止，后者就是可以指定收尸进程。

进程的另一个名字是任务task。Linux内核通常把进程也叫做任务。只不过是从内核观点所看到的进程。

### 3.2进程描述符及任务结构

这里就是应用没有深入解释的地方了，内核把进程的列表存放在叫做**任务队列**（task list）的**双向循环链表**中。链表中的每一项都是类型为task_struct、称为**进程描述符**（进程描述符是任务队列的节点类型）的结构（process descriptor），该结构定义在<linux/sched.h>文件中。进程描述符中包含了一个具体进程的所有信息。这里的进程描述符和进程控制块有啥关系吗。

task_struct相对较大，在32位机器上，大约有1.7KB。但如果考虑到该结构内包含了**内核管理一个进程所需的所有信息**，那么他的大小也算是比较小了。进程描述符中包含的数据能完整地描述一个正在执行的程序：**打开的文件**、进程的地址空间、挂起的信号、进程状态等等。

#### 3.2.1分配进程描述符

Linux通过**slab分配器**分配task_struct结构，这里的slab是个啥，这样能达到对象复用和缓存着色（cache coloring）的目的。在2.6以前的内核中，各个进程的task_struct存放在他们**内核栈的尾端。**这样做是为了让那些像**x86寄存器较少的硬件体系结构只要通过栈指针就能计算出他们的位置，避免浪费额外的寄存器专门记录**。（栈帧结构中esp是栈指针，指向栈顶，ebp帧指针指向栈帧结构的开头，把进程描述符放到栈的最下方确实可以用栈指针找到）

在include/linux/sched.h文件找到了task_struct结构体，整个结构体加一起600+行。

由于现在用slab分配器动态生成task_struct，所以只需要在栈底（对于向下增长的栈来说）或栈顶（对于向上增长的栈来说）这里指的就是在栈新生成的部分创建进程描述符，现在因为是动态生成的所以地址不需要？

只需要创建一个新的结构体struct thread_info，这个结构体对我们找到task_struct结构体有帮助。在task_struct结构体中存在上述结构体，并且由于current_thread_info()的原因，thread_info必须是task_struct结构体的第一个成员。

![1669721816916](C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1669721816916.png)thread_info实际上就是task_struct其中的第一个成员，所谓的thread_info结构体中有一个指向进程描述符的指针，其实就是指向包含了本结构体的结构体地址吧，寄存器较弱的体系结构并不是引入thread_info结构的唯一原因。这个新建的结构体使得在汇编代码中计算其偏移变得非常容易。

每个任务的thread_info结构在他的内核栈的尾端分配，结构体中task域中存放的是指向该任务实际task_struct的指针。这个thread_info结构体在<asm/thread_info.h>中定义，这里从Linux头文件变成了体系结构相关的头文件了，在arch/arm/asm/thread_info.h文件中，描述了thread_info结构体，其中有一个指针struct task_struct * 成员，这个指针正是指向包含了该thread_info的结构体task_struct。很常见的结构体嵌套结构体指针。至于current_thread_info()，这是一个内联型的函数，返回值是thread_info指针，应该是帮助我们寻找进程描述符地址的函数。通过current_thread_info函数得到thread_info结构体，继而找到其中指向task_struct结构体的指针

```c
static inline struct thread_info *current_thread_info(void)
{
	return (struct thread_info *)
		(current_stack_pointer & ~(THREAD_SIZE - 1));//就是这里计算偏移从而获得thread_info地址
}
```

#### 3.2.2进程描述符的存放

内核通过一个唯一的进程标识值（process identification value）或PID来表示每个进程。PID是一个数，标识为pid_t类型，实质上就是一个int类型。为了**与老版本的Unix和Linux兼容**，PID的最大值默认设置为32768（short int短整型的最大值）这是15位二进制的最大值。尽管这个值也可以增加到高达400万（受<linux/threads.h>中定义PID最大值的限制）。内核把每个进程的PID存放在他们各自的进程描述符中。

这个最大值很重要，因为它实际上就是系统中允许同时存在的进程最大数目。尽管32768对于一般的桌面系统足够用了，但是大型服务器可能需要更多进程。这个值越小，转一圈就越快。这里的转一圈指的是什么，本来数值大的进程比数值小的进程迟运行，但这样一来就破坏了这一原则。如果确实需要的话，可以不考虑与老式系统的兼容，由系统管理员通过修改/proc/sys/kernel/pid_max来提高上限。

在内核中，访问任务通常需要获得指向其task_struct的指针，实际上，内核中大部分处理进程的代码都是直接通过task_struct进行的，毕竟task_struct结构体包含了所有描述进程的信息。因此，通过current宏查找当前正在运行进程的进程描述符的速度就很重要了。硬件体系结构不同，该宏的实现也不同，必须针对专门的硬件体系结构处理，这就是为啥thread_info结构体会存放在asm下了，有的硬件体系结构可以拿出一个专门寄存器来存放指向当前进程task_struct的指针，用于加快访问速度，这样不需要计算了直接访问寄存器就可以了。

有的像x86这样的体系结构，寄存器并不富裕，只能够在内核栈的尾端创建thread_info结构体，通过计算偏移间接的找到task_struct结构体。这里的计算偏移就是在current_thread_info返回值处计算的。

在x86系统上，current把栈指针的后13个有效位屏蔽掉，用来计算出thread_info的偏移，该操作是通过current_thread_info()函数来完成的。

最后current再从thread_info的task域中提取并返回task_struct的地址

```c
current_thread_info()->task;
```

对比以下这部分在PowerPC上的实现（IBM基于RISC的现代微处理器），可以发现PPC当前的task_struct是保存在一个寄存器中。所以在PPC上，current宏只需要把r2寄存器中的值返回就行了，与x86不一样，PPC有足够多的寄存器，所以他的实现有这样选择的余地，而访问进程描述符是一个重要的频繁操作，所以PPC的内核开发者觉得完全有必要为此使用一个专门的寄存器。

#### 3.2.2进程状态

进程描述符中的state域描述了进程的当前状态。系统中的每个进程都必然处于五种进程状态中的一种。该域的值必为以下五种状态标志之一：

- TASK_RUNNING(运行)--进程是可执行的；他或者正在执行，或者在运行队列中等待执行。这是进程在用户空间中执行的唯一可能的状态；这种状态也可以应用到内核空间中正在执行的进程。
- TASK_INTERRUPTIBLE(可中断)--进程正在睡眠（被阻塞了），等待某些条件的达成。一旦这些条件达成，内核就会把进程状态设置为运行，处于此状态的进程也会因为接收到信号而提前被唤醒并随时准备投入运行。
- TASK_UNINTERRUPTIBLE(不可中断)--除了就算是接收到信号也不会被唤醒或者投入运行外，这个状态和可打断状态相同。这个状态通常在进程必须在等待时不受干扰或者等待事件很快就会发生时出现。由于处于此状态的任务对信号不做响应，所以用的很少。
- __TASK_TRACED--被其他进程跟踪的进程，例如通过ptrace对调试程序进行跟踪。这个后来学到了，被跟踪的话你死掉的时候时候把SIGCHID发送给跟踪你的进程的
- __TASK_STOPPED(停止)--进程停止执行；进程没有投入运行也不能投入运行。通常这种状态发生在接收到SIGSTOP/SIGTSTP/SIGTTIN/SIGTTOU等信号的时候，所以学习应用篇还是很有帮助的，此外在调试期间接收到任何信号，都会使进程进入这种状态。

学习操作系统的时候，介绍的进程状态就是执行、阻塞、就绪。后来学习应用篇的时候增加了僵尸、孤儿。现在学习内核了，角度不同了，不再是从进程执行状态来划分了，而是从使用进程的角度考虑。是否可以做什么操作的角度来考虑问题。

<img src="C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1669725470759.png" alt="1669725470759" style="zoom:50%;" />

对于不可中断状态的进程，就是那些被标记为D状态但还不能被杀死的原因，因为任务将不响应信号，因此，你不能给他发送SIGKILL信号，即使可以发送，终止这样一个任务也是不明智的，因为他可能是正在执行重要的操作。上图其实不需要解释，本质上和之前学过的差不多，这里实例化了，是从父进程fork()函数创建子进程开始。已经具体到函数层面了。

#### 3.2.4设置当前进程状态

内核经常需要调整某个进程的状态。这时最好使用set_task_state(task,state)函数

```c
set_task_state(task,state);//将任务task的状态设置为state
```

该函数将指定的进程设置为指定的状态。必要的时候，它会设置内存屏障来强制其他处理器做重新排序。这里就听不懂了，一般只有在SMP系统中有必要（对称对处理器系统），否则，等价于：

```c
task->state = state;//这个就是通过current_thread_info()->task->state = state
```

set_current_state(state)和set_task_state(current,state)含义是等同的。这些函数也放在<linux/sched.h>中了。

```c
#define set_current_state(state_value)				\//本质上是一个宏，传入state_value作为新的进程状态
	do {							\
		WARN_ON_ONCE(is_special_task_state(state_value));\
		current->task_state_change = _THIS_IP_;		\
		smp_store_mb(current->state, (state_value));	\//这里可以看到还是用于SMP系统的，将state_value赋值给current->state中
	} while (0)//只运行一次
```

其中的current宏就是之前提到的

```c
#define get_current() (current_thread_info()->task)
#define current get_current()
```

#### 3.2.5进程上下文

可执行程序代码是进程的重要组成部分。这些代码从一个可执行文件载入到进程的地址空间执行。一般程序在用户空间执行，当一个程序执行了系统调用或者触发了某个异常，陷入到了内核空间，**此时称内核代表进程执行并处于进程上下文中**。在此上下文中current宏是有效的。除非在此间隙有更高优先级的进程需要执行并由调度器做出相应调整，否则在内核退出的时候，程序恢复在用户空间会继续执行。其实就是当内核运行由用户空间陷入的进程时表示处于进程上下文。（这里我是这么理解的，首先虚实映射关系没有改变，使用的还是同一个页表，只不过访问从主页表中代表任务的二级页表变成了主页表中代表内核空间的段项，软中断导致处理器从用户模式切换到svc模式，以管理者的身份访问段项是被允许的，从而实现内核代表进程执行。）

系统调用和异常处理程序是对内核明确定义的接口。进程只有通过这些接口才能陷入内核执行--对内核的所有访问都必须通过这些接口，其实**大部分**的标准C库函数都是对系统调用的封装，所以可以认为使用这些库函数都需要进入内核运行。比如生成一个子进程，需要生成对应的task_struct结构体进程描述符，而这些描述符需要存放在硬件或者内核栈中，无论哪一个都需要陷入到内核中实现，不是用户空间能完成的操作了。

#### 3.2.6进程家族树

Unix系统的进程之间存在一个明显的继承关系，在Linux系统中也是如此。所有的进程都是**PID为1的init进程的后代**，应用中也讲到过，终端就是所有在终端下运行的进程的父进程。内核在系统启动的最后阶段启动init进程。该进程读取系统的初始化脚本initscript并执行其他的相关程序，最终完成系统启动的整个过程。

系统中的每个进程必有一个父进程，相应的，每个进程也可以拥有零个或者多个子进程，**拥有同一个父进程的所有进程被称为兄弟**。进程间的关系存放在进程描述符中。**每个task_struct都包含一个指向其父进程task_struct叫做parent的指针**，还**包含一个称为children的子进程链表**。所以，对于当前进程，可以通过以下的代码获得其父进程的进程描述符：

```c
struct task_struct *my_parent = current->parent;//这里获得就是当前进程的父进程
```

同样也可以按以下方式依次访问子进程：

```c
struct task_struct *task;
struct list_head *list;//这个结构体就是链表中的节点了。
list_for_each(list,&current->children){//这是很常见的遍历链表的方式，for_each_set_bit这是驱动模块中的遍历每一位的方式，这里表示的是遍历链表中每一个子进程
    task = list_entry(list,struct task_struct,sibling);//此时task指向当前的某个子进程，sibling表示兄弟进程，类型list_head这是任务队列的类型
}
```

init进程的进程描述符是作为init_task静态分配的。下面的代码可以很好的演示所有进程间的关系：

```c
struct task_struct *task;
for(task = current;task !=&init_task;task = task->parent)
//这其实就是不断地找父进程直到init进程
```

实际上，可以通过这种继承体系从系统的任何一个进程出发查找到任意指定的其他进程。但大多数事件，只需要通过简单的重复方式就可以遍历系统中的所有进程。这非常容易做到，因为任务队列本来就是一个双向的循环链表。对于给定的进程，获取链表中的下一个进程：

```c
list_entry(task->tasks.next,struct task_struct,tasks)//这里的list_entry没讲过，tasks表示任务队列，所以tasks.next表示的是当前进程在任务队列中的下一项。
```

```c
list_entry(task->tasks.prev,struct task_struct,tasks)
```

这两个例程分别通过next_task(task)宏和prev_task(task)宏实现，而实际上，for_each_process(task)宏提供了依次访问整个任务队列的能力。每次访问，任务指针都指向链表中的下一个元素：

```c
struct task_struct *task;
for_each_process(task){
printk("%s[%d]\n",task->comm,task->pid);//comm表示不包括路径的可执行文件的名字，毕竟可执行文件就是一个进程。
}
```

在一个拥有大量进程的系统中通过重复来遍历所有进程的代价是很大的，因此如果没有充足理由别这么做。

### 3.3进程创建

Unix的进程创建很特别。许多其他的操作系统都提供了产生（spawn）进程的机制，首先在新的地址空间里创建进程，读入可执行文件，最后去执行。这是Unix采用了与众不同的实现方式，把上述两步分解到两个单独的函数中执行fork()和exec()。首先，fork()通过拷贝当前进程创建一个子进程。子进程域父进程的区别仅仅在于PID、PPID和某些资源和统计量。exec()函数负责读取可执行文件并将其载入地址空间开始运行。把这两个函数组合起来使用的效果跟其他系统使用的单一函数的效果相似。

exec()函数有很多，通常称为exec族的函数。

#### 3.3.1写时拷贝

传统的fork()系统调用直接把所有的资源（页帧、页表、进程地址空间）复制给新创建的进程。这种实现过于简单并且效率低下，因为他拷贝的数据也许并不共享，更糟糕的是，如果新进程打算立即执行一个新的映像，那么所有的拷贝都将前功尽弃，Linux的fork()使用写时拷贝（copy-on-write）页实现。写时拷贝是一种可以推迟甚至免除拷贝数据的技术，内核此时并不复制整个进程地址空间，而是让父进程和子进程共享一个拷贝。（也就是说此时fork并不会复制页表、特诊，而是CLONE_VM，只有当一方写入的时候才拷贝。相当于clone(CLONE_VM | CLONE_CHILD)）

只有在需要写入的时候，数据才会被复制，从而使各个进程拥有各自的拷贝。也就是说资源的复制只有在需要写入的时候才进行，在此之前，只是以只读方式共享。这种技术使地址空间上的页的拷贝被推迟到实际发生写入的时候才进行。在页根本不会被写入的情况下，比如fork()后立即调用exec()就无需复制了。

fork()的实际开销就是复制父进程的页表以及给子进程创建唯一的进程描述符。（这里就理解了，实际上就是创建了一份主页表下的二级页表，页表其实就是虚实映射关系，让子进程的映射关系同样映射到父进程的页帧上。）在一般情况下，进程创建后都会马上运行一个可执行的文件，这种优化可以避免拷贝大量根本就不会被使用的数据（地址空间里常常包含数十兆的数据）。由于Unix强调进程快速执行的能力，所以这个优化很重要。

#### 3.3.2fork()

Linux通过clone()系统调用实现fork()，这个调用通过一系列的参数标志来指明父、子进程需要共享的资源，fork()、vfork()、和__clone()库函数都根据各自需要的参数标志去调用clone()，然后由clone()去调用do_fork()，所以在内核的文件中只有clone以及do_fork函数，之前还在合计怎么没看到fork函数，，

do_fork完成了创建中的大部分工作，定义在kernel/fork.c文件中。kernel中存放的就是核心子系统用到的函数了。和之前的include和asm不是一个等级的。该函数调用copy_process()函数，然后让进程开始运行。其中copy_process()函数过程如下：

1. 调用dup_task_struct()为新进程创建一个**内核栈**、thread_info结构和task_struct，这些值与当前进程的值相同，那么此时子进程与父进程的描述完全一致。因为使用的是父进程的node创建的子进程的进程描述符

   ```c
   fork()->do_fork()->_do_fork()->copy_process()->dup_task_struct(current,node)//传入指向thread_info的指针以及int类型的node，此函数返回值task_struct指针
       static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
   {
   	struct task_struct *tsk;
   	unsigned long *stack;
       struct vm_struct *stack_vm_area __maybe_unused;
      if (node == NUMA_NO_NODE)
   	node = tsk_fork_get_node(orig);//这里是为了获取即将创建的进程的节点信息，这个node就是任务节点这里获得的是父进程的node然后通过这个node创建子进程的进程描述符，这就是为什么刚开始的时候父进程与子进程的进程描述符相同的原因了，从这个角度来说分配到的线程栈stack也是相同的啊
    tsk = alloc_task_struct_node(node);//用父进程的节点创建子进程的描述符，此时tsk==父进程的tsk
    stack = alloc_thread_stack_node(tsk, node);//这里使用了父进程的内核栈给子进程
           tsk->stack = stack;//这里为进程创建了内核栈，这个内核栈和进程地址空间无关。使用的是父进程的内核栈。并不会因为进程地址空间被删除而消失。进程在内核态中运行时，数据保存到内核栈中。
           stack_vm_area = task_stack_vm_area(tsk);//这里为进程创建了用户进程栈。
   // 接下来会清除一些与内核栈相关的字段，并且将一些task_struct结构体成员设置为零或者初始值。
           err = arch_dup_task_struct(tsk, orig);
           setup_thread_stack(tsk, orig);
   	clear_user_return_notifier(tsk);
   	clear_tsk_need_resched(tsk);
   	set_task_stack_end_magic(tsk);
   	tsk->splice_pipe = NULL;
   	tsk->task_frag.page = NULL;
   	tsk->wake_q.next = NULL;
   ```

2. 检查并确保新创建这个子进程后，当前用户所拥有的进程数目没有超出给它分配的资源的限制。就是检查下pid_max

   ```c
   if (nr_threads >= max_threads)//这个是在copy_process函数中进行的判断
   ```

3. 子进程着手是自己与父进程区别开来，进程描述符内的许多成员都要被清零或设为初始值。那些不是继承而来的进程描述符成员，主要是统计信息。task_struct中的大多数数据依然未被修改。这个上面讲了是在dup_task_struct()函数中执行的

4. 子进程的状态被设置为TASK_UNINTERRUPTIBLE，以保证不会投入运行，此时不可被中断，不接受信号。无论是kill还是run，并没有找到

5. copy_process()调用copy_flags()以更新task_struct的flags成员，表明进程是否拥有超级用户权限的PF_SUPERPRIV标志被清零，表明进程还没有调用exec()函数的PF_FORKNOEXEC标志被设置。就是清除进程的权限

   ```c
   p->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER | PF_IDLE);
   	p->flags |= PF_FORKNOEXEC;//也是在copy_process()中直接配置的，应该是由于内核版本不同，导致的没有使用coopy_flags()来配置flags
   ```

6. 调用alloc_pid()为新进程分配一个有效的PID。

   ```c
   pid = alloc_pid(p->nsproxy->pid_ns_for_children);//这里为新进程创建一个PID
   ```

7. 根据传递给clone()的参数标志，copy_process()拷贝或共享打开的文件、文件系统信息、信号处理函数、进程地址空间和命名空间等。在一般情况下，这些资源会被给定进程的所有线程共享；否则，这些资源对每个线程是不同的。这个操作是在_do_fork函数里面执行的，当cipy_process返回了创建的task_struct *p之后，因为fork()函数最终是调用系统调用clone()来执行的。（这里拷贝的页表项，那还是创建了一个二级页表）

8. 最后，copy_process()做扫尾工作，并返回一个指向子进程的指针。

   最后执行ptrace_event_pid(trace, pid);新创建的子进程被唤醒并投入运行。内核有意选择子进程首先执行，毕竟还没等到fork()返回，在调用的do_fork()->_do_fork()函数阶段就已经执行子进程了。因为一般子进程都会马上调用exec()函数，这样可以避免写时拷贝的额外开销，如果父进程首先执行的话，有可能会开始向地址空间写入。
   
   ----
   
   这个fork函数，只是创建一个task_struct结构体，然后修改结构体成员，最后拷贝一个二级页表给新的进程。
   
   ------

#### 3.3.3vfork()

不能随便看内核源码了，确实有帮助但是页浪费时间 ，上面的流程花了我一个小时！

除了不拷贝父进程的页表项外，vfork()系统调用和fork()功能相同。（连页表项都不拷贝了，那给子进程的就是一个task_struct结构体）子进程作为父进程的一个单独的线程在他的地址空间里运行，父进程被阻塞，直到子进程退出或执行exec()，子进程不能向地址空间写入。这里写的不对吧，子进程要是作为单独的线程运行的话，这不就是一个线程了么，这就是vfork函数的特点了吗，创建子进程之后会导致父进程阻塞。在过去的3BSD时期这是很有意义的，那是并未使用写时拷贝页来实现fork()。也就是说创建 子进程就会直接给他复制绝大多数的内核栈，必然会占用空间。现在由于在执行fork()时引入了写时拷贝页并且明确了子进程先执行，vfork()的好处就仅仅限于不拷贝父进程的页表项了。（如果fork的写时拷贝应用到页表项，而不仅仅是页帧的话，连页表项都不需要复制了，这样的话fcse中的特殊重定位寄存器没有意义了，两个进程使用相同的虚拟地址，相同的主页表项，那么就不需要区分了）其实现在已经没有用了另外由于vfork()语义非常微妙（试想，如果exec()调用失败会发生什么，如果调用失败的话，子进程将作为父进程的一个线程运行了，并且无法对这个子进程写入地址空间），理想情况下，不要调用vfork()，内核也不用实现它，完全可以将vfork()实现成一个普普通通的fork()。在内核Linux2.2以前就是把vfork当成fork实现的。

vfork()系统调用的实现是通过向clone系统调用传递一个特殊标志来进行，这里的系统调用实际上是定义在用户空间的unisted.h文件中，所以fork以及vfork函数在内核里找不到的。他们是传入参数并调用clone函数。

1. 在调用copy_process()时，task_struct的vfork_done成员会被设置为NULL。这个找到了

2. 在执行do_fork()时，如果给定特别标志，则vfork_done会指向一个特定地址。

   ```c
   if (clone_flags & CLONE_VFORK) {
   		p->vfork_done = &vfork;
       /*这是task_struct中的成员，原来这个vfork_done就是completion结构体，这是老朋友了，之前驱动模块中就使用过这是用来阻塞线程的*/
       struct completion		*vfork_done;
   ```

3. 子进程先开始执行后，父进程不是马上恢复执行，而是一直等待，直到子进程通过vfork_done指针向父进程发送信号。直到子进程执行了complete函数才唤醒被阻塞的父进程。

4. 在调用mm_release()时，该函数用于进程退出内存地址空间这里的意思其实就是子进程结束了吧，并且检查vfork_done是否为空，如果不为空，则会向父进程发送信号。不过这里执行mm_release并不在do_fork()函数中。

   ```c
   if (tsk->vfork_done)
   		complete_vfork_done(tsk);//在mm_release函数中检查是否存在，然后唤醒父进程，要注意啊，在子进程退出的时候确实会唤醒父进程，但这里是因为父进程要收尸而被阻塞，所以确实需要唤醒，看起来收尸被阻塞也是因为completion。而另一个操作就是子进程调用exec函数也会唤醒因为创建子进程而被阻塞的父进程，只不过这次唤醒不会使用mm_release函数了
   ```

5. 忽略第四步，那不属于do_fork()函数的，返回do_fork()，父进程醒来并返回。这里是因为执行了exec函数而被唤醒的

如果一切顺利，子进程在新的地址空间里运行而父进程也恢复了在源地址空间的运行 ，这样，开销确实降低了，不过他的实现并不是优良的。

### 3.4线程在Linux中实现

线程机制是现代编程技术中常用的一种抽象概念。该机制提供了在同一程序内共享内存地址空间运行的一组线程。这些线程还可以共享打开的文件和其他资源。线程机制支持并发程序设计技术（concurrent programming），在多处理器系统上，他也能保证真正的并行处理（parallelism)。线程类似于实时操作系统中的任务，采用的是时间片轮询。创建线程的时候需要传入一个函数指针，指向线程的实例化函数，创建完就会执行。如果创建多个就是多个线程包括主线程一起竞争cpu资源，并发执行，从而导致顺序的不确定性。从这个角度来讲子进程其实也是并发执行。

Linux实现线程的机制非常独特。从内核的角度来说，它并没有线程这个概念，之前就提到了，Linux把所有的线程都当做进程实现。内核也没有准备特别的调度算法或是定义特别的数据结构来表征线程。相反，线程仅仅被视为一个与其他进程共享某些资源的进程。每个线程都拥有唯一隶属于自己的task_struct，所以在内核中，看起来就像是一个进程只是与其他一些进程共享某些资源，比如地址空间。这里的地址空间指的就是内核栈了吧void *stack，这里就是将透了，子进程无论如何都不会和父进程共享内核栈，人家在执行copy_process函数的第一时间就和创建自己的内核栈。而这里线程并不会这么做。

上述线程机制的实现与Microsoft Windows或是Sun Solaris等操作系统的实现差异非常大。这些系统都在内核中提供了专门支持线程的机制（这些系统常常把线程称作轻量级进程（lightweight processes））。轻量级进程这种叫法本身就涵盖了Linux在此处与其他系统的差异。在其他的系统中，相较于重量级的进程，线程被抽象成一种耗费较少资源，运行迅速的执行单元。假如我们有一个包含四个线程的进程，在提供专门线程支持的系统中会有一个包含指向四个不同线程的指针的进程描述符，认为线程是进程的一部分。该描述符负责描述像地址空间、打开的文件这样的共享资源。而线程本身再去描述它独占的资源 ，每个线程都有自己的线程栈。相反，Linux仅仅创建四个进程并分配四个普通的task_struct结构。建立这四个进程时指定他们共享某些资源。这就是Linux！（创建四个进程描述符，分别指向各自的线程栈，指向同一个进程地址空间这个就是vm_area_struct）

从内核给这些进程、线程分配资源以及对他们的描述方法上来分析进程线程之间的区别，比从应用层分析直观透彻。还分析了不同系统下的处理方式，应用层因为可移植性的问题，已经没区别了。

#### 3.4.1创建线程

线程的创建和普通进程的创建类似，只不过在调用clone()的时候需要传递一些参数标志来表明需要共享的资源：

```c
clone(CLONE_VM|CLONE_FS|CLONE_FILES|CLONE_SIGHAND,0);//这里指的分别是虚拟地址、FS是什么，打开的文件、信号，线程不拷贝页表项，那么进程地址空间只能是共享了，文件信息和文件系统都共享
```

除了线程栈以及内核栈和进程描述符外都共享同一个进程资源。

对比一下，一个普通的fork()的实现：

```c
clone(CLONE_SIGCHLD,0);//既然拷贝了页表项，就不使用父进程的进程地址空间了。并且有着自己的用户进程栈和内核栈，页帧只读。
```

而vfork()的实现是：

```c
clone(CLONE_VFORK|CLONE_VM|SIGCHLD,0);//因为没有拷贝页表，所以共享父进程的vm，使用父进程的内核栈以及使用父进程的用户进程栈。
```

CLONE_VM就是父子进程共享地址，CLONE_VFORK就是父进程阻塞等待子进程唤醒，CLONE_SIGHAND是父子进程共享信号处理函数以及被阻断的信号，CLONE_FS表示父子进程共享文件系统信息，CLONE_FILES父子进程共享打开的文件。

#### 3.4.2内核线程

内核经常需要在后台执行一些操作，这种任务可以通过内核线程(kernel thread)完成--独立运行在内核空间的标准进程。内核线程和普通的进程间的区别在于内核线程没有独立的地址空间（实际上指向地址空间的mm指针被设置为NULL），因为Linux是单内核，占用一段连续的内存。原来task_struct->struct mm_struct		*mm;这个指向的就是地址空间了啊，内核栈是存放函数调用的局部变量，压栈。进程地址空间是操作系统分配给进程的运行空间，这是虚拟地址空间，内核栈是存在于内核空间的一段连续地址。

```c
mm_release(tsk, mm);//这里实际上释放的就是tsk->mm地址空间指针
```

他们只在内核空间运行，从来不切换到用户空间去，内核进程和普通进程一样，可以被调度也可以被抢占。没有自己的地址空间就是使用整个内核空间。

Linux确实会把一些任务交给内核线程去做，像flush和ksofirqd这些任务就是明显的例子，这里的flush就是刷新内核缓冲区的函数，必然是交给内核线程执行。

在装有Linux系统的机子上运行ps -ef命令，就可以看到内核线程，有很多，这些线程在系统启动时由另外一些内核线程创建。实际上，内核线程也只能由其他内核线程创建。内核是通过从kthreadd内核进程中衍生出所有新的内核线程。在<linux/kthread.h>中申明有接口，于是在现有内核线程中创建一个新的内核线程的方法如下：

```c
struct task_struct *kthread_create(int (*threadfn)(void *data),void *data,const char namefmt[],...);
```

新的任务是由kthread内核进程通过clone()系统调用而创建的，原来系统调用是在内核中的。新的进程将运行threadfn函数，给其传递的参数为data，进程会被命名为namefmt，namefmt接受可变参数列表类似于printf()的格式化参数。新创建的进程处于不可运行状态，如果不通过调用wake_up_process()明确地唤醒它，他不会主动运行。可以通过调用kthread_run()来达到：这是创建进程并让其运行起来。

```c
struct task_struct *kthread_run(int(*threadfn)(void *data),void *data,const char namefmt[],...);
```

其实kthread_run以及kthread_create两个都是宏实现的。后者是简单调用了kthread_create()和wake_up_process();

内核线程**启动**后就一直**运行**直到调用do_exit()**退出**，或者内核的其他部分调用kthread_stop()退出，传递给kthread_stop()的参数为kthread_create()函数返回的是task_struct 结构的地址：

```c
int kthread_stop(struct task_struct *k)
```

之后会详细讨论具体的内核线程。

---

内核线程没有自己的二级页表，只拥有自己的stack内核栈。

----

### 3.5进程终结

当一个进程终结时，内核必须释放它所占用的资源并把这事件告知其父进程。

一般来说，进程的析构是自身引起的。它发生在进程调用exit()系统调用时，既可能显式地调用这个系统调用，也可能隐式地从某个程序的主函数返回（C语言编译器会在main()函数的返回点后面放置exit()代码）。当进程接受到它既不能处理也不能忽略的信号或者异常时，可能被动的终结。其实就是系统默认操作，不管进程是如何终结的，该任务大部分都要靠do_exit()来完成定义在kernel/exit.c来完成，要做以下工作：

1. 将task_struct中的标志成员设置为PF_EXITING，task_struct->flags之前将这个标志添加了超级用户。

   ```c
   exit_signals(tsk);  /* sets PF_EXITING flags此时设置为退出模式了*/
   ```

2. 调用del_timer_sync()删除任一内核定时器。根据返回的结果，确保没有定时器在排队，也没有定时器处理程序在运行。

3. 如果BSD的进程记账功能是开启的，do_exit()调用acct_update_integrals()来输出记账信息。

   ```c
   /* sync mm's RSS info before statistics gathering 在统计数据收集前同步mm的RSS信息 */
   	if (tsk->mm)//如果此时进程地址空间还存在的话收集下同步mm的RSS信息
   		sync_mm_rss(tsk->mm);
   	acct_update_integrals(tsk);
   ```

4. 然后调用exit_mm()函数释放进程占用的mm_struct这个之前就知道了指向的是进程地址空间 ，如果没有别的进程使用它们，这个地址空间没有被共享的话，就彻底释放。（这里因为进程地址空间有可能被线程共享，所以不一定能被释放，比如vfork）

   ```c
   	exit_mm();//释放进程地址空间，之前写错了，VM_area_struct是内存区域，并不是进程地址空间，mm_struct。如果删除它的话二级页表也应该删除。
   ```

5. 接下来调用sem __exit()函数，如果进程排队等候IPC信号，它则离开队列。这里的sem在树莓派代码中看到过，指的是信号

   ```c
   exit_sem(tsk);/*树莓派提到这两个了，第一个sem是信号量，第二个还没学到，这两个适用于进程间通信IPC的，让进程离开他所等待的队列*/
   	exit_shm(tsk);
   ```

6. 调用exit_files()和exit_fs()，以分别递减文件描述符、文件系统数据的引用计数。如果其中某个引用计数的数值降为零，那么就代表没有进程在使用相应的资源，此时可以释放。

   ```c
   exit_files(tsk);
   	exit_fs(tsk);
   ```

7. 接着把存放在task_struct的exit_code成员中的任务退出代码设为由exit()提供的退出代码，或者去完成任何其他由内核机制规定的退出动作。退出代码存放在这里供父进程随时检索。这里的退出代码并不是为了应用在这个do_exit()执行期间，而是为了在wait函数给子进程收尸的时候，查看进程描述符中的退出代码，可以知道是因为什么而退出了，这里就是因为exit()函数而退出的。

   ```c
   void __noreturn do_exit(long code)
   {
   ...
   tsk->exit_code = code;//由do_exit()传入的退出代码复制给task_struct->exit_code中
   }
   ```

8. 调用exit_notify()向父进程发送信号，然后再给结束的进程的子进程重新找养父，养父为线程组中的其他线程或者为init进程，在应用里说的就是init进程。并把进程状态（存放在task_struct结构的exit_state中）设成EXIT_ZOMBIE。这是进程描述符被释放前的操作。这不就是僵尸进程吗！！这个操作是在exit_notify中设置task_struct->exit_state

   ```c
   exit_notify(tsk, group_dead);/*就是这里实现的给子进程找养父，通知父进程*/
   exit_notify(){
     tsk->exit_state = EXIT_ZOMBIE;  
   }
   ```

9. do_exit()调用schedule()切换到新的进程。因为处于EXIT_ZOMBIE状态的进程不会再被调度，所以这是进程所执行的最后一段代码，进程最后执行do_exit()函数，而这个函数最后切换到其他的进程。

   ```c
   if (unlikely(tsk->flags & PF_EXITING)) {/*这里使用了unlikely也就是说内核认为括号中为假的可能性大，内核不认为进程会退出，以至于不执行if而执行else，通过这种方式，编译器在编译过程中，会将可能性更大的代码紧跟着if前的带代码，应该是最后才会执行这个schedule函数*/
   		pr_alert("Fixing recursive fault but reboot is needed!\n");
   		futex_exit_recursive(tsk);
   		set_current_state(TASK_UNINTERRUPTIBLE);
   		schedule();
   	}
   ```

至此与进程相关联的所有资源都被释放掉了，除了进程描述符并没有被释放，因为父进程给子进程收尸会释放掉一些资源，这个进程描述符应该就属于需要由父进程释放的资源吧。进程不可运行也没有地址空间让他运行并处于EXIT_ZOMBIE退出状态。它占用的所有内存就是**内核栈、thread_info结构体、task_struct结构了**。此时进程存在的唯一目的就是向她的父进程提供信息，父进程检索到信息后，或者通知内核那是无关的信息后，由进程所持有的剩余内存被释放，归还给系统使用。和我想得一样，是由父进程收尸的时候把这最后的资源释放，在应用中没有具体提到。（进程地址空间中包含了进程堆栈，mm_struct中的vm_area_struct有一个就是堆栈信息，那么释放了进程地址空间，如何保存堆栈呢，这里提到的是内核栈。一个用户进程拥有内核栈和用户栈两种堆栈，内核栈是task_struct->stack）

#### 3.5.1删除进程描述符

在调用了do_exit()之后，尽管线程已经僵死不能再运行了，但是系统还保留了他的进程描述符。这样做可以让系统有办法在子进程终结后仍能获取他的信息。因此，进程终结时所需的清理工作和进程描述符的删除被分开执行。在父进程获取已终结的子进程的信息后，或者不获取信息，告诉内核它不关注那些信息后，子进程的task_struct结构才被释放。

wait()这是收尸函数，这一族函数都是通过唯一的系统调用wait4()来实现的。他的标准动作是挂起调用他的进程，直到其中一个子进程退出，突然感觉这种方式和poll/select函数很像。此时函数会返回该子进程的PID，此外，调用该函数时提供的指针会包含子函数退出时的退出代码，这个退出代码存放在task_struct->exit_code中。这个在do_exit的时候就已经够设置了

当调用wait函数的时候会调用wait_task_zombie函数，然后在此函数中调用release_task函数

当最终需要释放进程描述符时，release_task()会被调用，用以完成以下工作，这里是父进程为其收尸的时候做的工作：

1. 他调用__ exit_signal()，该函数调用 __unhash_process()，后者又调用 detach _pid()从pidhash上删除该进程，同时也要从任务列表中删除该进程，任务列表就是struct list_head结构的那个链表。

   ```c
   __exit_signal(p);
   	__unhash_process(tsk, group_dead);/*最后从进程号pid的哈希表中删除该进程号，然后从任务列表中删除进程*/
   ```

2. _exit_signal()释放目前僵死进程所使用的所有剩余资源，并进行最终统计与记录

3. 如果这个进程是线程组最后一个进程，并且领头进程已经死掉，好像进程组这个概念我学过，那么release_task()就要通知僵死的领头进程的父进程。

4. release_task()调用put_task_struct()**释放进程内核栈和thread_info结构所占的页，并释放task_struct所占的slab高速缓存**，之前的这个进程描述符就是由slab分配器创建的。

   ```c
   put_task_struct(p);/*此函数是由wait_task_zombie调用的，释放一大推东西*/
   	cgroup_free(tsk);
   	task_numa_free(tsk, true);
   	security_task_free(tsk);
   	exit_creds(tsk);
   	delayacct_tsk_free(tsk);
   	put_signal_struct(tsk->signal);
   
   	if (!profile_handoff_task(tsk))
   		free_task(tsk);
   ```

   至此，进程描述符和所有进程独享的资源就全部释放掉了。

#### 3.5.2孤儿进程造成的进退维谷

如果父进程在子进程之前退出，必须有机制来保证子进程能找到一个新的父亲，否则这些称为孤儿的进程就会在退出时永远处于僵死状态，浪费内存。在do_exit()函数中调用了exit_notify函数，此函数会将本进程设置为僵死进程，也是在此函数中调用该forget_original_parent()函数，忘记原来的父亲，然后在此函数中调用find_new_reaper函数来执行寻父过程。

需要先学习下task_struct->parent和real_parent之间的区别

前者是追踪本进程的进程，当本进程死掉发送SIGCHLD信号给parent，而真实父进程指的是fork本进程的进程。

```c
static struct task_struct *find_new_reaper(struct task_struct *father,
					   struct task_struct *child_reaper)
{
	struct task_struct *thread, *reaper;/*reaper表示托管进程也就是继父进程*/

	thread = find_alive_thread(father);/*找到结束的进程所在线程组的下一个进程*/
	if (thread)
		return thread;/*找到的话直接返回了*/

	if (father->signal->has_child_subreaper) {
		unsigned int ns_level = task_pid(father)->level;/*应该是保证找到的进程都属于死掉进程的进程组*/
		for (reaper = father->real_parent;
		     task_pid(reaper)->level == ns_level;
		     reaper = reaper->real_parent) {/*托管进程从死掉的进程的真实父进程开始查找，只要是死掉进程的进程组就可以，不断地找其真实父进程*/
			if (reaper == &init_task)/*如果发现此时的托管进程等于init进程了直接退出吧找到头了*/
				break;
			if (!reaper->signal->is_child_subreaper)
				continue;
			thread = find_alive_thread(reaper);/*查看托管进程的*/
			if (thread)
				return thread;
		}
	}
	return child_reaper;
}
```

这段代码作用就是一个是找到养父。当找到养父之后给子进程分配养父，也就是认爹环节，这里的找到养父环节没有2.6写的明白，2.6中就是找到init进程或者死掉进程同组进程组中的其他进程， 原结束进程所在进程命名空间中 child_reaper指向的托管进程 这句话不理解。

需要了解下task_struct结构体的另一个成员。

task_struct->ptraced，

```c
list_for_each_entry(p, &father->children, sibling) {
		for_each_thread(p, t) {
			t->real_parent = reaper;/*找到每一个子进程并将子进程的真实父进程设置为继父进程*/
			BUG_ON((!t->ptrace) != (t->parent == father));
			if (likely(!t->ptrace))/*认为t->ptrace是假的概率大，也就是说ptrace并不是跟踪进程*/
				t->parent = t->real_parent;
			if (t->pdeath_signal)
				group_send_sig_info(t->pdeath_signal,
						    SEND_SIG_NOINFO, t,
						    PIDTYPE_TGID);
		}
```

直接看5.4.31的源码看不懂，需要看2.6的源码才可以

```c
static struct task_struct *find_new_reaper(struct task_struct *father)
{
	struct pid_namespace *pid_ns = task_active_pid_ns(father);/*这里还有一个命名空间的概念，真的是理解不了*/
	struct task_struct *thread;
 
	thread = father;
	while_each_thread(father, thread) {
		//依次遍历该结束的进程所在线程组的下一个进程
		if (thread->flags & PF_EXITING)  continue;/*如果找到的进程标志被设置了EXITING就表示该进程死掉了，那就不是我们要找的了*/
/*child_reaper 的作用是在当前线程组如果没有找到养父的话，需要通过托管表示进程结束后，需要这个child_reaper指向的进程对这个结束的进程进行托管，其中的一个目的是对孤儿进程进行回收。若该托管进程是该结束进程本身，就需要重新设置托管进程，就是将该托管进程设置为当前进程的养父进程thread。*/
		if (unlikely(pid_ns->child_reaper == father))/*托管已经意味着当前线程组没有合适的养父了，需要交给孤儿院了*/
        pid_ns->child_reaper = thread;/*直到找到合适的进程也就是孤儿院托管吧*/
		return thread;//在该结束进程所在的线程组中找到符合要求的进程，返回即可
	}
/*
	如果该结束进程所在的线程组中没有其他的进程，
	函数就返回该结束进程所在命名空间的 child_reaper 指向的托管进程
	(前提是该托管进程不是该结束进程本身)
	*/
	if (unlikely(pid_ns->child_reaper == father)) {
		/*
		如果该结束进程所在命名空间的 child_reaper 指向的托管进程就是该结束进程本身，
		而程序运行至此，说明在该线程组中已经找不到符合要求的进程，
		此时，需要将托管进程设置为 init 进程，供函数返回
		*/
		write_unlock_irq(&tasklist_lock);
		if (unlikely(pid_ns == &init_pid_ns))//如果是init进程需要给出error，init进程不能终止
			panic("Attempted to kill init!");
 
		zap_pid_ns_processes(pid_ns);
		write_lock_irq(&tasklist_lock);
		/*
		 * We can not clear ->child_reaper or leave it alone.
		 * There may by stealth EXIT_DEAD tasks on ->children,
		 * forget_original_parent() must move them somewhere.
		 */
		pid_ns->child_reaper = init_pid_ns.child_reaper;//把当前进程空间的托管进程设置为init进程的托管进程
	}
 
	return pid_ns->child_reaper;//返回找到的托管进程也就是养父进程
}
```

总结一下，在 find_new_reaper 中有可能返回以下3种进程作为新父进程：

1、原结束进程所在线程组中的一个符合要求的进程

2、原结束进程所在进程命名空间中 child_reaper指向的托管进程，这是当所在线程组里没有合适的进程的时候。

3、init进程

现在，给子进程找到合适的父进程了，只需要遍历所有子进程并为他们设置新的父进程

```c
reaper = find_new_reaper(father);

	list_for_each_entry_safe(p, n, &father->children, sibling) {
		p->real_parent = reaper;/*设置真实父进程为找到的父进程*/
		if (p->parent == father) {/*如果p进程的追踪进程是死掉的父进程需要为其追踪进程设置为新找到的真实父进程*/
			BUG_ON(task_ptrace(p));
			p->parent = p->real_parent;
		}
		reparent_thread(father, p, &dead_children);
	}
```

这里需要介绍下PID namespace的概念，进程命名空间对进程PID重新标号，不同的命名空间下的进程可以有同一个PID，导致的结果就是父节点可以看到子节点中的进程，可以通过信号对字节点的进程产生影响，而子节点无法看到父节点命名空间下的其他进程，也就是说子节点无法对父进程的兄弟产生影响。所以当一个 进程的父进程退出后，该进程变成孤儿进程，孤儿进程会被当前PIDnamespace中PID为1的进程接管，而不会被最外层的init进程接管。

光找到子进程还不够，还要给那些被死掉进程追踪的子进程进行新的浔父进程。这段代码遍历了两个链表：子进程链表和ptrace子进程链表，给每个子进程设置新的父进程。这两个链表同时存在的意义是**当一个进程被跟踪的时候，他的临时父进程设定为调试进程**。此时如果他的父进程退出了，系统会为他和他的所有兄弟重新寻找一个父进程。在以前的内核中，需要遍历系统所有的进程来找到这些子进程，现在的解决办法是在一个单独的被ptrace跟踪的子进程链表中搜索相关的兄弟进程，用两个相对较小的链表减轻了遍历带来的消耗。

### 3.6小结

在本章，考察了操作系统的核心概念进程。我们讨论了进程与线程之间的关系，然后讨论了Linux如何存放和表示进程的，用task_struct和thread_info，如何创建进程的，使用fork()，vfork就是不会复制父进程的页表一般不使用，本质上就是调用clone函数，如何把新的执行映像装入到地址空间通过exec()系统调用族，如何表示进程的层次关系，父进程如何收集其后代尸体的信息通过wait()系统调用族，以及进程最终如何消亡，强制或自愿地调用exit()，无论是什么退出方式最终都会调用do_exit函数。进程是一个非常基础、非常关键的抽象概念，其实还学了下进程命名空间这是为了让进程只能知道自己相关的进程信息，而不能知道其他无关的进程的信息。进程位于每一种现代操作系统的核心位置。主要是熟悉下task_struct结构体中的成员。

第四章讨论进程调度，内核以微妙而有趣的方式来决定哪个进程执行，何时运行，以何种顺序运行。

---

三刷的感悟

重点放在了对于页表的操作以及堆栈的操作上，fork会拷贝页表项，每个进程都有两种堆栈，进程描述符由slab分配器分配内存，所以不在内核栈上，但是内核栈上有对应的指针。fork子进程并不共享父进程的进程地址空间，所以拥有自己的用户进程栈，也有自己的内核栈。

vfork函数创建的子进程，共享父进程的进程地址空间，不需要拷贝页表项，共享用户进程栈，只有内核栈、进程描述符是自己的。

进程终结的时候会释放进程地址空间，但不会释放内核栈以及task_struct，内核线程的mm_struct = NULL，所以没有用户进程栈，但是拥有内核栈。

用户线程不仅拥有各自的内核栈，毕竟进程描述符不同，虽然指向同一个进程地址空间，但是这个进程地址空间中划分了不同的用户线程栈。并且共享用户进程栈。

如何理解用户进程在内核态下运行。首先用户进程触发软中断SWI指令，处理器模式切换到SVC模式，此时有权限访问内核空间页表，指令开始对内核空间的段项进行访问，局部变量保存到该用户进程的内核栈上。

内核栈是否存在栈溢出，内核栈是由SVC模式访问的，svc模式下的r10是堆栈限制寄存器，栈底在最高位，栈顶如果超过r10依旧会发生栈溢出。

中断的内核栈也是放在内核空间中，但是使用的是中断模式下的中断内核堆栈。

----

## 第四章进程调度

第三章讨论了进程，他在操作系统看来是程序的运行态表现形式。本章讨论进程调度程序，它是确保进程能有效工作的一个内核子系统。

通过第三章的学习，也告诉我们了看内核源码版本不同差距会非常大，所以我们可以看下文件、结构体成员而不要细扣里面的程序实现。

调度程序负责决定将哪个进程投入运行，何时运行以及运行多长的时间。进程调度程序常常简称调度程序。可看作在可运行态进程之间分配有限的处理器时间资源的内核子系统。调度程序是像Linux这样的多任务操作系统的基础。只有通过调度程序的合理调度，系统资源才能最大限度地发挥作用，多进程才会有并发执行的效果。

调度程序没有太复杂的原理。最大限度利用处理器时间的原则是：只要有可以执行的进程，那么就总会有进程正在执行。但是只要系统中可运行的进程数目比处理器的个数多，就注定某一给定时刻会有一些进程不能够执行。这些进程在等待运行。在一组处于可运行状态的进程中选择一个来执行，是调度程序所需完成的基本工资。

### 4.1多任务

多任务操作系统就是能同时并发地交互执行多个进程的操作系统。在单处理器机器上，这会产生多个进程在同时运行的幻觉。在多处理器机器上，这会使多个进程在不同的处理器上真正同时、并行地运行。无论是在单处理器或者多处理器上，多任务操作系统都能使多个进程处于阻塞或者休眠状态，也就是说，实际上不被投入执行，直到工作确实就绪。这些任务尽管位于内存，但并不处于可运行状态。相反，这些进程利用内核阻塞自己，直到某一事件发生，因此现代Linux系统也许有100个进程在内存，但是只有一个处于可运行状态。在内存中也不代表被执行。

多任务系统可以划分为两类：非抢占式多任务和抢占式多任务，像所有Unix的变体和许多其他现代操作系统一样，Linux提供了抢占式的多任务模式。在此模式下，由调度程序来决定什么时候停止一个进程的运行，以便其他进程能够得到执行机会，抢占式内核指的就是进程有优先级。这个强制的挂起动作就叫做抢占。进程在被抢占之前能够运行的时间是预先设置好的，而且有一个专门的名字，叫进程的时间片timeslice。时间片实际上就是分配给每个可运行进程的处理器时间段。有效管理时间片能使调度程序从系统全局角度做出调度时间，这样做还可以避免个别进程独占系统资源。当今众多现代操作系统对程序运行都采用了动态时间片计算的方式，并且引入了可配置的计算策略。

Linux独一无二的公平调度程序本身并没有采取时间片来达到公平调度。

相反，在非抢占式多任务模式下，除非进程自己主动停止运行，否则他会一直运行，进程主动挂起自己的操作称为让步。理想情况下，进程通常做出让步，以便让每个可运行进程享有足够的处理器时间。但这种机制有很多缺点：调度程序无法对每个进程该执行多长时间做出同一规定，所以进程独占的处理器时间可能会很长；更糟糕的是一个绝不做出让步的悬挂进程就能使系统崩溃。Unix一开始就采用的是抢占式的多任务。

### 4.2Linux的进程调度

从1991年Linux的第一版到后来的2.4内核系列，Linux的调度程序都相当简陋，设计近乎原始。当然它很容易理解，但是他在众多可运行进程或者多处理器的环境下都难以胜任。

在Linux2.5开发系列的内核中，调度程序做了大手术，开始采用了一种叫做O(1)时间复杂度，调度程序的新调度程序-它是因为其算法的行为而得名的。解决了先前版本Linux调度程序的许多不足，引入了许多强大的新特性和性能特征。这里主要要感谢静态时间片算法和针对每一处理器的运行队列，他们帮助我们摆脱了先前调度程序设计上的限制。

O(1)调度器虽然在拥有数以十计的多处理器的环境下尚能展现出近乎完美的性能和可扩展性，但是时间证明该调度算法对于调度那些响应时间敏感的程序却有一些先天不足。这些程序称其为交互进程-无疑包括了所有需要用户交互的程序。正因为如此，O(1)调度程序虽然对于大服务器的工作负责很理想，但是在有很多交互程序运行的桌面系统上表现不佳，因为其缺少交互进程。自2.6内核系统开发初期，开发人员为了提高对交互程序的调度性能引入了新的进程调度算法。最为著名的是反转楼梯最后期限调度算法RSDL，该算法吸取了队列理论，将公平调度的概念引入了Linux调度程序。并且最终在2.6.23内核版本中替代了O(1)调度算法，此刻称为完全公平调度算法，简称CFS。

本章将讲解调度程序设计的基础和完全公平调度程序如何运用、如何设计、如何实现以及与他相关的系统调用。也会讲解O(1)调度算法，毕竟是经典Unix调度程序模型的实现方式。

### 4.3策略

策略决定调度程序在何时让什么进程运行。调度器的策略往往就决定系统的整体印象，并且还要负责优化使用处理器时间。无论从哪个方面来看，都是很重要的。

#### 4.3.1I/O消耗性和处理器消耗性的进程

进程可以被分为I/O消耗型和处理器消耗型。前者指进程的大部分时间用来提交IO请求或是等待IO请求。因此，这样的进程经常处于可运行状态，但通常都是运行短短的一会，因为他在等待更多的IO请求时最后总会阻塞。举例说明，多数用户图形界面程序GUI都属于IO密集型。

相反，处理器耗费型进程把时间大多用在执行代码上。除非被抢占，否则他们经常一直不停的运行，因为他们没有太多的IO需求。但是，因为他们不属于IO驱动类型，所以从系统响应速度考虑，调度器不应该经常让他们运行。对于这类处理器消耗型的进程，调度策略往往是尽量降低他们的调度频率，而延长其运行时间。

这种划分方法并非是绝对的。进程可以同时展示这两种行为：比如X Window服务器既是IO消耗型，也是处理器消耗型。典型的是字处理器。通常坐以等待键盘输入，但在任意时刻可能又黏住处理器疯狂 的进行红计算。

调度策略通常要在两个矛盾的目标中间寻找平衡：进程响应迅速和最大系统利用率。为了满足上述需求，调度程序通常采用一套非常复杂的算法来决定最值得运行的进程投入运行，但是它往往并不保证低优先级进程会被公平对待。Unix系统的调度程序更倾向于IO消耗型程序，以提供更好的程序响应速度。就是执行时间短，响应频繁。

Linux为了保证交互式应用和桌面程序的性能，在进程的响应上做了优化也是和Unix一样，缩短响应时间，更倾向于优先调度IO消耗型进程。但调度程序也并未忽略处理器消耗型的进程。

#### 4.3.2进程优先级

调度算法中最基本的一类就是基于优先级的调度。这是一种根据进程的价值和其对处理器时间的需求来对进程分级的想法。通常做法并未被Linux完全的采纳，优先级高的进程先运行，低的后运行，相同优先级的进程按轮转方式进行调度就是时间片轮询。在某些系统中，优先级高的进程使用的时间片也较长。调度程序总是选择时间片未用尽而且优先级最高的进程运行，这里的时间片未用尽是什么意思，用户和系统都可以通过设置进程的优先级来影响系统的调度。

Linux采用了两种不同的优先级范围。第一种是用nice值，他的范围是从-20到+19，默认值为0；越大的nice值意味着更低的优先级-nice意味着你对系统中其他进程更优待。相比高nice值也就是低优先级的进程，高优先级的进程可以获得更多的处理器时间。nice值是所有Unix系统中的标准化的概念，但不同的Unix系统由于调度算法的不同，nice值的运用方式有所差异，Mac OSX，进程的nice值代表分配给进程的时间片的绝对值；而在Linux系统中，nice值代表时间片的比例。你可以通过ps-el命令查看系统中的进程列表，结果中标记N1的一列就是进程对应的nice值。

第二种范围是实时优先级，其值是可配置的，默认情况下他的变化范围是从0到99，与nice值意义相反，越高的实时优先级数值意味着进程优先级越高。任何实时进程的优先级都高于普通的进程，也就是说实时优先级和nice优先级处于互不相交的两个范畴。Linux实时优先级的实现参考了Unix相关标准-特别是POSIX.1b，大部分现代的Unix操作系统也都提供了类似的机制，这里的互不相交是什么意思？就是不会两种一起使用是么。

```c
ps -eo state,uid,pid,ppid,rtprio,time,comm.//这里的state是进程描述符中提到的比如僵尸进程就是其中一种状态EXIT_ZOMBIE，uid是用户id，进程号，父进程号，实时优先级、time、可执行文件名
```

如果由进程对应列显示“-”，则说明他不是实时进程。

其中的state表示进程状态，是task_struct->exit_state成员，里面的状态就是就绪态、运行态、僵尸态、不可中断、可中断、挂起态。Z表示僵尸态。time表示运行起始时间。不i过这里的进程状态与Linux认为的不是一个，Linux眼里的状态只有五个，不可中断、可中断、运行态、停止、被跟踪

#### 4.3.3时间片

时间片是一个数值，它表明进程在被抢占前所能持续运行的时间。调度策略必须规定一个默认的时间片，但这并不是件简单的事。时间片过长会导致系统对交互的响应表现欠佳，让人觉得系统无法并发执行应用程序：时间片太短会明显增大进程切换带来的处理器耗时，因为肯定会有相当一部分系统时间用在进程切换上，而这些进程能够用来运行的时间片却很短。

此外IO消耗型和处理器消耗型的进程之间的矛盾在这里也再次显露出来：IO消耗型不需要长的时间片，而处理器消耗型的进程则希望越长越好，而那些经常使用的进程所在的页会被存入高速缓存器中TAB，这样的话可以让他们在高速缓存命中率更高。

从上面的争论中可以看出，任何长时间片都将导致系统交互表现欠佳。很多操作系统都特别重视这一点，所以默认的时间片很短。但是Linux的CFS调度器完全公平调度算法**并没有直接分配时间片到进程**，它是**将处理器的使用比划分给了进程**。这样一来，进程所获得的处理器时间其实是和系统负载密切相关的。**这个比例进一步还会受进程nice值的影响**，nice值作为权重将调整进程所使用的处理器时间使用比。具有更高nice值的进程将被赋予低权重，**从而丧失一部分的处理器使用比**。这里的使用比为什么和系统负载相关，所谓的使用比指的是你原本默认的时间片的占比么。实际上是先平均分配下处理器使用比，然后根据nice值再次分配下。

像前面所说的，Linux系统是抢占式的，当一个进程进入可运行态，他就被准许投入运行。在多数操作系统中 ，是否要将一个进程立刻投入运行（这势必会抢占当前进程），是完全由进程优先级和是否有时间片决定的，而在Linux中使用新的CFS调度器，其抢占时机取决于新的可运行程序**消耗了多少处理器使用比**。如果消耗的使用比比当前进程小，则立刻投入运行，抢占当前进程。使用比就是使用处理器的占比。这里指的是消耗了多少处理器使用比，我本来给你多少钱，结果你就花费了很少一部分，那我就要在你每次想要花钱的时候最快速度满足你，这样才能让你抓紧花完钱。比较的并不是剩余的使用比，而是你花费的使用比。

#### 4.3.4调度策略的活动

想象下面这样一个系统，拥有两个可运行的进程：一个是文字编辑程序和一个视频编码程序。在这样的场景下，理想情况是调度器应该给予文本编辑程序相比视频编码程序更多的处理器时间，因为它属于交互式应用，对文本编辑器而言，我们有两个目标。第一是我们希望系统给它更多的处理器时间，这并非因为它需要更多的处理器时间，是因为我们希望在它需要时总是能得到处理器，其实就是让他的处理器使用比更低这样就可以更优先的执行了；第二是希望文本编辑器能在其被唤醒时抢占视频解码程序。

在多数操作系统中，上述目标的达成是要依靠系统分配给 文本编辑器比视频解码程序更高的优先级和更多的时间片，这样岂不是导致其处理器使用比更高了吗；先进的操作系统可以自动发现文本编辑器是交互性程序，从而自动地完成上述分配动作。Linux操作系统同样需要追求上述目标，但是采用不同方法。他不再通过给文本编辑器分配给定的优先级和时间片，而是分配一个给定的处理器使用比，假设文本编辑器和视频解码程序是仅有的两个运行进程，并且又具有相同的nice值，那么处理器的使用比将会是50%，两个进程平分了处理器时间。但因为文本编辑器将更多的时间用于等待用户输入，因此他肯定不会用到处理器的50%.这就是所谓的会受到系统负载的影响吧。同时，视频解码程序无疑将能有机会用到超过50%的处理器时间，以便他能更快速地完成解码任务。

关键的问题是，当文本编辑器程序被唤醒时会发生什么。一旦文本编辑器被唤醒，CFS调度器注意到给他的处理器使用比是50%，但是其实它用的少之又少。特别是，CFS发现文本编辑器比视频解码器运行的时间短得多。这种情况下，为了兑现让所有进程能公平分享处理器的承诺，他会立刻抢占视频解码程序，让文本编辑器投入运行。立即处理了用户的击键输入，再次进入睡眠等待用户下一次输入。因为文本编辑器并没有消耗掉承诺给他的50%处理器使用比，因此情况依旧，CFS总是会毫不犹豫让文本编辑器在需要的时候投入运行 。而视频处理程序只能在剩余时间运行。

谁花费的使用比少就让谁优先抢占。所谓的CFS完全公平分配算法，就是考虑到了交互进程，这种花费时间少，还需要快速响应的进程。

### 4.4Linux调度算法

在前面内容中，抽象地讨论了进程调度原理，只是偶尔提及Linux如何把给定的理论应用到实际中。在已有的调度原理基础上，进一步讨论具有Linux特色的进程调度程序。

#### 4.4.1调度器类

Linux调度器是以模块方式提供的，这样做的目的是允许**不同类型的进程可以有针对性地选择调度算法**。模块？

这种模块化结构被称为调度器类（scheduler classes），它允许多种不同的可动态添加的调度算法并存，调度属于自己范畴的进程。每个调度器都有一个优先级，基础的调度器代码定义在kernel/sched.c文件中，他会**按照优先级顺序遍历调度器类**，**拥有一个可执行进程的最高优先级的调度器类胜出**，**也就是说调度器的优先级就是自己手里处于可执行状态的进程的优先级来决定**，谁手里的可执行进程的优先级最高就先执行该调度器类，由该调度器选择下面要执行的那个程序。

完全公平调度（CFS）是一个针对普通进程的调度类，在Linux中称为SCHED_NORMAL，在POSIX（通用API的协议）中称为SCHED_OTHER，CFS算法实现定义在文件kernel/sched_fair.c中。本节下面的内容将重点讨论CFS算法，该内容对于所有2.6.23以后的内核版本意义非凡。

#### 4.4.2Unix系统中的进程调度

在讨论公平调度算法前，首先认识下传统Unix系统的调度过程。正如前面所述，现代进程调度器有两个通用的概念：进程优先级和时间片。时间片是指进程运行多少时间，进程一旦启动就会有一个默认的时间片，具有更高优先级的进程将运行的更频繁，而且也会被赋予更多的时间片。很合理的结构，也很好理解。在Unix系统上 ，优先级以nice值形式输出给用户空间。这点听起来简单，但是在现实中，却会导致许多反常的问题。

第一个问题：若要将nice值映射到时间片，就必然需要将nice单位值对应到处理器的绝对时间。没错根据优先级来提供时间片长度。但这样做将导致进程切换无法最优化进行。举例说明，假定我们将默认nice值0分配给一个进程对应的是一个100ms的时间片；同时再分配一个最高nice值（+20，这也是最低的优先级）给另一个进程，对应的时间片是5ms的处理器时间。

接着假定上述两个进程都处于可运行状态。那么默认优先级的进程将会获得20/21的处理器时间，而低优先级的进程会获得1/21的处理器时间。

如果运行两个同等低优先级的进程情况将如何。我们是希望他们能各自获得一般的处理器时间，而事实上也确实如此。但是任何一个进程每次仅仅只能获得5ms的处理器时间。也就是说，相比刚才例子中105ms内进行两次次上下文切换，现在则需要在10ms内进行两次上下文切换。这就导致**上下文切换次数过于频繁**了。

很显然，可以看到这些时间片的分配方式并不理想：他们是给定nice值到时间片映射与进程运行优先级混合的共同作用结果。这里的nice就是优先级，只不过映射到时间片长度上，并且抢占优先级也是nice。事实上，给定高nice值低优先级的进程往往是后台进程，且多是计算密集型；而普通优先级的进程更多是前台用户任务，优先级低的进程反而需要更多的时间片，而优先级高的进程反而大部分时间用来阻塞了。这就是因为优先级与时间片挂钩导致的。优先级高的往往是交互进程他们反而不需要时间片长。

第二个问题涉及相对nice值，同时和前面的nice值到时间片映射关系也脱不开关系。

假设我们有两个进程，分别具有不同的优先级，假设第一个是nice是0，第二个是1，他们分别映射到时间片100ms和95ms，时间片几乎相同。但是如果进程分别赋予18和19的nice值，他们则分别被映射为10ms和5ms的时间片。如果这样，前者相比后者获得了两倍的处理器时间。**nice值通常使用相对值**，也就是说nice上减少1所带来的效果极大地取决于其nice的初始值。在不同的初始值进行相同的操作，带来的结果不一样

第三个问题，如果执行nice值到时间片的映射，需要能分配一个绝对时间片，之前例子就是5ms，而且这个绝对时间片必须能在内核的测试范围内。在多数操作系统中，上述要求意味着**时间片必须是定时器节拍的整数倍**。而这么做存在几个问题：

1. 首先，最小时间片必然是定时器节拍的整数倍，也就是10ms或者1ms的倍数。
2. 其次，系统定时器限制了两个时间片的差异：连续的nice值映射到时间片，其差别范围为多至10ms或者少则1ms。
3. 最后**时间片还会随着定时器节拍改变**！正是这一点引入了CFS完全公平分配算法。

第四个问题，是关于基于优先级的调度器为了优化交互任务而唤醒相关进程的问题。这种系统中，你可能为了进程能更快地投入运行，而去**对要唤醒的进程提升优先级**，即使他们的时间片已经用尽了。对于那些交互任务赋予更多的特权，让他们可以玩弄调度器，使得给定进程打破公平原则，获得更多处理器时间，损害系统中其他进程的利益。

上述问题中的绝大多数都可以通过对传统Unix调度器进行改造解决。虽然这种改造修改很大，但也并非是结构性调整。

比如，将nice值呈几何增加而非算数增加的方式解决第二个问题，这样的话就不存在18与19之间的区别了，而是可能15与19之间的区别。刻度不再是根据nice值决定而是根据时间片长度决定。变成非线性。

采用一个新的度量机制将从nice值到时间片的映射与定时器节拍分离开，以此解决第三个问题，与定时器节拍无关了。

按时这些解决方案回避了实质问题，就是分配绝对的时间片引发的固定的切换频率，给公平性造成了很大变数。你一次只能运行我给你的时间片长度，想要再次运行就去切换上下文重新获得时间片吧。

CFS采用的方法是对时间片分配方式进行根本性的重新设计：完全摒弃时间片而是分配给进程一个处理器使用比重。通过这种方式，CFS确保了进程调度中能有恒定的公平性，而将切换频率置于不断变动中。采用比重，而不是具体的执行时间。

#### 4.4.3公平调度

CFS的出发点基于一个简单的理念：进程调度的效果应如同系统具备一个理想中的完美多任务处理器。在这种系统中，每个进程将获得1/n的处理器时间，n是指可运行进程的数量。完美的多任务处理器模型应该是这样的：我们能在10ms内同时运行两个进程那么他们各自使用10/n的能力吧，而不是我们各自给他们1ms的时间片，切换五次吧。

CFS实现中首先要确保系统性能不受损失。CFS的做法是允许每个进程运行一段时间、循环转换、选择运行最少的进程（就是消耗使用比最少的进程，虽然每个进程都是1/n的时间，但是总有进程会陷入阻塞而放弃执行）作为下一个运行进程（就是抢占优先级最高的进程），不再采用分配给每个进程时间片的做法了。

CFS在所有可运行进程总数基础上计算出一个进程应该运行多久，而不是依靠nice值来映射时间片。nice值在CFS中被作为进程获得的处理器运行比的权重：刚开始给你们每个进程1/n处理器使用比，但你们每个进程nice优先级不同，所以还要根据nice进行权重分配下，越高的nice值代表着越低的优先级那么它的进程获得更少的处理器使用权重，这是相对默认nice值进程的进程而言的；更低的nice值获得更高的处理器使用权重。

每个进程都按其权重在全部可运行进程中所占比例的“时间片”来运行，为了计算准确的时间片，CFS为完美多任务中的无线小调度周期（希望每个进程只运行一个非常短的周期毕竟越小的调度周期将带来越好的交互性，这里的无限小调度周期指的是所有进程都调度一次的时间）设置一个目标称为目标延迟（目标延迟的由来是因为所有进程被调度一遍的周期是调度周期，当你执行了当前进程之后下一次执行就是一个调度周期之后了，所以目标延迟了一个调度周期），越小的调度周期将带来越好的交互性，同时也更接近完美的多任务。但是你必须承受更高的切换代价和更差的系统总吞吐能力。

当可运行任务数量趋于无限时，他们各自所获得处理器使用比和"时间片"都讲过趋于零。CFS为此引入每个进程获得的时间片底线，这个底线称为最小粒度。默认情况下这个值是1ms，如此一来即便是可运行进程数量趋于无穷，每个进程最少也能获得1ms的运行时间，确保切换消耗被限制在一定范围内了。而这也是CFS算法的折中了。就是当进程数量非常多的时候每个进程还是会执行1ms而不能更小。导致其调度周期过大。

看看具有不同nice值的两个可运行进程的运行情况：

一个默认nice值是0，另一个具有的nice值是5，这些不同的nice值对应不同的权重，注意这里不再是nice映射时间片了，上述两个进程将获得不同的处理器使用比。

nice值是5的进程权重将是默认nice进程的1/3，如果目标延迟是20ms的话也就是说调度周期是20ms，两个进程将分别获得15ms和5ms的处理器时间，再比如两个可运行进程的nice值分别是10和15，他们分配的时间片依旧是15和5，0-5和10-15之间的nice值相差的相同那么他们分配的时间片的倍数相同。之前是nice值对应一个确定的时间片。

可见，**绝对的nice值不再影响调度决策：只有相对值才影响处理器时间的分配比例**。所谓的绝对的nice值指的是10，15这些不再影响调度决策了，而是15-10这个相对值才影响处理器时间的分配比例。

总结下，任何进程所获得的处理器时间是由他自己和其他所有可运行进程nice值的相对差值决定的。nice值对时间片的作用不再是算术加权，而是几何加权。任何nice值对应的绝对时间不再是绝对值，而是处理器的使用比。

CFS称为公平调度器是因为他确保每个进程公平的处理器使用比。不会出现我们之间优先级差相同但是分配的处理器使用比不同。CFS不是完美的公平，当进程数量无穷大的时候他的调度周期也就是目标延迟也是无限大。但它确实在多进程环境下，降低了调度延迟带来的不公平性。CFS关注的是nice相对值的处理器使用比，而不是nice绝对值

### 4.5Linux调度的实现

在讨论了采用CFS调度算法的动机和内部逻辑后，我们开始具体探索CFS是如何得以试下呢，其相关代码位于文件kernel/sched_fair.c中，我们将特别关注其四个组成部分：

- 时间记账，这个之前提到过是进程记账
- 进程选择
- 调度器入口
- 睡眠和唤醒

#### 4.5.1时间记账

所有的调度器都必须对进程运行时间做记账。因为优先是让消耗处理器使用比低的进程作为下一个运行进程。多数Unix系统，分配一个时间片给每一个进程。那么当每次系统时钟节拍发生时，时间片都会减少一个节拍周期。当一个进程的时间片被减少到零 后，它会被另一个尚未减少到零的时间片可运行 进程抢占。谁消耗的最少就由谁来抢占。

##### 1.调度器实体结构

CFS不再有时间片的概念，但是它也必须维护每个进程运行的时间记账，因为它需要确保每个进程只在公平分配给他的处理器时间内运行。所谓的公平分配就是由nice相对值决定进程的处理器时间。

CFS使用调度器实体结构<linux/sched.h>的struct_sched_entity中来追踪进程运行记账：

调度器实体结构作为一个名为se的成员变量，嵌入在进程描述符task_struct内。

##### 2.虚拟实时

vruntime变量存放进程的虚拟运行时间，该运行时间（花在运行上的时间和）的计算是经过了所有可运行进程总数的标准化。虚拟时间是以ns为单位，如果是于定时器节拍相关的话就是ms为单位了，所以vruntime和定时器节拍不再相关。虚拟运行时间可以帮助我们逼近CFS模型所追求的理想多任务处理器，也就是无限小的调度周期。优先级相同的所有进程的虚拟运行时间都是相同的，所有任务都将接收到相同的处理器使用比，前提就是并行实现多任务。但是处理器无法实现完美的多任务，它必须一次运行每个任务，因此CFS使用vruntime变量来记录一个程序到底运行了多长时间以及它还应该再运行多久。

```c
static void update_curr(struct cfs_rq *cfs_rq)
{
struct sched_entity *curr = cfs_rq->curr;/*通过传入的cfs_rq结构体找到其中的sched_entity调度器实体*/
    u64 now = rq_of(cfs_rq)->clock;/*获取当前的时间*/
    unsigned long delta_exec;/*用来存放当前进程执行的时间*/
    if(unlikely(!curr))/*获得的当前时间大概率是真的，所以默认跳过*/
    	return;
    /*获取从最后一次修改负载后当前任务所占用的运行总时间*/
    delta_exec = (unsigned long)(now - curr->exec_start);
    if(!delta_exec)
        return;//如果运行总时间为0就直接退出吧，为啥这里没有使用likely呢
    __update_curr(cfs_rq,curr,delta_exec);//传入cfs_rq，调度器实体，运行总时间
    curr->exec_start = now;//更新下进程开始运行时间是当前了
    /*下面看不懂了*/
    if(entity_is_task(curr))
    {
        struct task_struct *curtask = task_of(curr);
        trace_sched_stat_runtime(curtask,delta_exec,curr->vruntime);
        cpuacct_charge(curtask,delta_exec);
        account_group_exec_runtime(curtask,delta_exec);
    }
}
```

定义在kernel/shced_fair.c文件中的update_curr()实现该记账功能。

updata_curr()函数计算了当前进程的执行时间，通过now(当前时间)-exec_start(进程开始时间点)存入了delta_exec变量中。如果执行时间为0直接返回。

然后将运行时间delta_exec传递给__update_curr()函数，由该函数根据当前可运行进程总数对运行时间进行加权计算。最终将上述的权重值与当前运行进程的vruntime相加。vruntime表示已经运行的时间，当与权重值相加之后就是准确的给定进程的运行时间了。

这里的__update_curr()函数就是计算出给定进程的vruntime。看不懂。

```c
static inline void __update_curr(Struct cfs_rq *cfs_rq,struct sched_entity *curr,unsigned long delta_exec)/*字里行间函数，执行函数不会跳转，在编译期间就直接原地展开，执行速度快，占用空间大，传入进程运行总时间*/
{
    unsigned long delta_exec_weighted;/*权重，之前提过nice是以权重的形式影响分配给每个进程的处理器使用比*/
    schedstat_set(curr->exec_max,max((u64)delta_exec,curr->exec_max));
    curr->sum_exec_runtime+=delta_exec;/*将本次运行总时间加入到sum_exec_runtime中*/
    schedstat_add(cfs_rq,exec_clock,delta_exec);
    delta_exec_weighted = calc_delta_fair(delta_exec,curr);/*将运行时间转化为权重*/
    curr->vruntime +=delta_exec_weighted;/*最终更新虚拟运行时间*/
    update_min_vruntime(cfs_rq);/*更新最小虚拟运行时间，在后续的查找最小vruntime的时候会用到*/
}
```

update_curr()是由系统定时器周期性调用的，无论是在进程处于可运行态，还是被阻塞处于不可运行态。根据这种方式，vruntime可以准确地测量给定进程的运行时间，而且可知道谁应该是下一个被运行的进程。

#### 4.5.2进程选择

在前面内容中，我们的讨论谈到如果存在一个完美的多任务处理器，所有可运行进程的vruntime值将一致。但事实上没有找到完美的多任务处理器，因此CFS试图利用一个简单的规则去均衡进程的虚拟运行时间：当CFS需要选择下一个运行进程时，它会挑一个具有最小vruntime的进程。也就是已经执行时间最小的进程。

这其实就是CFS调度算法的核心：选择具有最小的vruntime的任务。那么剩下的内容就是讨论到底是如何实现选择最小vruntime值的进程。

CFS使用红黑树来组织可运行进程队列，并利用其迅速找到最小vruntime值的进程。在Linux中，红黑树称为rbtree，它是一个自平衡二叉搜索树。红黑树是一种以树节点形式存储的数据，这些数据都会对应一个键值.可以通过这些键值来快速检索节点上的数据。重要的是，通过键值检索到对应节点的速度与整个树的节点规模成指数比关系。可以理解为是字典与二叉树的组合。

其实这里就能看出来了，Linux内核会用到数据结构，以及大量算法，这不是今年我能搞清楚的，所以这部分就跳过吧哭哭..

##### 1.挑选下一个任务

有那么一个红黑树存储了系统中所有的可运行进程，其中的节点的键值便是可运行进程的虚拟运行时间。也就是已经消耗了的进程的处理器使用比，稍后可以看到如何生成该树，但现在我们假定已经拥有它了。CFS调度器选取待运行的下一个进程，是所有进程中vruntime最小的那个，它对应的便是在树中最左侧的叶子节点。也就是说，你从树的根节点沿着左边的子节点向下找，一直找到叶子节点，便找到了其vruntime值最小的那个进程。（这里只要知道红黑树用来存储vruntime的，根据其特点最左侧的叶子节点存储的就是最小的vruntime），CFS的进程选择算法可简单总结为：**运行rbtree树种最左边叶子节点所代表的那个进程**。实现这一过程的函数是 __pick_next_entity()，他定义在文件kernel/sched_fair.c中：

```c
static struct sched_entity *__pick_next_entity(struct cfs_rq *cfs_rq)/*从返回值就能看出，返回的是调度器实体结构体，其中存放了vruntime成员，传入的cfs_rq是cfs调度算法的就绪队列cfs_rq->rb_leftmost这是最左侧的红黑树的叶子节点，另一个成员cfs_rq->rb_root这个成员是红黑树的根节点*/
{
    struct rb_node *left = cfs_rq->rb_leftmost;
    if(!left)
        return NULL;
    return rb_entry(left,struct sched_entity,run_node);/*这里的run_node是红黑树的数据节点，常用来通过run_node找到其父结构也就是红黑树，是sched_entity结构体中的成员,rb_node类型*/
}/*__pick_next_entity()函数本身并不会遍历树找到最左叶子节点，因为该值已经缓存在rb_leftmost字段中了，虽然红黑树让我们可以很有效的找到最左侧叶子节点，但是更容易地做法是将最左侧的叶子节点存储在cfs_rq->rb_leftmost成员中。此函数的返回值是便是CFS调度选择的下一个运行进程，传入最左侧叶子节点、调度器实体、红黑树到rb_entry函数中，返回值就是对应的运行进程应该是通过最左侧叶子节点找到对应的vruntime从而找到成员所在的sched_entity结构体，如果该函数返回值为NULL，那么表示没有最左叶子节点，也就是说树中没有任何节点了，毕竟总会有最小的vruntime，除非无了，这种情况下，表示没有可运行进程，CFS调度器便选择idel任务运行也就是空闲任务*/
```

##### 2.向树中加入进程

之前知道了如何查找到要运行的进程，现在看下CFS如何将进程加入rbtree中，以及如何缓存最左叶子节点。这一切发生在进程变为可运行状态（被唤醒），或者是通过fork()调用第一次创建进程时。很合理，进程不可能天生就在rbtree中。这也说明了当rb_entry函数返回值为NULL的时候说明没有进程是可运行状态的。，enqueue_entity()函数实现了这一目的：

```c
static void enqueue_entity(struct cfs_rq *cfs_rq,struct sched_entity *se,int flags)
{
if(!(flags&ENQUEUE_WAKEUP)||(flags & ENQUEUE_MIGRATE))
	se->vruntime += cfs_rq->min_vruntime;/*这里的min_vruntime也许是在update_min_vruntime函数中更新的*/
    up_date_curr(cfs_rq);//更新当前任务的运行时统计数据，包括运行总时长，vruntime，min_vruntime，为方便将进程插入到红黑树中，根据vruntime决定进程的节点位置。
    /*下面不知道干啥用的，删除树中的进程的时候会让实体出队*/
    account_entity_enqueue(cfs_rq,se);
	if(flags&ENQUEUE_WAKEUP){
        place_entity(cfs_rq,se,0);
        enqueue_sleeper(cfs_rq,se);
    }
    update_stats_enqueue(Cfs_rq,se);
    check_spread(cfs_rq,se);
    if(se ! = cfs_rq->curr)/*每个进程都有自己的调度器实体，cfs算法的调度器是一个类，在各自的调度器实体中保存进程的运行参数，比较下传入的调度器实体与cfs_rq->curr，可以将多个进程捆绑在一起作为一个调度单位进行调度，因此，可调度实体可以说一个进程也可以是一个组，将struct sched_entity* 类型的curr理解为是当前正在运行的进程组就可以理解了*/
        __enqueue_entity(cfs_rq,se);
}
```

该函数更新运行时间和其他的统计数据，内核本身也会定时的更新运行时间，然后调用__enqueue_entity()进行繁重的插入操作，把数据项真正的插入到红黑树中：

```c
static void __enqueue_entity(struct cfs_rq *cfs_rq,struct sched_entity *se)/*cfs_rq的全称是cfs的就绪队列read_queue，用红黑树管理的*/
{
    struct rb_node **link = &cfs_rq->tasks_timeline.rb_node;
    struct rb_node *parent = NULL;
    struct sched_entity *entry;
    s64 key = entity_key(cfs_rq,se);/*当前进程的键值，通过entity_key将vruntime赋值给key*/
    int leftmost = 1;//这是用于判断是否赋值cfs_rq->rb_leftmost成员
    //在红黑树中查找合适的位置
    while( *link){
        parent = *link;
        entry = rb_entry(parent,struct sched_entity,run_node);/*使用rb_entry函数查找对应的调度器实体*/
        //并不关心冲突，具有相同键值的节点呆在一起
        if(key<entity_key(cfs_rq,entry)){/*将当前键值与cfs就绪队列中的所有键值挨个比较*/
            link = &parent->rb_left;/*如果键值小与队列键值的话将link变成其左节点，接着和左节点比较*/
        }else{
            link = &parent->rb_right;
            leftmost = 0;/*如果不是最小虚拟运行时间的话该进程没必要缓存到rb_leftmost中*/
        }
    }/*当没有节点就退出，不需要和所有节点都比较，只需要比较一路即可*/
    /*维护一个缓存，其中存放树最左叶子节点*/
    if(leftmost)
        cfs_rq->rb_leftmost = &se->run_node;/*每个调度器实体与进程一一对应，与红黑树中的节点也是一一对应的，可以通过调度器实体找到该红黑树节点*/
    rb_link_node(&se->run_node,parent,link);/*在这里插入红黑树，之前是不断地比较*/
    rb_insert_color(&se->run_node,&cfs_rq->tasks_timeline);
}
```

while()循环中遍历树以寻找合适的匹配键值，该值就是被插入进程的vruntime，平衡二叉树的基本规则是，如果键值小于当前节点的键值，则需转向树的左分支；如果大于当前节点的键值，则转向右分支，如果一旦走过右边分支，哪怕一次，也说明插入的进程不会是最新的最左节点，因此可以设置为leftmost为0.如果一直都是向左移动，那么leftmost位置1，这说明我们有一个新的最左节点，并且可以更新缓存，设置rb_leftmost指向被插入的进程。当我们沿着一个方向和一个没有子节点的节点比较后：link如果为NULL表示没有节点了，循环终止，当退出循环后，接着在父节点上调用rb_link_node()，以使得新插入的进程成为其子节点。最后函数rb_insert_color()更新树的自平衡相关属性。着色问题不讨论在第六章再说。

##### 3.从树中删除进程

最后我们看看CFS是如何从红黑树中删除进程的。删除动作发生在进程堵塞（变成不可运行态）或者终止时（结束运行）：

```c
static void dequeue_entity(strut cfs_rq *cfs_rq,struct sched_entity *se,int sleep)
{
    update_curr(cfs_rq);/*更新当前任务的运行统计数据，都被删除了还要更i性能数据*/
    update_stats_dequeue(cfs_rq,se);
    clear_buddies(cfs_rq,se);
    if(se != cfs_rq->curr)/*如果删除的调度器实体并不是当前运行中的进程组，就删除吧*/
        __dequeue_entity(cfs_rq,se);
    account_entity_dequeue(cfs_rq,se);
    update_min_vruntime(Cfs_rq);
    if(!sleep)
        se->vruntime-=cfs_rq->min_vruntime;/*被删除进程的虚拟运行时间减去就绪队列的最小虚拟运行时间*/
}
```

和给红黑树添加进程一样，实际工作是由辅助函数__dequeue_entity()完成。

```c
static void __dequeue_entity(struct cfs_rq *cfs_rq,struct sched_entity *se)
{
if(cfs_rq->rb_leftmost == &se->run_node)
{
//判断下就绪队列（这个就是红黑树，因为红黑树管理的就绪队列，实际上就是说就绪队列是由红黑树数据结构实现的，就好像iio_buffer是由kfifo实现的一样），判断下最左侧叶子节点是否是我们要退队的节点
    struct rb_node *next_node;
    next_node = rb_next(&se->run_node);/*查找run_node的下一个节点，通过遍历，和父节点无关*/
    cfs_rq->rb_leftmost = next_node;/*将最左侧叶子节点设置为原最左侧叶子节点的下一个节点，毕竟如果去掉最左侧节点之后，最小的vruntime就是下一个节点了*/
}
    rb_erase(&se->run_node,&cfs_rq->tasks_timeline);/*实际上run_noce在cfs_rq中是属于timeline结构体的，将其擦除*/
}
```

从红黑树中删除进程要容易得多，因为rbtree实现了rb_erase()函数，它可完成所有工作。该函数的剩下工作是更新rb_leftmost缓存，如果删除的进程是最左节点，那么该函数要调用rb_next()按顺序遍历，找到谁是下一个节点，也就是当前最左节点被删除后，新的最左节点。

#### 4.5.3调度器入口

进程调度器的主要入口是函数schedule()，他定义在文件kernel/sched.c中，我记得在do_exit函数最终会执行schedule()函数，因为一个进程退出了需要提供下一个运行的进程。

他正是内核其他部分用于调用进程调度器的入口：选择哪个进程可以运行，何时将其投入运行。schedule()通常都需要和一个具体的调度类相关联，而使用哪个调度类取决于该进程的优先级最高，他会找到一个最高优先级的调度类-后者需要有自己的可运行队列，然后问调度类谁才是该运行的下一个进程。实际上在schedule()函数中唯一重要的事情就是他会调用pick_next_task()，该函数会以优先级 为序，从高到低，依次检查每一个调度类，并且从最高优先级的调度类中，选择最高优先级的进程：一直困惑地是之前将的说调度类的优先级取决于其中优先级最高的进程。

```c
static inline struct task_struct * pick_next_task(struct rq*rq)
{/*同样是一个内联函数，这种函数调用会很频繁，而函数本身占用的内存很少，返回值是进程描述符，传入一个就绪队列，这里并不一定是cfs的就绪队列*/
    const struct sched_class *class;
    struct task_struct *p;
    /*认为就绪队列在公平类中，直接调用该函数查找下一个进程*/
    if(likely(rq->nr_running == rq->cfs.nr_running)){
        p = fair_sched_class.pick_next_task(rq);/*调用公平类中的查找函数找到该类中的下一个进程*/
        if(likely(p))/*大概率能得到一个非空进程，返回p*/
            return p;
    }
    class = sched_class_highest;//如果不是所有的就绪队列都使用的公平类的话，先获得优先级最高的调度类
    for(;;)
    {
        p = class->pick_next_task(rq);//获得该类中的优先级最高的进程
        if(p)
            return p;//绝不可能出现返回为空的进程，因为有一个idle类返回非NULL的p
        class = class->next;/*如果该类返回了一个空的进程说明该类没有进程了，那就查找该类的下一个优先级最高的类吧*/
    }
}
```

注意该函数开始部分的优化，因为CFS是普通进程的调度类，而系统运行的绝大多数进程都是普通进程，因此这里有一个小技巧用来加速选择下一个CFS提供的进程，前提是所有可运行进程的数量等于CFS类对应的可运行进程数。这也说明了所有的可运行进程都是CFS类的。

该函数的核心是for()循环，以优先级为序，从最高的优先级类开始，遍历了每一个调度类，每一个调度类都实现了pick_next_task()函数，他会返回指向下一个可运行进程的指针，或者没有进程时返回NULL，我们会从第一个返回非NULL值的类中选择下一个可运行进程，CFS中pick_next_task()实现会调用pick_next_entity()，而该函数会再来调用我们前面内容中讨论过的__pick_next_entity()函数从而实现在该类中读取缓存cfs_rq->rb_leftmost

#### 4.5.4睡眠和唤醒

休眠的进程处于一个特殊的不可执行状态。这点非常重要，如果没有这种特殊状态的话，如果没有这种特殊状态的话，调度程序就可能选出一个本不愿意被执行的进程，更糟糕的是，休眠就必须以轮询的方式实现。进程休眠有多种原因，但肯定都是为了等待一些时间，事件可能是一段时间从文件IO读取更多数据，或者是某个硬件事件，比如外部中断。一个进程还可能尝试获取一个已被占用的内核信号量时被迫进入休眠。休眠的一个常见原因就是文件IO，如进程对一个文件执行了read()操作，而这需要从磁盘里读取。无论哪种情况内核的操作都相同：进程把自己标记成休眠状态，从可执行红黑树中移除其实就是从cfs_rq中移除，放入等待队列，然后调用schedule()选择和执行一个其他进程。唤醒的过程刚好相反：进程被设置为可执行状态，然后再从等待队列中移到可执行红黑树中。

之前讨论过，休眠有两种相关的进程状态：TASK_INTERRUPTIBLE和TASK_UNINTERRUPTIBLE。他们的唯一区别是出于不可中断的进程会忽略信号，而处于TASK_INTERRUPTIBLE状态的进程如果接收到一个信号，会被提前唤醒并响应该信号，两种状态的进程位于同一个等待队列上，等待某些事件唤醒.

##### 1.等待队列

休眠通过等待队列进行处理.等待队列是由等待某些事件发生的进程组成的简单链表.内核用**wake_queue_head_t**来代表等待队列.等待队列可以通过DECLARE_WAITQUEUE()静态创建,也可以由init_waitqueue_head()动态创建.两者区别并没有介绍.进程把自己放入等待队列并设置成不可执行状态.当与等待队列相关的事件发生的时候,队列上的进程会被唤醒.为了避免产生竞争条件,休眠和唤醒的实现不能有纰漏.

针对休眠,以前曾经使用过一些简单的接口,但那些接口会带来竞争条件:可能导致在判断条件变为真后,进程却开始了休眠,那样就会使进程无限期休眠下去,所以在内核中进行休眠的推荐操作相对复杂一些.

```c
DEFINE_WAIT(wait);/*创建一个等待队列项*/
add_wait_queue(q,&wait);//将wait添加到等待队列q中,该队列会在进程等待的条件满足时唤醒她.当然必须在其他地方撰写相关代码,在事件发生时,对等待队列执行wake_up()操作.
while(!condition){
    /*condition是我们等待的事件*/
    prepqre_to_wait(&q,&wait,TASK_INTERRUPTIBLE);/*将进程的状态变更为TASK_INTERRUPTIBLE或TASK_UNINTERRUPTIBLE,而且该函数有必要的话会将进程加回到等待队列*/
    if(signal_pending(current))/*如果状态设置为TASK_INTERRUPTIBLE,则信号唤醒进程.这就是所谓的伪唤醒,不是因为事件发生而唤醒统称为伪唤醒,因此检查并处理信号如果有信号就不执行schedule函数*/
        schedule();/*当进程被唤醒的时候会再次检查条件是否为真,如果是,他就退出循环;如果不是,他再次调用schedule()并一直重复这一步操作就是不断的阻塞本进程,这里才是真实的让进程休眠的函数,毕竟此时进程状态被修改了,然后schdule函数去执行其他的进程,那么本进程不就陷入休眠了吗*/
}
finish_wait(&&q,&wait);/*当条件满足后,进程为自己设置为TASK_RUNNING并调用finish_wait()方法,把自己移出等待队列*/
```

如果在进程开始休眠之前条件就已经达成了,那么循环会退出,进程不好存在错误地进入休眠的倾向.这种方式就是如果没进入循环的话,就是放入等待队列然后判断不进入循环,然后退出等待队列.而早期的简单操作就是判断下是否阻塞,如果是的话就进入等待队列.这就导致了判断为真但是并不需要阻塞的竞争关系.

需要注意的是,内核代码在循环体内常常需要完成一些其他的任务,比如他可能在调用schedule()之前需要释放掉锁,而在这以后再重新获取他们.

函数inotify_read(),位于文件fs/notify/inotify/inotify_user.c中,负责通知从文件描述符中读取信息,他的实现无疑是等待队列的一个典型用法:

```c
static ssize_t inotify_read(struct file*file,char __user *buf,size_t count,loff_t *pos)
{
    struct fsnotify_group *group;
    struct fsnotify_event *kevent;
    char __user *start;
    int ret;
    DEFINE_WAIT(wait);
    start = buf;
    group = file->private_data;
    while(1){
        prepare_to_wait(&group->notification_waitq,
                       &wait,
                       TASK_INTERRUPTIBLE);/*将wait加入等待队列中,而这里的等待队列是file文件的等待队列,并将wait进程状态设置为可中断模式*/
        mutex_lock(&group->notification_mutex);//加上互斥锁
        kevent = get_one_event(group,count);
        mutex_unlock(&group->notification_mutex);//解锁
        if(kevent){
            ret = PTR_ERR(kevent);
            if(IS_ERR(kevent))
                break;
            ret = copy_event_to_user(group,kevent,buf);/*如果真的获得了事件,就将此事件发送给用户吧*/
            fsnotify_put_event(kevent);
            if(ret<0)
                break;
            buf +=ret;
            count -=ret;
            continue;
        }
        ret = -EAGAIN;
        if(file->f_flags &O_NONBLOCK)/*如果文件打开方式是非阻塞的话直接退出循环*/
            break;
        ret = -EINTR;
        if(signal_pending(current))/*如果有信号的话直接退出循环*/
            break;
        if(start !=buf)
            break;
        schedule();
    }
    finish_wait(&group->notification_waitq,&wait);/*当退出循环的时候,醉解移除等待队列*/
    if(start !=buf && ret != -EFAULT)
        ret = buf - start;
    return ret;
}
```

这就是之前介绍阻塞的示例

##### 2.唤醒

唤醒操作通过函数wake_up()进行,他会唤醒指定的等待队列上的所有进程.他调用函数try_to_wake_up(),这里为啥唤醒所有队列,因为该等待队列是因为同一个资源而阻塞的,所以此时资源有了需要所有进程竞争下,然后再阻塞绝大部分进程.这里的唤醒和上方的finish_wait函数不同,后者是因为条件满足了从而把进程移除等待队列,这是保护机制.

该函数负责将进程设置为TASK_RUNNING状态,调用enqueue_task()将此进程放入红黑树中,如果被唤醒的进程优先级比当前正在执行的进程的优先级高,还要设置need_resched标志.

通常哪段代码促使等待条件达成,他就要负责随后调用wake_up()函数.举例来说,当磁盘数据到来时,VFS就要负责对等待队列调用wake_up(),以便唤醒队列中等待这些数据的进程.

关于休眠有一点需要注意,存在虚假的唤醒,有时候进程给被唤醒并不是因为他所等待的条件达成了,所以我们才需要用一个循环处理来保证他等待的条件真正达成.

![1670158471022](C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1670158471022.png)

这里可以看到,schedule函数才是让进程休眠的函数,他将被阻塞的进程从就绪队列中移除.并执行其他可用的进程.有两种唤醒方式,一种是伪唤醒,由非事件唤醒的,一种是事件唤醒的.

### 4.6抢占和上下文切换

上下文切换,也就是从一个可执行进程切换到另一个可执行进程,由定义在kernel/sched.c中的context_switch()函数负责处理.

每当一个新的进程被选出来准备投入运行的时候,schedule()就会调用context_switch()函数,他完成两项基本的工作:

- 调用声明在<asm/mmu_context.h>这是体系结构的文件,函数负责把虚拟内存从上一个进程映射切换到新进程中.
- 调用声明在<asm/system.h>中的switch_to(),该函数负责从上一个进程的处理器状态切换到新进程的处理器状态.这包括保存/恢复栈信息和寄存器信息,还有其他任何与体系结构相关的状态信息,都必须以每个进程为对象进行管理和保存.

实际上之前学的那些进程加入树/从树中移除,挑选下一个任务,睡眠唤醒之类的都不是底层,只是不断地修改数据结构而已.

上下文切换才是真正的底层.这是需要操作寄存器的,之前的都是操作schedule函数而已,此函数最终调用context_switch()函数.

内核必须知道在什么时候调用schedule(),如果仅靠用户程序代码显式地调用schedule(),他们可能就会永远的执行下去.这里指的是我们显式往往考虑不够全面导致调用不足.

相反,内核提供了一个need_resched标志来表明是否需要重新执行一次调度.当某个进程应该被抢占时,scheduler_tick()就会设置这个标志;当一个优先级高的进程进入可执行状态的时候,try_to_wake_up()也会设置这个标志,内核检查该标志,确认被设置,调用schedule()来切换到一个新的进程.该标志对于内核来讲是一个信息,他表示有其他进程应当被运行,要尽快调用调度程序.

| 函数                     | 目的                                                         |
| ------------------------ | ------------------------------------------------------------ |
| set_tsk_need_resched()   | 设置指定进程中的need_resched标志                             |
| clear_tsk_need_resched() | 清除指定进程中的need_resched标志,在创建一个进程的时候会调用此函数 |
| need_resched()           | 检查need_resched标志的值,如果被设置就返回真,否则返回假       |

再返回用户空间以及从中断返回的时候,内核也会检查need_resched标志,如果也被设置,内核会在继续执行之前调用调度程序.

每个进程都包含一个need_resched标志,这是因为i访问进程描述符内的数值要比访问一个全局变量快,(current宏速度很快,并且描述符通常都在高速缓存中).在2.2以前的内核版本中,该标志曾经是一个全局变量.2.2到2.4版内核中他在task_struct中,而2.6版本中,他被移到thread_info结构体中,用一个特别的标志变量中的一位来表示.

看得出来在很多地方都会设置need_resched标志.

#### 4.6.1用户抢占

内核即将返回用户空间的时候,如果need_resched标志被设置,会导致schedule()被调用.此时就会发生用户抢占.

在内核返回用户空间的时候,他知道自己是安全的,因为既然它可以继续去执行当前进程,那么他当然可以再去选择一个新的进程去执行.这句话没看懂.

所以内核无论是在中断处理程序还是在系统调用后返回用户层,都会检查need_resched标志.如果它被设置了,那么内核会选择一个其他进程投入运行.从中断处理函数或者系统调用返回的返回路径都是和体系结构有关.

在entry.S文件中通过汇编语言实现.(此文件不仅包含内核入口部分的程序,内核退出部分的相关代码也在其中)

简而言之,用户抢占在以下情况时产生:用户抢占并不是说用户来抢占cpu了,而是在返回用户层的时候被其他进程抢占了

- 从系统调用的时候**返回用户空间**的时候
- 从中断处理程序**返回用户空间**的时候

#### 4.6.2内核抢占

与其他大部分的Unix变体和其他大部分的操作系统不同,Linux完整地支持内核抢占,在不支持内核抢占的内核中,内核代码可以一直执行,直到运行完毕,也就是说,**调度程序没有办法在一个内核级的任务正在执行的时候重新调度**,内核中的各任务是以协作方式调度的,不具备抢占性,内核代码一直要执行到完成(其实就是直到返回用户空间)或明显的阻塞为止.在2.6版的内核中,内核引入了抢占内核;

现在,只要重新调度是安全的,内核就可以在任何时间抢占正在执行的任务.

那么,什么时候重新调度才是安全的呢?只要没有持有锁,内核就可以进行抢占.锁是非抢占区域的标志.内核是不希望当前正在运行的进程持有锁被阻塞,这样的话容易引发死锁现象.只要你空无一物,啥也没有.人家就可以随便的搁置你.

为了支持内核抢占所做的第一处变动,就是为每个进程的thread_info引入preempt_count计数器.该计数器初始值为0,每当使用锁的时候数值加1,释放锁的时候数值减一,当数值为0的时候,内核就可执行抢占.

从中断返回内核空间的时候,内核会检查need_resched和preempt_count的值.如果need_resched被设置,并且preempt_count为0的话,这说明有一个更为重要的任务需要执行并且可以安全地抢占,此时调度程序就会被调度.如果preempt_count不为0,说明当前任务持有锁,所以抢占是不安全的,这时,内核就会像通常那样直接从中断返回当前执行进程.

如果内核中的进程被阻塞了,或它显式地调用lschedule(),内核抢占也会显示地发生.这种形式的内核抢占从来都是受支持的,因为根本无需额外的逻辑来保证内核可以安全地被抢占.如果代码显式地调用了schedule(),那么它应该清楚自己是可以安全地抢占的.

内核抢占会发生在:

- 中断处理程序正在执行,且返回内核空间之前
- 内核代码具有可抢占性的时候,这里指的就是出现了一个比当前进程更优先的进程
- 如果内核中的任务显式地调用schedule()
- 如果内核中的任务阻塞

其实就是说只要隐式need_resched=1且preempty_count =0或显式的调用了schedule,就会导致内核抢占.其实就是在内核进程执行的时候被抢占了而已.

这里的区别在于用户抢占是在内核进程执行完毕返回用户层的时候发生抢占

而内核抢占指的是在内核进程执行一半的时候发生抢占.

### 4.7实时调度策略

Linux提供了两种实时调度策略:SCHED_FIFO和SCHED_RR.而普通的/非实时的调度策略是SCHED_NORMAL,借助调度类的框架,这些实时策略并不被完全公平调度器管理,而是被一个特殊的实时调度器管理.具体的实现定义在文件kernel/sched_rt.c中,在接下来的内容中我们将讨论实时调度策略和算法.

SCHED_FIFO实现了一种简单的/先入先出的调度算法:他不使用时间片.处于可运行状态的SCHED_FIFO级的进程会比任何SCHED_NORMAL级的进程都先得到调度.一旦一个SCHED_FIFO级进程处于可执行状态,就会一直执行,直到他自己受阻塞或显式地释放处理器为止;因为不基于时间片,可以一直执行下去,除非有更高优先级的SCHED_FIFO或者SCHED_RR任务才能抢占SCHED_FIFO任务.如果有两个或者更多的同优先级的SCHED_FIFO级进程,他们会轮流执行,但是依然只有 他们愿意让出处理器时才会退出.只要有SCHED_FIFO级进程在执行,其他级别较低的进程就只能等待它变为不可运行态才有机会执行.

SCHED_RR与SCHED_FIFO大体相同，只是SCHED_RR级的进程在耗尽事先分配给他的时间后就不能再继续执行了，这种SCHED_RR级进程是通过时间片轮询的SCHEDFIFO，这是一种实时轮流调度算法。当SCHED_RR任务耗尽他的时间片时，再同一优先级的其他实时进程被轮流调度，时间片只用来重新调度同一优先级的进程。对于SCHED_FIFO进程，高优先级总是立即抢占低优先级，但低优先级进程绝不能抢占SCHED_RR任务，即使他的时间片耗尽。

这两种实时算法实现的都是静态优先级。内核不为实时进程计算动态优先级。这能保证给定优先级别的实时进程总能抢占优先级比他低的进程。之前提到过实时进程是再ps -oe会出现“-”的进程，所谓的动态优先级指的是采用cfs算法中的vruntime吗

Linux的实时调度算法提供了一种软实时工作方式，软实时的含义是，内核调度进程，尽力使进程在他的限定时间到来前运行，但内核不保证总能满足这些进程的要求，相反，硬实时系统保证在一定条件下，可以满足任何调度的要求。Linux对于实时任务的调度不做任何保证。虽然不能保证硬实时工作方式，但Linux的实时调度算法的性能还可以，2.6的内核可以满足严格的时间要求。

实时优先级范围从0道MAX_RT_PRIO减1.默认情况下，MAX_T_PRIO为100，所以默认的实时优先级范围是从0到99，这就是之前讲过的数值越大优先级大。之前还在困惑RT_PRIO与nice之间的关系，SCHED_NORMAL级进程这就是普通进程的nice值共享了这个取值空间；他的取值范围是从MAX_RT_PRIO到MAX_RT_PRIO+40，也就是说默认情况下，nice值从-20到+19直接对应的是从100到139的实时优先级范围。不过nice作为优先级并不是直接映射时间片的，是采用CFS调度算法。

### 4.8与调度相关的系统调用

Linux提供了一个系统调用族，用于管理和调度程序相关的参数。这些系统调用可以用来操作和处理进程优先级、调度策略及处理器绑定，同时还提供了显式地将处理器将给其他进程的机制。

许多书籍都提供了这些系统调用的说明，下表列举了这些系统调用并给出简短的说明。

| 系统调用                 | 描述                                                   |
| ------------------------ | ------------------------------------------------------ |
| nice()                   | 设置进程的nice值                                       |
| sched_setscheduler()     | 设置进程的调度策略                                     |
| sched_getscheduler()     | 获取进程的调度策略                                     |
| sched_setparam()         | 设置进程的实时优先级                                   |
| sched_getparam()         | 获取进程的实时优先级                                   |
| sched_get_priority_max() | 获取实时优先级的最大值                                 |
| sched_get_priority_min() | 获取实时优先级的最小值                                 |
| sched_rr_get_interval()  | 获取进程时间片值，毕竟只有rr才有时间片                 |
| sched_setaffinity()      | 设置进程的处理器的亲和力，指的是进程在哪个处理器上运行 |
| sched_getaffinity()      | 获取进程的处理器的亲和力，不知道这个亲和力是啥         |
| sched_yield()            | 暂时让出处理器                                         |

#### 4.8.1与调度策略和优先级相关的系统调用

sched_setscheduler()和sched_getscheduler()分别用于设置和获取进程的调度策略和实时优先级。与其他的系统调用相关，他们的实现也是由许多参数检查、初始化和清理构成的，其实最重要的工作在于读取或改写进程task_struct的policy和rt_priority的值。

sched_setparam()和sched_getparam()分别用于设置和获取进程的实时优先级。这两个系统调用获取封装在sched_param特殊结构体的rt_priority。sched_get_priority_max()和sched_get_priority_min()分别用于返回给定调度策略的最大和最小优先级。实时调度策略的最大优先级是MAX_USER_RT_PRIO减1，最小优先级等于1

对于一个普通的进程，nice()函数可以将给定进程的静态优先级增加一个给定的量。只有超级用户才能在调用它时使用负值，从而提高进程的优先级。nice()函数会调用 内核的set_user_nice()函数，这个函数会设置进程的task_struct的staic_prio和prio值。这个是静态优先级的话，之前的rt_priority是什么实时优先级吗但是之前的实时进程使用的也是静态优先级

#### 4.8.2与处理器绑定有关的系统调用

Linux调度程序提高强制的处理器绑定(processor affinity)机制。也就是说，虽然它尽力通过一种软的亲和力试图使进程尽量在同一个处理器上运行，但他也允许用户强制指定这个进程无论如何都必须在这些处理器上运行。这种强制的亲和性保存在task_struct的cpus_allowed这个位掩码标志中。该掩码标志的每一位对应一个系统可用的处理器。默认情况下，所有的位都被设置，进程可以在系统中所有可用的处理器上执行。用户可以通过sched_setaffinity()设置不同的一个或几个位组合的位掩码，而调用sched_getaffinity()则返回当前的cpus_allowed位掩码。

内核提供的强制处理器绑定的方法很简单。首先，当处理进行第一次创建时，他继承了其父进程的相关掩码，由于父进程运行在指定处理器上，子进程也运行在相应处理器上。其次，当处理器绑定关系改变时，内核会采用**移植线程**把任务推到合法的处理器上。最后，加载平衡器只把任务拉到允许的处理器上，因此，进程只运行在指定处理器上，对处理器的指定是由该进程描述符的cpus_allowed设置的。这里只是简单提了下流程而已。

#### 4.8.3放弃处理器时间

Linux通过sched_yield()系统调用，提供了一种让进程显式地将处理器时间让给其他等待执行进程的机制。它是通过将进程从活动队列中（因为进程正在执行，所以他肯定位于此队列当中），移到过期队列中实现的。由此产生的效果不仅抢占了该进程，并将其放入优先级队列的最后面，还将其放入过期队列中。（过期队列、就绪队列、运行队列、等待队列怎么这么多队列），这样能确保在一段时间内不会再被执行了。由于实时进程不会过期，所以属于例外。他们只被移动到其优先级队列的最后面不会放到过期队列中。再Linux的早期版本中，sched_yield()的语意有所不同，进程只会被放置到优先级队列的末尾，放弃的时间往往不会太长。现在，应用程序甚至内核代码在调用sched_yield()前，应该仔细考虑是否真的希望放弃处理器时间 。

### 4.9小结

进程调度程序是内核重要的组成部分，因为运行着的进程首先在在使用计算机。然而，满足进程调度的各种需要绝不是轻而易举的：很难找到一刀切的算法，既适合众多的可运行进程，又具有可伸缩性，还能在调度周期和吞吐量之间求得平衡，同时还满足各种负载的需求。不过Linux内核的新CFS调度程序尽量满足了各个方面的需求，并以较完善的可伸缩性和新颖的方法提供了最佳的解决方案。

前面的章节覆盖了进程管理的相关内容，本章则考察了进程调度所遵循的基本原理、具体实现、调度算法以及目前Linux内核所使用的接口。第五章将涵盖内核提供给运行进程的主要接口-系统调用

### 总结

本章主要学习了Linux的进程调度,首先是讲了下进程优先级与时间片之间的映射关系/以及采用时间片方式的危害.最后采用了CFS调度策略.然后是这个策略如何实现,包括时间记账/进程选择/调用调度器的入口/睡眠和唤醒

由于Linux支持内核抢占,所以可以实现内核进程执行一半的时候就抢占,用户抢占是返回用户空间的时候也就是内核进程结束之后可以发送抢占,最后介绍了下用到的系统调用.

现在知道进程调度的实现步骤以及选择CFS的原因了,和用到的系统调用.

## 第五章系统调用

在现代操作系统中,**内核提供了用户进程与内核进行交互的一组接口**.这些接口让应用程序受限地访问硬件设备,提供了创建新进程并与已有进程进行通信的机制,也提供了申请操作系统其他资源的能力.这些接口在应用程序和内核之间扮演了使者的角色,应用程序发出各种请求,而内核负责满足这些请求(或者无法满足时返回一个错误).实际上提供这些接口主要是为了保证系统稳定可靠,避免应用程序随便.就是通过调用系统调用陷入的内核空间.不太对,这里说的意思好像是系统调用/陷入/异常三种情况访问内核.(陷入==系统调用是软中断进入到svc模式，异常可能是进入中止模式，避免应用程序肆意指的是内核空间的页表项具有访问权限的位域段，禁止用户模式下访问)

### 5.1与内核通信

系统调用在用户空间和硬件设备之间添加了一个中间层.该层作用有三个。

- 首先,他**为用户空间提供了一个硬件的抽象接口**.举例来说,当需要读写文件的时候,应用程序就可以不去管磁盘类型和介质,甚至不用去管文件所在的文件系统到底是哪种类型.（这个涉及到VFS问题，系统调用属于虚拟文件系统中的一部分，包括驱动模块中注册字符设备的时候有一个操作函数结构体，这实际上就是替换了vfs中对应的系统调用。）
- 第二,**系统调用保证了系统的稳定和安全**.作为硬件设备和应用程序之间的中间人,内核可以基于权限/用户类型和其他一些规则对需要进行的访问进行裁决.举例说明,这样可以避免应用程序不正确地使用硬件设备,窃取其他进程的资源,或做出其他危害系统的事情.（这就是页表项中访问权限）
- 第三,**每个进程都运行在虚拟系统中**,而在用户空间和系统的其余部分提供这样一层公共接口,也是出于这种考虑.如果应用程序可以随意访问硬件,而内核又对此一无所知的话,几乎就无法实现多任务和虚拟内存,当然也不可能实现良好的稳定性和安全性.这个讲的和第二点没有区别

Linux中,系统调用是用户空间访问内核的唯一手段;除异常和陷入外,他们是内核唯一的合法入口.实际上,其他的像设备文件和/proc虚拟文件系统之类的方式,最终也是要通过系统调用进行访问的.而有趣的是,Linux提供的系统调用却比大部分操作系统都少的多.本章重点强调Linux系统调用的规则和实现方式.

### 5.2API/POSIX和C库

一般情况下,应用程序通过在用户空间实现的应用编程接口API而不是直接通过系统调用来编程.这点很重要,因为应用程序使用的这种编程接口时尚并不需要和内核提供的系统调用对应.一个API定义了一组应用程序使用的编程接口.他们可以实现成一个系统调用,也可以通过调用多个系统调用来实现,而完全不适用任何系统调用也可以的.

实际上API可以在各种不同的操作系统上实现,给应用程序提供完全相同 接口,而他们本身在这些系统上的实现却可能不同.这就提供了上层软件可移植性的可能.下图给出了应用程序和C库和内核之间的关系.

![1670208199269](C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1670208199269.png)

在Unix世界中,最流行的应用编程接口是基于POSIX标准的,之前见过太多次了.从纯技术的角度看,POSIX是由IEEE的一组标准组成,其目的是提供一套大体上基于Unix的可移植操作系统标准.在应用场合,Linux尽力与POSIX和SUSv3兼容.

POSIX是说明API和系统调用之间关系的一个极好例子.在大多数Unix系统上,根据POSIX定义的API函数和系统调用之间有着直接关系.实际上,POSIX标准就是仿照早期Unix系统的接口建立的.另一方面,许多操作系统像微软的Windows,尽管是非Unix系统,也提供了与POSIX兼容的库.

Linux的系统调用像大多数Unix系统一样,作为C库的一部分提供.C库实现了Unix系统的主要API,包括标准C库函数和系统调用接口.所有的C程序都可以使用C库,由于C语言本社你的特点,其他语言也可以很方便地把他们封装起来使用.此外,**C库提供了POSIX的绝大部分API**.这里说出来C库与POSIX标准之间的关系.

从上层软件程序员的角度看,系统调用无关紧要,他们只需要跟API打交道就可以了.相反,内核只跟系统调用打交道;库函数和应用程序是怎么使用系统调用,不是内核所关心的.但是,内核必须时刻牢记系统调用所有潜在的用途,并保证他们有良好的通用性和灵活性.

关于Unix的接口设计有一句格言,提供机制而不是策略.换句话说,Unix的系统调用抽象出了用于完成某种确定的目的的函数.这是机制.这些函数怎么用(这是策略)完全不需要内核关系.

### 5.3系统调用

要访问系统调用(Linux中常称为syscall),通常通过C库中定义的函数调用来进行.他们通常都需要定义0-多个参数,而且可能产生一些副作用,例如写某个文件,系统调用还会通过一个long类型的返回值来表示成功或者错误.通常,但也不绝对,用一个负的返回值来表明错误.返回一个0值通常表示成功.系统调用在出现错误的时候C库会把错误码写入errno全局变量.通过调用perror()库函数,可以把该变量翻译成用户可以理解的错误字符串.

区别对待机制和策略是Unix设计中的一大亮点.大部分的变成问题都可以被切割成两个部分:需要提供什么功能和怎么实现这些功能.

当然,系统调用最终具有一种明确的操作.例如getpid()系统调用,根据定义会返回当前进程的PID,内核中实现很简单

```c
SYSCALL_DEFINE0(getpid)
{
return task_tgid_vnr(current);//returns current->tgid,current是一个宏,实际上是current_thread_info()->task
}
```

注意,定义中并没有规定他要如何实现,内核必须提供系统调用所希望完成的功能,但他完全可以按照自己预期的方式去实现,只要最后的结果正确就行了.当然,上面的系统调用太简单,也没有什么更多的实现手段.

SYSCALL_DEFINE0只是一个宏,它定义了一个无参数的系统调用,展开后的代码如下:

```c
asmlinkage long sys_getpid(void)
```

我们看一下,如何定义系统调用.首先,注意函数声明中的asmlinkage限定词,这是一个编译指令,通知编译器仅从栈中提取该函数的参数.不理解这句话,所有的系统调用都需要这个限定词,其次函数返回long,为了保证32位和64系统的兼容,系统调用在用户空间和内核空间有不同返回值类型,在用户空间为int,在内核空间为long.比如调用get_pid()系统调用,会导致其陷入内核中,执行内核相应的系统调用,然后内核中返回的是long类型,返回给用户层,然后get_pid()函数返回,把这个long类型修改成int类型了。（这里仅从栈中获取函数参数，这里的栈是内核栈还是用户进程栈）。

最后,注意系统调用get_pid()在内核中被定义成sys_getpid().这是Linux中所有系统调用都应该遵守的命令规则,系统调用bar()在内核中也实现为sys_bar()函数.

#### 5.3.1系统调用号

在Linux中,每个系统调用被赋予一个系统调用号.这样通过这个独一无二的号就可以关联系统调用了.**当用户空间的进程执行一个系统调用的时候,这个系统调用号就用来指明到底是要执行内核中的哪个系统调用**,进程不会提及系统调用的名称,这里把如何从用户空间的系统调用到内核中对应的系统调用了.（这里的系统调用号就是软中断号，因为应用层的系统调用接口具体实现就是SWI指令，执行SWI指令后跳转到SWI处理程序计算出SWI号，然后跳转到SWI服务子程序）

系统调用号很重要,一旦分配就不能再有任何变更,否则编译好的应用程序就会崩溃,此外,如果一个系统调用被删除,她所占用的系统调用号也不允许被回收利用,否则,以前编译过的diamagnetic会调用这个系统调用,但事实上却调用的是哪个被删除的系统调用.Linux有一个未实现系统调用sys_ni_syscall(),它除了返回-ENOSYS外不做任何其他工作,这个错误号就是专门针对无效的系统调用而设的,虽然很罕见,但如果一个系统调用被删除,或者变得不可调用,这个函数就要负责填补空缺.

内核记录了系统调用表中的所有已注册过的系统调用的列表,存储在sys_call_table中.每一种体系结构,都明确定义了这个表,在x86-64中,他定义在arch/i386/kernel/syscall_64.c文件中,这个表为每一个有效的系统调用指定了唯一的系统调用号。（这个表其实类似于中断向量表，只不过这是软件控制的，通过SWI号找到，中断向量表是通过跳转到向量表基地址偏移）

#### 5.3.2系统调用的性能

Linux系统调用比其他许多操作系统执行得要快。Linux很短的上下文切换时间是一个重要原因，进出内核都被优化得简洁高效。另外一个原因是系统调用处理程序和每个系统调用本身也都非常简介。

### 5.4系统调用处理程序

用户空间的程序无法直接执行内核代码。他们不能够直接调用内核空间中的函数，因为内核驻留在受保护的地址空间上。如果进程可以直接在内核的地址空间上读写的话，系统的安全性和稳定性将不复存在。

所以，应用程序应该以某种方式通知系统，告诉内核自己要执行一个系统调用，希望系统切换到内核态，这样内核就可以代表应用程序在内核空间执行系统调用。

通知内核的机制是靠软中断实现的：通过引发一个异常来促使系统切换到内核态去执行异常处理函数。此时的异常处理函数实际上就是系统调用处理程序。在x86系统上预定义的**软中断是中断号128**，通过int$0x80指令触发该中断。这条指令会触发一个异常导致系统切换到内核态并执行地128号异常处理程序，而该程序正是系统调用处理程序。这个处理程序名字起的很贴切，叫system_call().实际上就是触发一个软中断去执行处理函数，毕竟异常是软件无法使用硬件中断处理。他与硬件体系结构紧密相关，x86-64的系统上在entry_64.S文件中用汇编语言编写。最近，x86处理器增加了一条叫做sysenter的指令，与int中断指令相比，这条指令提供了更快、更专业的陷入内核执行系统调用方式，对这条指令的支持很快被加入内核。且不管系统调用处理函数是如何被调用的，用户空间引起异常或陷入内核就是一个重要的概念。我感觉异常也是系统调用的一种，他们都是通过陷入内核实现在内核中执行进程的。

#### 5.4.1指定恰当的系统调用

因为所有的系统调用陷入内核的方式都是一样，所以仅仅是陷入内核空间是不够的。因此必须把系统调用号一并传给内核。在x86上，系统调用号是通过eax寄存器传递给内核的。在陷入内核之前，用户空间就把相应系统调用所对应的号放入eax中。这样系统调用处理程序一旦运行，就可以从eax中得到数据。其他体系结构上的实现也都类似。从这个角度反而能理解了，之前停留在像信号那样携带伴随数据，这里是直接存入特定的寄存器中。(意味着在用户空间下实现SWi处理程序获得软中断号，对于ARM架构而言，SWI号就是系统调用号，SWI号是通过屏蔽软中断指令地址的高8位获得的，然后根据中断号寻找系统调用表。中断是通过处理器识别中断类型确定的偏移地址，然后获得中断向量表基地址相加得到的)。

system_call()函数通过将给定的系统调用号与NR_syscalls做比较来检查其有效性。如果它大于或者等于NR_syscalls，这里的就是系统调用号，在<asm/unistd.h>文件中列举了每个进程的系统调用号的宏定义！该函数就返回-ENOSYS，否则就执行相应的系统调用：

```c
call *sys_call_table(,%rax,x);
```

由于系统调用表中的表项是以64位类型存放的，所以内核需要将给定的系统调用号乘以4，然后用所得的结果在该表中查询其位置。

在x86-32系统上，代码类似，只是用4代替8.这个乘法不懂。

![1670238142904](C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1670238142904.png)

上图可以看出来，在进程中调用read()，这是标准c库的函数在c库中查找真正实现的系统调用，执行系统调用导致触发软中断陷入到内核中，将系统调用号存入exa寄存器中，然后再内核中执行处理函数system_call()在这个函数中读取exa寄存器值，通过这个系统调用号去找系统调用表，在表中找到相应的系统调用，sys_read()函数。

#### 5.4.2参数传递

除了系统调用号以外，大部分系统调用都还需要一些外部的参数输入。所以，在发生陷入的时候，应该将这些参数从用户空间传给内核，最简单的办法就是像传递系统调用号一样，把这些参数也存放在寄存器中，毕竟内存不重合，所以无法在内存中调用数据。（这是我一刷的时候能说出的话吗，页表项访问权限不允许）

在x86-32系统上，ebx/ecx/edx/esi/edi按照顺序存放前五个参数。需要第六个或者六个以上参数的情况不多见，此时，应该用一个单独的寄存器存放指向所有这些参数在用户空间地址的指针。给用户空间的返回值也通过寄存器传递。在x86系统上，他存放在eax寄存器中。驱动模块的read函数也是存放在寄存器中从而实现传递给用户层的吧。

### 5.5系统调用的实现

实际上，一个Linux的系统调用在实现时并不需要太关心他和系统调用处理程序之间的关系。给Linux添加一个新的系统调用是件相对容易的工作。其实系统调用处理程序是通用的，只不过是我们传递的系统调用号导致的区别。怎样设计和实现一个系统调用是难题所在，而把他加到内核里却无需太多周折。关注下实现一个新的Linux系统调用所需的步骤。

#### 5.5.1实现系统调用

实现一个新的系统调用的第一步是决定他的用途。他要做什么？每个系统调用都应该有一个明确的用途。在Linux中不提倡采用多用途的系统调用（一个系统调用通过传递不同的参数值来选择完成不同的工作）。ioctl()就是一个很好的例子，告诉我们不应当做什么。我记得ioctl被称为文件的工具箱。

新系统调用的参数、返回值和错误码又该是什么，系统调用的接口应该力求简洁，参数尽可能少。系统调用的语义和行为非常关键；因为应用程序依赖于他们，所以他们应力求稳定，不做改动。设想一下，

- 如果功能多次改变会如何，
- 新的功能是否可以追加到系统调用亦或是
- 某个改动将需要一个全新的函数？
- 是否可以容易的修订错误而不用破坏向后兼容？

很多系统调用提供了标志参数以确保向前兼容。标志并不是用来让单个系统调用具有多个不同的行为，这是不被允许的，而是为了在不破坏向后兼容或不需要增加新的系统调用的情况下增加新的功能和选项。

设计接口的时候要尽量为将来多做考虑。你是不是对函数做了不必要的限制，系统调用设计的越通用越好，不要假设这个系统调用现在怎么用，将来也一定这么使用，系统调用的目的可能不变，但他的用法却可能改变。这个系统调用可移植吗？别对机器的字节长度和字节序做假设（就是大小端）。要确保不对系统调用做错误的假设，否则将来这个调用就可能会崩溃。

记住Unix的格言：提供机制而不是策略。

当你写一个系统调用的时候，要时刻注意可移植性和健壮性，不但要考虑当前，还要为将来做打算。基本的Unix系统调用经受住了时间的考验，他们中很大一部分到现在还和40年前一样适用和有效。

#### 5.5.2参数验证

系统调用必须仔细检查他们的所有参数是否合法有效。系统调用在内核空间执行，如果任由用户将不合法的输入传递到内核，那么系统的安全和稳定将面临极大的考验。

举例来说，与文件IO相关的系统调用必须检查文件描述符是否有效。与进程相关的函数必须检查提供的PID是否有效。必须检查每个参数，保证他们不但合法有效，而且正确。进程不应当让内核去访问那些它无权访问资源。

最重要的一种检查就是检查用户提供的指针是否有效。在接收一个用户空间的指针之前，内核必须保证：

- 指针指向的内存区域属于用户空间，进程绝不能哄骗内核去读内核空间的数据。这里让内核读取本身的数据有问题吗，用户层读取驱动模块不就是读内核空间的数据吗，也不太对，是读取用户空间下的设备节点文件，然后设备节点中的进程运行在内核态下。这么说好像没问题。（这里意思是如果提供指针）
- 指针指向的内存区域在进程的地址空间里，绝不能哄骗内核去读其他进程的数据。（其他进程的数据不许访问，有没有可能也是指针提供虚拟地址，内核应该会经过FCSE）
- 如果是读，该内存应被标记为可读；如果是写，该内存应被标记为可写；如果是可执行，该内存被标记为可执行，进程绝不能绕过内存访问限制。（这个是页表项的访问权限）

内核提供了两个方法来完成必须的检查和内核空间与用户空间之间数据的来回拷贝。注意内核无论何时都不能轻率地接受来自用户空间的指针，这两个方法中必须有一个经常被使用。

为了向用户空间写入数据，内核提供了copy_to_user()，正是此函数实现的驱动模块的数据传输，使用寄存器是传入参数和返回值的时候，大量数据肯定不能用寄存器。

他需要三个参数，第一个参数是进程空间中的目的内存地址，第二个是内核空间内的源地址，最后一个参数是需要拷贝的数据长度也就是字节数。没毛病，当初使用的时候需要提供用户层的内存地址，这个就是寄存器传入的，以及内核空间中的指针，最后是字节数。(这两个函数也是内核空间与用户空间交流的方式之一)

如果执行失败，这两个函数返回的都是没能完成拷贝的数据的字节数。如果成功则返回0，当出现上述错误的话，系统调用返回标准-EFAULT，不会设置errno的这个全局变量定义在用户空间的。

当我们以一个即使用了copy_from_user()又用了copy_to_user()的系统调用做例子进行考察。

这个系统调用silly_copy()毫无实际用处，他从第一个参数里拷贝数据到第二个参数。没必要让内核空间作为中转站，把用户空间的数据从一个位置复制到另一个位置。但他却演示出上述函数做法。

silly_copy没有实际价值的系统调用，他把len字节的数据从src拷贝到dst，毫无理由地让内核空间作为中转站。

```c
SYSCALL_DEFINE3(silly_copy,unsigned long *,src,unsigned long *,dst,unsigned long len)//这里的SYSCALL_DEFINE3是一个宏里面存的就是sys_xx系统调用了
{
    unsigned long buf;
    if(copy_from_user(&buf,src,len))//将用户地址空间中的src拷贝进buf
        return -EFAULT;
    if(copy_to_user(dst,&buf,len))
        return -EFAULT;
    return len;//返回拷贝的字节数，通过存入eax寄存器传递给用户层。
}
```

注意，copy_to_user和copy_from_user都有可能引起阻塞。当包含用户数据的页被换出到硬盘上而不是在物理内存上的时候，就会发生阻塞，此时，进程会休眠，直到缺页处理程序将该页从影片重新换回物理内存，这不是我们能引起的阻塞，是因为缺页无法写上去导致的。

最后一项检查针对特定资源是否有合法权限。在老版本的Linux内核中，需要超级用户权限的系统调用才可以通过调用suser()函数这个标准动作来完成检查的。这个函数只能检查用户是否为超级用户；现在他已经被一个更细粒度的权能机制代替。就是无权限访问啥的就是suser()函数给出的。但只能检查用户是否为超级用户，这太粗糙了。所以引入了新的函数代替

新的系统允许检查针对特定资源的特殊权限。调用者可以使用capable()函数来检查是否有权能对指定的资源进行操作，如果他返回非零值，调用者就有权进行操作，返回零就无权进行操作。也就是说从检查用户级别变成了检查用户是否有权对特定资源访问。

举个例子，capable(CAP_SYS_NICE)可以检查调用者是否有权改变其他进程的nice值。默认情况下超级用户的进程拥有所有的权力，而非超级用户没有权利，下面是reboot()系统调用，注意，第一步是如何确保调用进程具有CAP_SYS_REBOOT权能。如果那样一个条件语句被删除，任何进程都可以启动系统了。

```c
SYSCALL_DEFINE4(reboot,int ,magic1,int,magic2,unsigned int,cmd,void __user*,arg)
{
char buffer[256];
	//我们只信任启动系统的系统管理员
    if(!capable(CAP_SYS_BOOT))
        return -EPERM;
    //但是即使你是root还不够，还需要magic参数
    if(magic1 != LINUX_REBOOT_mAGIC1 || (
        magic2 != LINUX_REBOOT_MAGIC2 && 
        magic2 !=LINUX_REBOOT_MAGIC2A && 
        magic2 != LINUX_REBOOT_MAGIC2B && 
        magic2 != LINUX_REBOOT_MAGIC2C))
        return -EINAL;
    //当未设置pm_power_off时，请不要试图让power_off的代码看起来像是可以停机，而应该采用更简单的方式
    if((cmd == LINU_REBOOT_CMD_POWER_OFF) && ! pm_power_off)
        cmd = LINUX_REBOOT_CMD_HALT;
    lock_kernel();
    switch(cmd){
            case LINUX_REBOOT_CMD_RESTART;
            		kernel_restart(NULL);
            		break;
            ...
    }
}
```

上述的例子是为了证明条件语句至关重要，关系到权限问题。

参考<linux/capability.h>，其中包含一份所有这些权能和其对应的权限的列表。

### 5.6系统调用上下文

----

一提上下文，让我想起了中断和进程上下文，学到现在内核态、内核空间都能理解了，上下文分为寄存器、用户、内核上下文。其实就是寄存器、用户进程栈、内核栈和进程描述符。至于是否可休眠的问题，这个和哪个硬件有关呢

----

在第三章中曾经讨论过，内核在执行系统调用的时候处于进程上下文，current指针指向当前任务，即引发系统调用的那个进程。

在进程上下文中，内核可以休眠（比如在系统调用阻塞或显式调用schedule()的时候）并且可以被抢占。这两点很重要。这两点很重要。首先能够休眠说明系统调用可以使用内核提供的绝大部分功能。休眠的能力会给内核编程带来极大便利。在进程上下文中能够被抢占说明就像用户空间的进程一样，当前的进程同样可以被其他进程抢占。因为新的进程可以使用相同的系统调用。所以必须小心，保证该系统调用是可重入的，（突然蹦出一个熟悉的名词，可重入表示函数被多个执行流并发或者并行的执行，但仍能保证每一次的结果可预测）当然，这也是在对称多处理中必须同样关心的问题。

当系统调用返回的时候，控制权仍然在system_call()系统调用处理函数中，毕竟是通过它调用的系统调用，返回的时候是返回到该函数中，最终通过此函数负责切换到用户空间，并让用户进程继续执行下去。此函数就是中断处理函数，现在执行完了自然要切换回原来的用户空间进程上下文。

#### 5.6.1绑定一个系统调用的最后步骤

当编写完一个系统调用后，把它注册成一个正式的系统调用是件琐碎的工作：

1. 首先，在系统调用表的最后加入一个表项。每种支持该系统调用的硬件体系都必须做这样的工作（大部分的系统调用都针对所有的体系结构），从0开始算起，系统调用在该表中的位置就是他的系统调用好，比如第10个系统调用分配到的系统调用号为9
2. 对于所支持的各种体系结构，系统调用号必须定义于<asm/unistd.h>，在用户空间有unistd.h文件。
3. 系统调用必须被编译进内核映像（不能被编译成模块）。只要把它放进kernel/目录下的一个相关文件中即可，比如sys.c包含了各种各样的系统调用。

让我们通过一个虚构的系统调用foo()来仔细观察一下这些步骤。首先，我们要把sys_foo加入到系统调用表中，对于大多数体系结构来说，该表位于entry.s文件中。

```c
ENTRY(sys_call_table)
	.long sys_restart_syscall//0
	...
	.long sys_open//5
    ...
    .long sys_foo//338，将新的系统调用加到这个表的末尾
```

虽然没有明确地指定编号，但我们加入的这个系统调用被按照次序分配给了338这个系统调用号。对于每种需要支持的体系结构，我们都必须将自己的系统调用加入到其系统调用表中去。**每种体系结构不需要对应相同的系统调用号**，就是这个原因导致APP移植不同平台需要进行修改。系统调用号是专属于体系结构ABI（应用程序二进制接口）的部分，通常，你需要让系统调用适应每种体系结构。你可以注意下，每隔5个表项就加入一个调用号注释的习惯，这可以在查找系统调用对应的调用号时提供方便。

接下来，我们把系统调用号加入到<asm/unistd.h>中，格式如下：

```c
#define __NR_restart_syscall 0
#define __NR_exit			1
...
#define __NR_foo			338
```

最后，我们来实现foo()系统调用。无论何种配置，系统调用都必须编译到核心的内核映像中去，所以在这个例子中，把系统调用的实现放到kernel/sys.c中，也可以将其放到与其功能联系最紧密的代码中去，假如他的功能和调度相关，也可以将其放到kernel/sched.c中去。

```c
#include <asm/page.h>
asmlinkage long sys_foo(void)//asmlinkage限定词，仅在栈中提取其函数参数，这里指的就是内核栈了
{
    return THREAD_SIZE;
}
```

这样就可以启动内核并在用户空间调用foo()系统调用了。

#### 5.6.2从用户空间访问系统调用

通常，系统调用靠C库支持，用户程序通过包含标准头文件与C库链接，就可以使用系统调用，但如果仅仅写出系统调用，glibc库恐怕并不提供支持。

庆幸的是，Linux本身提供了一组宏，用于直接对系统调用进行访问。他会设置好寄存器并调用陷入指令。这些宏是_syscalln()，其中n的范围从0到6，代表需要传递给系统调用的参数个数。（很显然，这里必须设置好寄存器，不然系统调用号和实参如何实现陷入内核空间）

之前就在思考了，系统调用是在内核中定义的，我们在用户层实际上就是引发软中断从而陷入内核层，并传递系统调用号。所以我们在用户层编写的系统调用内部实现其实就是Linux设置的一组宏来设置好寄存器并调用陷入指令。

```c
long open(const char *filename,int flags,int mode)
{
#define NR_open 5
_syscall3(long ,open,const char * filename,int flas,int mode);//open是long类型的其实就是传入的系统调用号5
}
```

这样就可以在应用程序直接使用open()，至此，从标准C库->用户层系统调用->sys_call函数->sys_xx系统调用，整个流程清晰了。

对于每个宏来说，都有2+2 * n个参数。第一个参数对应着系统调用的返回值类型。第二个参数是系统调用的名称。再以后是按照系统调用参数的顺序排列的每个参数的类型和名称，

_NR_open在<asm/unistd.h>里定义，是系统调用号，该宏会被扩展成为**内嵌汇编**的C函数（这里和我想的一样，内嵌汇编实现对寄存器的操作以及执行SWI指令）；由汇编语言执行前面内容中所讨论的步骤，将系统调用号和参数压入寄存器并触发软中断来陷入内核，调用open()系统调用直接把上面的宏放置在应用程序中即可。

写一个宏来使用前面编写的foo()系统调用，然后再写出测试代码炫耀下我们所作的努力。

#### 5.6.3为什么不通过系统调用的方式实现

前面的内容已经告诉大家，建立一个新的系统调用非常简单，步骤都已经列出来了，但是绝不提倡这么做，通常会有更好的办法来代替新建一个系统调用以作实现。让我们看看采用系统调用作为实现方式的利弊和代替的方法。

建立一个新的系统调用的好处：

- 系统调用创建容易且使用 方便
- Linux系统调用的高性能显而易见。

问题是：

- 你需要一个系统调用号，这需要一个内核在处于开发版本的时候由官方分配给你。
- 系统调用被加入稳定内核后就被固化了，为了避免应用程序的崩溃，他的接口不允许作改动。
- 需要将系统调用分别注册到每个需要支持的体系结构中，因为每个体系结构的系统调用表都是专属的。需要注册到<entry.S>以及<asm/unistd.h>两个文件中注册系统调用表和系统调用号，然后需要在<kernel/sys.c>中实现系统调用的实现。
- 在脚本中不容易调用系统调用，也不能从文件系统直接访问系统调用。这需要我们实现应用层的系统调用的实现，调用_syscallx函数，这样才能进入到内核态
- 由于需要系统调用号在实现用户层系统调用的时候传入的系统调用号，因此在主内核树之外是很难维护和使用系统调用的。也就是在应用层
- 如果仅仅进行简单的信息交换，系统调用就大材小用了。

替代方法：

- 实现一个设备节点，对其实现read()和write()，使用ioctl()对特定的设置进行过操作或者对特定的信息进行检索。file_operation操作结构体我们实现的在驱动模块中，确实不属于系统调用，借此实现访问内核确实可以实现。
- 像信号量这样的某些接口，可以用文件描述符来表示，因此也就可以按上述方式对其进行操作。这里没听过
- 把增加的信息作为一个文件放在sysfs的合适位置。这个好理解，sysfs虚拟文件系统就是为了将内核空间中的某些属性变成在用户空间中的接口，以此来显示和操作

对于许多接口来说，系统调用都被视为正确的解决之道。但Linux系统尽量避免每出现一种新的抽象就简单的加入一个新的系统调用。这会使得他的系统调用接口简洁的令人叹为观止，也就避免了许多反对意见。新系统调用增添频率很低页反映出Linux是一个相对较为稳定并且功能已经较为完善的操作系统。

### 5.7小结

在本章，描述了系统调用到底是什么，他们与库函数和应用程序接口之间是怎样的关系，然后我们考察了Linux内核如何实现系统调用，以及执行系统调用的连锁反应：陷入内核、传递系统调用号和参数、执行正确的系统调用函数、并把返回值带回用户空间。

然后，我们讨论了如何增加系统调用，并提供从用户空间调用系统调用的简单例子，增加一个新的系统调用的过程的就是系统调用的实现过程。本书的其余部分讨论了编写规范的、最优化的、安全的系统调用所遵循的概念和内核接口规范。

最后，我们通过讨论实现系统调用的优缺点以及列举其替代方案的形式对全章内容进行了总结。

### 总结

本章对我来说是一个解惑的过程，其实就是库函数->用户层系统调用->_syscall->通过软中断陷入内核空间，传入系统调用号以及参数->system_call函数作为系统调用处理函数->通过系统调用号查阅entry.S文件，这就是系统调用表从而找到对应的系统调用->sys_xxx函数。

这个过程让我明白了是如何陷入内核态的，以及实现系统调用的组成部分。

## 第六章内核数据结构

本章将介绍几种Linux内核常用的内建数据结构。和其他大型项目一样，Linux内核实现了这些通用数据结构，而且提倡大家在开发时多用，内核开发者应该尽可能地使用这些数据结构，而不要搞自作主张的山寨方法。在下面的内容中，我们讲述这些通用数据结构中最有用的几个：

- 链表
- 队列
- 映射
- 二叉树

本章最后还要讨论算法复杂度，以及何种规模的算法或数据结构可以相对容易地支持更大的输入集合。

### 6.1链表

链表是Linux内核中最简单、最普通的数据结构，链表是一种存放和操作可变数量元素（元素常称为节点）的数据结构。链表和静态数组的不同之处在于，他所包含的元素都是动态创建并插入链表的。在编译时不必知道具体需要创建多少个元素。另外也因为链表中每个元素的创建时间 各不相同，所以他们在内存中无须占用连续内存区。正是因为离散的内存分布所以在申请的时候不需要一次性申请连续大段的内存，这就是动态插入的原因。正是因为元素不连续地存放，所以各元素需要通过某种方式被连接在一起，于是每个元素都包含一个指向下一个元素的指针，当有元素加入链表或者从链表中删除元素时，简单调整指向下一个节点的指针就可以了。

其实学这本书还可以复习数据结构、操作系统，学qt复习c++，能看得出来接下来的复习是一个系统的复习。

#### 6.1.1单向链表和双向链表

可以用一种最简单的数据结构来表示这样一个链表：

```c
struct list_element{/*链表中的元素*/
void *data;//有效数据
struct list_element *next;//指向下一个元素的指针
}
```

![1670290372131](C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1670290372131.png)

有些链表中，每个元素还包含一个指向前一个元素的指针，因为他们可以同时向前和向后相互连接，所以这种链表被称为双向链表。上面结构体的哪种只能向后连接的链表被称作单向链表。

表示双向链表的一种数据结构如下：

```c
struct list_element{
void *data;
struct list_element *next;
struct list_element *prev;
}
```

![1670290533778](C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1670290533778.png)

#### 6.1.2环形链表

通常情况下，因为链表中最后一个元素不再有下一个元素，所以将链表尾元素中的向后指针设置为NULL，以此表明它是链表中的最后一个元素。但在有些链表中，末尾元素并不指向特殊值，相反，它指向链表的首元素，这种链表印尼为首尾相连，所以被称为是环形链表。环形链表页存在双向链表和单向链表两种形式。在环形双向链表中，首届点的向前指针指向尾节点。

![1670290687044](C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1670290687044.png)

![1670290698536](C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1670290698536.png)

因为唤醒双向链表提供了最大的灵活性，所以Linux内核的标准链表就是采用环形双向链表形式实现的，所以在内核中提到链表只要没有特别说明，就是环形双向链表。

#### 6.1.3沿链表移动

沿链表移动只能说线性移动，不能实现随机访问，先访问某个元素，然后掩盖元素的向后指针访问下一个元素，不断重复这个过程，就可以沿链表向后移动了。这是一种最简单的沿链表移动方法，也是最适合访问链表的方法。如果需要随机访问数组，一般不使用链表，毕竟离散内存分配我们无法自加地址找到下一个元素。使用链表存放数据的理想情况是需要遍历所有数据或者需要动态加入和删除数据时。

有时，首元素会用一个特殊指针表示，该指针称为头指针，利用头指针可方便、快速地找到链表的起始端。不然链表就没有头尾区别了，在非环形链表中，向后指针指向NULL的元素是尾元素，而在环形链表里向后指针指向头元素的元素是尾元素。遍历一个链表需要线性地访问从第一个元素到最后一个元素之间的所有元素。对于双向链表来说，也可以反向遍历链表，可以从最后一个元素线性访问到第一个元素。当然还可以从链表中的指定元素开始向前和向后访问数个元素，并不一定要访问整个链表。

#### 6.1.4Linux内核中的实现

之前介绍的都属于复习了，这里开始才是新的内容。相比普遍的链表实现方式，Linux内核的实现可以说独树一帜。回忆早先提到的数据比如一个struct，通过在内部添加一个指向数据的next（或者previous）节点指针，才能串联在链表中。比如，假定我们有一个fox数据结构来描述犬科动物中的一员。

```c
struct fox{
unsigned long tail_length;//尾巴长度以厘米为单位
unsigned long weight;//重量，以千克为单位
bool is_fantastic;
}
```

一下子想明白了，原来如此，之前看到的那些结构体其实只要内部有指针指向下一个相同类型的结构体就可以是一个链表了，看那些其他的成员其实都属于该结点的数据。

存储这个结构到链表中的通常方法是在数据结构中嵌入一个链表指针，比如：

```c
struct fox{
unsigned long tail_length;
unsigned long weight;
bool is_fantastic;
struct fox *next;//之前理解为结构体嵌套真的是太不对了，很明显是数据结构
struct fox *prev;
}
```

Linux内核方式与众不同，他不是将数据结构塞入链表，而是将链表节点塞入数据结构。

##### 1.链表数据结构

在过去，内核中有许多链表的实现，该选一个既简单又高效的链表统一他们。在2.1内核开发系列中，首次引入了官方内核链表实现。从此内核中的所有链表现在都使用管发的链表实现了，千万不要再自己造轮子了。

链表代码在头文件<linux/list.h>实际的路径是include/linux/list.h

```c
struct list_head{
struct list_head *next;
struct list_head *prev;
}
```

就是这个list_head真的是太常用了，但是我不理解这里的结构体是什么意思，看上去的话确实指向了相同结构体类型的指针，但是这里面并没有数据，就是一个空链表

其实关键在于理解list_head结构是如何使用的。

```c
struct fox{
unsigned long tail_length;
unsigned long weight;
bool is_fantastic;
struct list_head list;//这里是所有fox结构体组成的链表，这里就是结构体嵌套结构体了，而在这个结构体中就存放了上下节点的地址，举个例字就是原本是手机是特定的类型，不同链表的节点类型都不一样，可以查看数据，也可以联系别人，而现在把这个手机的功能只保留到联系别人变成一个制式手机，并把这个手机放到工具箱中。这个工具箱就是fox，手机就是list_head仅用来通信，现在就是数据归数据，数据结构归数据结构
}
```

上述结构中，fox中的list.next指向下一个元素，list.prev指向前一个元素。现在链表已经能用了，但显然还不够方便，因此内核又提供了一组链表操作例程。比如list_add()方法加入一个新节点到链表中。但是，这些方法都有一个统一的特点：他们只接受list_head结构作为参数，毕竟这是要操作链表而不是整个fox结构体，这样的话确实方便插入和删除时要传入的结构体 类型统一了。

想明白了，链表存入数据结构中与链表包含数据结构的好处是：**只需要传入list_head类型节点即可，不需要关心其父结构类型，直接通过下面的宏找到即可，而不用我们传入该特定的结构体。**

使用宏**container_of**()我们可以很方便地从链表指针找到父结构中包含的任何变量，这就实现了查看fox中数据的功能。这是因为在C语言中，**一个给定结构中的变量偏移在编译时地址就被ABI固定下来**。

---

```c
#define offsetof(TYPE, MEMBER) ((size_t) &((TYPE *)0)->MEMBER)
#define container_of(ptr, type, member) ({             \
const typeof( ((type *)0)->member ) *__mptr = (ptr);   \
(type *)( (char *)__mptr - offsetof(type,member) );})
```

传入的三个参数分别是变量地址，type是父结构的类型，member为变量名称。用到了零值指针，将零指针强转成type类型的父结构并指向成员member，再用typeof获取member的类型并创建一个指向member的临时指针赋值为ptr地址。之后利用该地址减去该成员在结构体中的偏移从而获得结构体起始地址。

获得结构体中成员偏移地址同样是利用零值指针创建父结构并指向member，此时父结构起始地址为0，那么member取地址就是成员偏移，再让list地址减去成员偏移得到结构体起始地址。根据list地址来锁定是哪个结构体，根据父结构类型+list成员找到偏移地址。（宏的第一行无意义吧，最后还是(char *）ptr - (char *)member有意义的，如果传入的是ptr++，那么就会增加两次，这里相当于取临时变量，类似于函数，为什么要分成形参实参两种，直接把实参传入得了，就是防止增加两次，形参作为局部变量，这也是值传递的特点。）

花括号的值为最后一条语句值。container_of(&list,struct xxx,list)

其实这就是面向对象的思想，如果是c++有办法找到成员变量的对象，但是c不支持，只能根据成员地址查找。

-----

ABI（Application Binary Interface）：应用程序二进制接口，描述了应用程

序和操作系统之间，一个应用和它的库之间，或者应用的组成部分之间的低接口。 不理解

```c
#define container_of(ptr,type,member)({
const typeof(((type *)0)->member)*__mptr = (ptr);
(type *)((char *)__mptr-offsetof(type,member));
})
```

这个宏看不懂，反正知道这是用来查看父结构成员即可。

使用container_of()宏，我们定义一个简单的函数便可返回包含list_head的父类型结构体：

```c
#define list_entry(ptr,type,member) \//这里就是大名鼎鼎的list_entry函数啊，这个函数也很常见，原来就是用来找到链表的父结构，主要是之前确实没想到将数据结构放入结构体中的操作
container_of(ptr,type,member);
```

依靠list_entry()方法，内核提供了创建、操作以及其他链表管理的各种例程-所有这些方法都不需要知道list_head所嵌入对象的数据结构。这里的数据结构指的就是fox父结构类型。传入的type是谁的类型。

##### 2.定义一个链表

正如看到的：list_head本身其实并没有意义，他需要被嵌入到你自己的数据结构中才能生效，链表需要在使用前初始化，因为多数元素都是动态创建的，所以最常见的方式是在运行时初始化链表。

```c
struct fox *red_fox;
ref_fox = kmalloc(sizeof(*ref_fox),GFP_KERNEL);//创建节点
red_fox->tail_length = 40;
red_fox->weight = 6;
red_fox->is_fantastic = false;
INIT_LIST_HEAD(&red_fox->list);//将fox结构体中的链表节点加入到链表头中
```

如果一个结构在编译期静态创建，而你需要在其中给出一个链表的直接引用，下面是最简方式：

```c
struct fox red_fox ={
.tail_length = 40,
.weight = 6,
    .list = LIST_HEAD_INIT(red_fox.list),
};
```

##### 3.链表头

前面我们展示了如何把一个现有的数据结构就是fox改造成链表。

简单修改上述代码，我们的结构便可以被内核链表例程，但是在可以使用这些例程前，需要一个标准的索引指针指向整个链表，即链表的头指针。

内核链表实现中最杰出的特性就是：我们的fox节点都是无差别的，每一个都包含一个list_head结构体，于是我们可以从任何一个节点起遍历链表，知道我们看到所有节点。这种方式确实很优美，不过有时确实也需要一个特殊指针索引到整个链表，而不从一个链表节点触发。这个特殊的索引节点事实上也就是一个常规的list_head:

```c
static LIST_HEAD(fox_list);//创建一个链表头
```

该函数定义并初始化了一个名为fox_list的链表例程，这些例程中的大多数都只接受一个或者两个参数：头结点或者头结点加上一个特殊链表节点。下面具体看看这些操作例程。

#### 6.1.5操作链表

内核提供了一组函数来操作链表，这些函数都要使用一个或多个list_head结构体指针作为参数，因为函数都是用C语言以内联函数形式实现的，所以他们原型在文件<linux/list.h>中。

所有这些函数的复杂度都为O(1)，这意味着，无论这些函数操作的链表大型如何，无论他们得到的参数如何，他们都在恒定时间内完成，比如，不管是对于包含3个元素的链表还是对于包含3000个元素的链表，从链表中删除一项或者加入一项花费的时间都是相同的。不是需要遍历的时间吗。

##### 1.向链表中增加一个节点

给链表增加一个节点：

```c
list_add(struct list_head *new,struct list_head *head)
```

该函数向指定链表的head节点后插入new节点。因为链表是循环的，而且通常没有首尾节点的概念，所以你可以把任何一个节点当成head。如果把最后一个节点当成head的话可以用来实现一个栈，先进后出。不是没有首尾概念，怎么会有前后的概念呢

假设创建一个新的struct fox，并把它加入fox_list，那么就是这么做：

```c
list_add(&f->list,&fox_list);//fox_list是创建的第一个头节点，这个头节点可没有父节点就是用来指向我们初始化的list节点，这个节点是有父节点f的，这里实现的指针是fox_list的后指针和list的前指针
```

把节点增加到链表尾：

```
list_add_tail(struct list_head *new,struct list_head *head)
```

该函数向指定链表的head节点前插入new节点，和list_add函数类似，因为链表是环形的，所以可以把任何一个节点当作head。如果把第一个元素当作head的话，可以实现一个队列，队列是需要连续地址的。突然想起了静态链表了，申请一个连续大段的内存空间，然后指针指向的是这个内存空间上离散的位置。

##### 2.从链表中删除一个节点

在链表中增加一个节点后，从中删除一个节点是另一个最重要的操作。从链表中删除一个节点，调用list_del();

```c
list_del(struct list_head *entry);
```

该函数从链表中删除entry元素，注意，该操作并不会释放entry或者释放包含entry的数据结构体所占用的内存；**该函数仅仅是将entry元素从链表中移走**，所以该函数被调用后，通常还需要再撤销包含entry的数据结构体和其中的entry项。

例如，为了删除fox节点，回到前面增加节点的fox_list:

```c
list_del(&f->list)
```

注意，该函数并没有接受fox_list作为输入参数，只是接受一个特定的节点，并修改其前后节点的指针，这样给定给的节点就从链表中删除了，代码的实现确实具有启发性，我只需要传入要删除的节点即可，但实际上要改动的恰恰是这个节点的前后节点的指针

```c
static inline void __list_del(struct list_head *prev,struct list_head *next)
{
next->prev = prev;//让a所指向的指针由b变成c
prev->next = next;
}
static inline void list_del(struct list_head *entry)
{
__list_del(entry->prev,entry->next);
}
```

从链表中删除一个节点并对其重新初始化：

```c
list_del_init();
list_del_init(struct list_head *entry);
```

此函数除了还需要再次初始化entry之外，其他和list_del()类似，我感觉就是将entry中的前后指针执行NULL，其他和list_del函数类似，虽然链表不再需要entry项，但是还可以再次使用包含entry的数据结构体。

##### 3.移动和合并链表节点

把节点从一个链表移到另一个链表：

```c
list_move(struct list_head *list,struct list_head *head)
```

该函数从一个链表中移除list项，然后将其加入到另一个链表的head节点后面。其实我们传入的是要操作的节点，不需要遍历啥的这就是O(1)的原因了。

把节点从一个链表移到另一个链表的尾部：

```c
list_move_tail(struct list_head *list,struct list_head *head);
```

该函数和list_move()函数一样，唯一的不同是将list项插入到head项前。这就是从末尾插入的方式是插入到head前面。

检查链表是否为空：

```
list_empty(struct list_head *head)
```

如果指定的链表为空，该函数返回非0值；否则返回0

把两个未连接的链表合并在一起：

```c
list_splice(struct list_head *list,struct list_head *head)
```

该函数合并两个链表，他将list指向的链表插入到指定链表的head元素后面。但这不是一个环形双向链表吗，另外两边也需要连接起来。内部实现的吗

把两个未连接的链表合并在一起，并重新初始化原来的链表：

```c
list_splice_init(struct list_head *list,struct list_head *head)
```

该函数和list_splice()函数一样，唯一的不同是由list指向的链表要被重新初始化。也不知道哪里初始化。

##### 节约两次提领

如果已经获得了next和prev指针，可以直接调用内部链表函数，从而剩下一点时间，其实之前的函数就是找到next和prev指针，再去调用内部函数而已。内部函数和他们的外部包装函数同名，仅仅在前面加了两条下划线，比如可以调用__list_del(prev,next)函数代替调用list_del(list)函数。具体看<linux/list.h>中具体的接口。

#### 6.1.6遍历链表

现在，你已经知道了如何在内核中声明、初始化和操作一个链表，这很了不起，但如果无法访问自己的数据，这些没有意义。链表仅仅是个能够包含重要数据的容器：我们必须利用该链表移动并访问包含我们数据的结构体。幸好，内核为我们提供了一组非常棒的接口，可以用来遍历链表和引用链表中的数据结构体。

和链表操作函数不同，遍历链表的复杂度为O(n)，n是链表所包含的元素数目。

##### 1.基本方法

遍历链表最简单的方法是使用list_for_each()宏，这个宏也经常能看到，该宏使用两个list_head类型的参数，第一个参数用来指向当前项，这是一个必须要提供的临时变量，第二个参数是需要遍历的链表的以头节点形式存在的list_head也就是链表头。每次遍历中，第一个参数在链表中不断移动指向下一个元素，直到链表中的所有元素都被访问为止。

```c
struct list_head *p;
list_for_each(p,list){
//这里就是循环体内部了，p就是每次遍历到的节点
}
```

好了，实话实说，其实一个指向链表结构的指针通常是无用的，毕竟只有链表没啥用，数据并不在里面，所需要的指向包含list_head的结构体的指针。比如前面fox结构体的例子，我们需要的是指向每个fox的指针，可以使用前面讨论的list_entry()宏，来获得包含给定list_head的数据结构。比如：

```c
struct  list_head *p;
struct fox *f;
list_for_each(p,&fox_list){
f = list_entry(p,struct fox,list);//p表示每次便利的list_head类型的节点，struct fox表示包含链表的结构体类型，list表示父结构体中的链表变量名称。
}
```

##### 2.可用的方法

前面的方法虽然确实展示了list_head节点的功效，但并不优美，而且也不够灵活。所以多数内核代码采用list_for_each_entry()宏遍历链表，这样就可以实现自动遍历每一个包含链表节点的结构体了，该宏内部也使用list_entry()宏，但简化了遍历过程：

```c
list_for_each_entry(pos,head,member);
```

这里的pos就是指向包含list_head节点对象的指针，可将它看作是list_entry宏的返回值。head是一个指向头节点的指针，即遍历开始位置，在我们前面例子中，fox_list.member是pos中list_head结构的变量名。确实没听懂，下面的代码片断展示了如何重写前面的list_for_each()，遍历所有fox节点：

```c
strcut fox *f;
list_for_each_entry(f,&fox_list,list){
    //这里的f表示是fox结构体类型的变量，fox_list依旧是链表头，至于这个list是在fox结构体中的struct list_head list变量名称。
}
```

循环中的每一个遍历，f都指向链表的父节点。看看实际例子。

```c
static struct inotify_watch *inode_find_handle(struct inode *inode,struct inotify_handle *ih){
strcut inotify_watch *watch;/*这是包含链表的结构体*/
list_for_each_entry(watch,&inode->inotify_watches,i_list){
    if(watch->ih == ih)//判断下每次遍历的父结构中的ih成员是否是要找的，是的话直接返回该父结构，&inode->inotify_watches这是我们要遍历的链表头，遍历开始的节点，inode是什么
        return watch;
}
    return NULL;
}
```

该函数遍历了inode->inotify_watches链表中的所有项，每个项的类型都是struct inotify_watch，list_head在结构中被命名为i_list，（struct list_head i_list）。循环中的每一个遍历，watch都指向链表的父结构。该函数的目的在于：在inode结构串联起来的inotify_watches链表中，搜寻父结构中的inotify_handle与所提供的句柄相匹配的inotify_watch项

##### 3.反向遍历链表

宏list_for_each_entry_reverse()的工作和list_for_each_entry()类似，不同点在于它是反向遍历链表的。也就是说，不再是沿着next指针向前遍历，而是沿着prev指针向后遍历，我之前以为反了。其用法和list_for_each_entry()相同：

```c
list_for_each_entry_reverse(pos,head,member)
```

很多原因会需要反向遍历链表。其中一个是性能原因，如果知道要寻找的节点最可能在搜索的起始点的前面，那么反向搜索会更快，第二个原因是顺序很重要，比如，如果使用链表实现堆栈，那么需要从尾部向前遍历才能达到先进先出FIFO的原则。如果没有确切的反向遍历的原因，就使用list_for_each_entry()宏吧。

##### 4.遍历的同时删除

标准的链表遍历方法在你遍历链表的同时要想删除节点是不行的，因为标准的链表方法建立在操作不会改变链表项这一假设上，所以如果当前项在遍历循环中被删除，那么接下来遍历就无法获得next/prev指针了。如果当前项被删除的话，当前项的指针会被初始化，那么就无法获得当前项的指针来指向下一个节点了。这其实是循环处理中的一个常见范式。

开发人员通过在潜在的删除操作之前存储next（或者previous）指针到一个临时变量中，以便能执行删除操作。好在Linux内核提供了例程处理这种情况：

```c
list_for_each_entry_safe(pos,next,head,member)
```

你可以按照list_for_each_entry()宏的方式使用上述例程，只是需要提供next指针，next指针和pos是同样的类型，你确定？pos表示的是包含链表的父结构，next指向的应该是链表节点。list_for_each_entry_safe()启用next指针来将下一项存进表中，以使得能安全删除当前项。再次看看inotify的例子

```c
void inotify_inode_is_dead(struct inode *inode)
{
    struct inotify_watch *watch,*next;//这里还真的是同一个类型啊
    mutex_lock(&inode->inotify_mutex);
    list_for_each_entry_safe(watch,next,&inode->inotify_watches,i_list){
        struct inotify_handle *ih = watch->ih;
        mutex_lock(&ih->mutex);
        inotify_remove_watch_locked(ih,watch);//删除watch
        mutex_unlock(&ih->mutex);
    }
    mutex_unlock(&inode->inotify_mutex);
}
```

该函数遍历并删除inotify_watches链表种的所有项，如果使用了标准的list_for_each_entry()，那么上述代码会造成使用在释放后的错误，因为在移向链表中下一项时，需要访问当前项watch，但这是他已经被撤销了。

如果需要在反向遍历链表的同时删除它，那么内核提供了list_for_each_entry_safe_reverse()宏完成任务：

```c
list_for_each_entry_safe_reverse(pos,n,head,member)
```

这种安全版本只能保护你在循环体种从链表中删除数据。如果有可能从别的地方并发进行删除，需要锁定链表。

##### 5.其他链表方法

Linux提供了很多链表操作方法，几乎是你所能想到的所有访问和操作链表方法，所有这些方法都在头文件<linux/list.h>中找到。

一个链表就这么学完了，介绍了Linux中特有的结构，将链表存入结构体中，以及大量的操作宏

### 6.2队列

任何操作系统内核都少不了一种编程模型：生产者和消费者。在该模型中，生产者创造数据（比如说需要读取的错误信息或者需要处理的网络包），而消费者反过来，读取消息和处理包，或者以其他方式消费这些数据。实现该模型的最简单的方法无非是使用队列。生产者将数据推进队列，然后消费者从队列中摘取数据，消费者获取数据的顺序和推入队列的顺序一致，也就是说，第一个进入队列的数据一定是第一个离开队列的，正是因为这个原因，队列也称为FIFO。先进先出的缩写。

![1670326971223](C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1670326971223.png)

Linux内核通用队列实现称为kfifo，终于讲到这里了，iio缓冲区就是由kfifo控制的。它实现在文件kernel/kfifo.c中，声明在文件<linux/kfifo.h>中。本节讨论的是字2.6.33以后更新的API，使用方法和2.6.33前的内核稍有不同。所以在使用前请仔细检查文件<linux/kfifo.h>

#### 6.2.1kfifo

Linux的kfifo和多数其他队列实现类似，提供了两个主要操作：enqueue（入队列）和dequeue(出队列)。kfifo对象维护了两个偏移量：入口偏移和出口偏移。入口偏移是指下一次入队列时的位置，出口偏移是指下一次出队列时的位置。出口偏移总是小于等于入口偏移，否则无意义，因为那样说明要出队列的元素根本还没有入队列。这两个偏移都是从入口作为起始位的。

enqueue操作拷贝数据到队列中的入口偏移位置。当上述动作完成后，入口偏移随之加上推入的元素数目。dequeue操作从队列中出口偏移处拷贝数据，当上述动作完成后，出口偏移随之减去摘取的元素数目。当出口偏移等于入口偏移时，说明队列空了：在新数据被推入前，不可再摘取任何数据了。当入口偏移等于队列长度时，说明在队列重置前，不可再有新数据推入队列。

#### 6.2.2创建队列

使用kfifo前，首先必须对他进行定义和初始化。和多数内核对象一样，有动态和静态方法供你选择，动态方法更为普遍：

```c
int kfifo_alloc(struct kfifo *fifo,unsigned int size,gfp_t gfp_mask);
```

该函数创建并且初始化一个大小为size的kfifo，内核使用gfp_mask标识分配队列，如果成功kfifo_alloc()返回0；错误则返回一个负数错误码，下面是实例：

```c
struct kfifo fifo;
int ret;
ret = kfifo_alloc(&fifo,PAGE_SIZE,GFP_KERNEL);
if(ret)
    return ret;
//此时fifo代表一个大小为PAGE_SIZE的队列
```

如果想要自己分配缓冲，可以调用：

```c
void kfifo_init(struct kfifo *fifo,void *buffer,unsigned int size);//特点是内存空间都已经指定为buffer了
```

该函数创建并初始化一个kfifo对象，他将使用由buffer指向的size字节大小内存。对于kfifo_alloc()和kfifo_init()，静态创建就是在栈上创建吧

静态生命kfifo更简单，但不大常用：

```c
DECLARE_KFIFO(name,size);
INIT_KFIFO(name);
```

上述方法会创建一个名称为name，大小为size的kfifo对象。

#### 6.2.3推入队列数据

当你的kfifo对象创建和初始化后，推入数据到队列需要通过kfifo_in()方法完成：

```c
unsigned int kfifo_in(struct kfifo*fifo,const void*from,unsigned int len);
```

该函数把from指针所指的len字节数据拷贝到fifo所指的队列中，如果成功，则返回推入数据的字节大小，如果队列中的空闲字节小于len，则该函数值最多可拷贝队列可用空间那么多的数据，这样的话返回值可能小于len，甚至会返回0，意味着没有任何数据被推入，这就不是变长数组了。

#### 6.2.4摘取队列数据

推入数据使用函数kfifo_in()，摘取数据则需要通过函数kfifo_out()完成：

```c
unsigned int kfifo_out(struct kfifo*fifo,void *to,unsigned int len);
```

该函数从fifo所指向的队列中拷贝出长度为len字节的数据到to所指的缓冲中。如果成功，该函数则返回拷贝的数据长度。如果队列中数据大小小于len，则该函数拷贝出的数据必然小于需要的数据大小。

当数据被摘取后，数据就不再存在于队列中，这是队列操作的常用方式。不过如果仅仅想偷窥队列中的数据，而不是真想删除它，你可以使用kfifo_out_peek()方法：

```c
unsigned int kfifo_out_peek(struct kfifo*fifo,void *to,unsigned int len,unsigned offset);
```

该函数和kfifo_out()类似，但出口偏移不增加，而且摘取的数据仍然可被下次kfifo_out获取，参数offset指向队列中的索引位置，如果该参数为0，则读队列头，这和kfifo_out()无异，就是从偏移量offset开始读取len长度的字节。

#### 6.2.5获取队列长度

若想获得用于存储kfifo队列的空间的总体大小，可调用方法kfifo_size():

```c
static inline unsigned int kfifo_size(struct kfifo*fifo);
```

另一个内核命名不佳的例子，kfifo_len()方法返回kfifo队列中已推入的数据大小：

```c
static inline unsigned int kfifo_len(struct kfifo *fifo)
```

如果想要得到kfifo队列中还有多少可用空间，则要调用方法：

```c
static inline unsigned int kfifo_avail(struct kfifo*fifo)
```

最后两个方法是kfifo_is_empty()和kfifo_is_full()如果给定的kfifo分别是空或者满，返回非0值，如果返回0，则相反。

```c
static inline int kfifo_is_empty(struct kfifo*fifo)
static inline int kfifo_is_full(struct kfifo *fifo)
```

#### 6.2.6重置和撤销队列

如果重置kfifo，意味着抛弃所有队列中的内容，调用kfifo_reset();

```c
static inline void kfifo_reset(struct kfifo *fifo)
```

撤销一个使用kfifo_alloc()分配的队列，调用kfifo_free()，因为是在堆上手动创建的。

```c
void kfifo_free(Struct kfifo * fifo);
```

如果你是使用kfifo_init()方法创建的队列，那么你需要负责释放相关的缓冲。也就是释放buffer内存

#### 6.2.7队列使用举例

使用上述接口，我们看一个kfifo的具体用例，假定我们创建了一个由fifo指向的8KB大小的kfifo。我们就可以推送数据到队列。这个例子中，我们推入简单的整型数。在你自己的代码中，可以推入更复杂的任务相关数据。这里使用整数，我们看看kfifo如何工作：

```c
unsigned int i;
//将0-32压入名为fifo的kfifo中
for(i =0;i<32;i++)
    kfifo_in(fifo,&i,sizeof(i));
```

名为fifo的kfifo现在包含了0到31的整数，我们查看下队列的第一个元素是否为0

```c
unsigned int val;
int ret;
ret = kfifo_out_peek(fifo,&val,sizeof(val),0);//字节数是val的大小，查看的队列是fifo，存入的地址是&val，偏移量是0，从0读取val大小的字节
if(ret !=sizeof(val))
    return -EINVAL;
printk(KERN_INFO "%u\n",val);//调试信息优先级为KERN_INFO，
```

摘取并打印kfifo中的所有元素，我们可以调用kfifo_out()

```c
//当队列中还有数据时
while(kfifo_avail(fifo)){
    unsigned int val;
    int ret;
    ret = kfifo_out(fifo,&val,sizeof(val));
    if(ret != sizeof(val))
        return -EINVAL;
    printk(KERN_INFO "%u\n",val);
}
```

0到31的整数将一一按序打印出来（如果需要逆序打印，即从31到0，那么我们应该使用堆栈而不是队列）。

### 6.3映射

一个映射，也常称为关联数组，其实是一个由唯一键组成的集合，而每个键必然关联一个特定的值，这种键到值的关联关系称为映射，映射要至少支持三个操作，这里的映射其实就是字典吧，<key ,val>

```assembly
Add(key,value)
Remove(key)
value = Lookup(key)
```

将键值和值一起添加，移除键值，通过键值查看值

虽然散列表是一种映射，但并非所有的映射都需要通过散列表实现。除了使用散列表外，映射也可以通过自平衡二叉搜索树存储数据，虽然散列表能提供更好的平均渐变复杂度，但是二叉搜索树在最坏情况下能有更好的表现。二叉搜索树同时满足顺序保证，这将给用户的按序遍历带来很好的性能。二叉搜索树的最后一个优势也是我唯一能看懂的就是它不需要散列函数，需要的键类型只要可以定义<=操作算子便可以。这里要做的是重载运算符吗

虽然键到值的映射属于一个通用说法，但是更多时候特指使用二叉树而非散列表实现的关联数组。比如C++的STL容器std::map便是采用**自平衡二叉搜索树**实现的，他能提供按序遍历的能力。

Linux内核提供了简单、有效的映射数据结构，但是他并非一个通用的映射。因为他的目标是：映射一个唯一的标识数（UID）到一个指针。除了提供三个标准的映射操作外，Linux还在add操作基础上实现了allocate操作。这个allocate操作不但向map中加入了键值对，而且还可产生UID。

idr数据结构用于映射用户空间的UID，比如将inodify_watch的描述符或者POSIX的定时器ID映射到内核中相关联的数据结构上，如inotify_watch或者k_itimer结构体，其命名仍然延续了内核中有些含混不清的命名体系，这个映射被命名为idr。

之前之所以是大结构体嵌套小结构体是因为这是内核早期产物，这样的话就是导致大结构体的大小过大，所以后来改成了指针，包含链表的父结构并没有改。

#### 6.3.1初始化一个idr

这个idr就是自平衡二叉搜索树，建立一个idr很简单，首先需要静态定义或动态分配一个idr数据结构，然后调用idr_init()

```c
void idr_init(struct idr * idp);
struct idr id_huh;//静态定义idr结构
idr_init(&id_huh);//初始化idr结构
```

#### 6.3.2分配一个新的UID

一旦建立了idr，就可以分配新的UID了，这个过程分两步完成，第一步，告诉idr需要分配新的UID，允许其在必要时调整后备树的大小，这个后备树是什么。然后，第二部才是真正请求新的UID之所以需要这两个组合动作是因为要允许调整初始大小，这中间涉及在无锁情况下分配内存的场景。现在我们先别管如何处理上锁问题。重点看看如何使用idr

第一个调整后备树大小的方法是idr_pre_get():

```c
int idr_pre_get(struct idr *idp,gfp_t gfp_mask);
```

该函数将在需要时进行UID的分配工作：调整由idp指向的idr的大小。如果真的需要调整大小，则内存分配例程使用gfp标识：gfp_mask这个标识凡是在内核中动态分配大部分都用到了。在第12章讨论，不需要对并发访问该方法进行同步保护，和内核中其他函数的做法相反，idr_pre_get()成功返回1，失败返回0

第二个函数，实际执行获取新的UID，并且将其加到idr的方法是idr_get_new():

```c
int idr_get_new(struct idr * idp,void *ptr,int *id);//分配的UID存放在id中，关联到指针ptr，给idp分配UID和ptr
```

该方法使用idp所指向的idr去分配一个新的UID，并且将其关联到指针ptr上。成功时该方法返回0，并且将新的UID存于id，错误时，返回非0的错误码，错误码是-EAGAIN。

看一个完整例子：

```c
int id;
do{
if(!idr_pre_get(&idr_huh,GFP_KERNEL))//成功的时候返回1，失败返回0
    return -ENOSPC;
    ret = idr_get_new(&dir_huh,ptr,&id);
}while(ret == -EAGAIN);//执行成功跳出循环
```

如果成功，上述代码片段将获得一个新的UID，他被存储在整型变量id中，而且将UID映射到ptr（我们没有在代码片段中定义它）

函数idr_get_new_above()使得调用者可指定一个最小的UID返回值：

```c
int idr_get_new_above(struct idr * idp,void *ptr,int starting_id ,int *id);//多出来的starting_id表示新的UID要大于等于它
```

该函数的作用和idr_get_new()相同，除了它确保新的UID大于或等于starting_id外，使用这种变种方法允许idr的使用者确保UID不会被重用，允许其值不但在当前分配的ID中唯一，而且还保证在系统的整个运行期间唯一。

```c
int id;
do{
if(!idr_pre_get(&idr_huh,GFP_KERNEL))/*判断下调整idr大小函数是否运行成功*/
	return -ENOSPC;
	ret = idr_get_new_above(&idr_huh,ptr,next_id,&id);//获得UID映射ptr给idr_huh
}while(ret == -EAGAIN);
if(!ret)//ret = 0成功，执行下面语句
    next_id = id + 1;//将最低UID+1，因为使用idr_get_new_above函数获得的UID是整个系统中唯一的
```

#### 6.3.3查找UID

当我们在一个idr中已经分配了一些UID时，我们自然就需要查找他们：调用者给给出UID，idr将返回对应的指针ptr，查找步骤显然要比分配一个新UID要简单，仅需使用idr_find()方法即可：

```c
void * idr_find(struct idr * idp,int id);
```

该函数如果调用成功，则返回id关联的指针；如果错误，则返回空指针，注意，如果使用idr_get_new()或者idr_get_new_above()的时候将空指针映射给UID了，那么idr_find函数在成功时也返回NULL，这样就无法区分是成功还是失败，所以最好不要将UID映射给空指针。

这个函数的使用比较简单：

```c
struct my_struct *ptr = idr_find(&idr_huh,id);//当初映射的时候就要给这个类型的。
if(!ptr)
	return -EINVAL;
```

#### 6.3.4删除UID

从idr中删除UID使用方法idr_remove():

```c
void idr_remove(struct idr * idp,int id);
```

如果idr_remove()成功，则将id关联的指针一起从映射中删除。遗憾的是，idr_remove()并没有办法提示任何错误。

#### 6.3.5撤销idr

撤销一个idr的操作很简单，调用idr_destroy()函数即可：

```c
void idr_destroy(struct idr * idp);
```

如果该方法成功，则只释放idr中未使用的内存，他并不释放当前分配给UID使用的任何内存。通常，内核代码不会撤销idr，除非关闭或者卸载，而且只有在没有更多的UID时才能删除。

可以调用idr_remove_all()方法强制删除所有的UID：

```c
void idr_remove_all(struct idr * idp);
```

你应该首先对idp指向的idr调用idr_remove_all()，然后再调用idr_destroy()，这样就能使idr占用的内存都被释放。

### 6.4二叉树

树结构是一个能提供分层的树形数据结构的特定数据结构。在数学意义上，树是一个无环的、连接的有向图，其中任何一个顶点也叫做节点具有0个或者多个出边以及0个或者1个入边。一个二叉树是每个节点最多只有两个出边的树，也就是说一个树，其节点具有0个、1个、或者2个子节点。

![1670335339429](C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1670335339429.png)

#### 6.4.1二叉搜索树

一个二叉搜索树通常简称为BST是一个节点有序的二叉树，其顺序通常遵循下列法则：

- 根的左分支节点值都小于根节点值
- 右分支节点值都大于根节点值。
- 所有的子树也都是二叉搜索树。

因此，一个二叉搜索树所有节点必然都有序，而且左子节点小于其父节点值，而右子节点大于其父节点值的二叉树。所以在树中搜索一个给定值或者按序遍历树都相当快捷。

![1670335861544](C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1670335861544.png)

#### 6.4.2自平衡二叉搜索树

这个词出现好久了，之前的红黑树就是这个类型的。一个节点的深度是指从其根节点其，达到它一共需经过的父节点数目。处于树底层的节点称为叶子节点。一个树的高度是指树中的处于最底层节点的深度。一个**平衡二叉搜索树**是一个所有叶子节点深度差不超过1的二叉搜索树。一个**自平衡二叉搜索树**是指其操作都试图维持平衡的二叉搜索树。

![1670335906987](C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1670335906987.png)

看起来自平衡二叉搜索树，和平衡二叉搜索树没啥区别，只不过所谓的“自”是其操作都是不会破坏平衡。

##### 1.红黑树

红黑树是一种自平衡二叉搜索树，Linux主要的平衡二叉树数据结构就是红黑树。cfs_rb这是CFS调度算法的就绪队列就是由红黑树管理的，当时并没有将着色的问题。

红黑树具有特殊的着色属性，或红色或黑色，红黑树因遵循下面六个属性，所以能维持半平衡结构（什么又是半平衡呢）：

1. 所有节点要么红色要么黑色
2. 叶子节点都是黑色
3. 叶子节点不包含数据
4. 所有非叶子节点都有两个子节点
5. 如果一个节点是红色，则他的子节点都是黑色
6. 在一个节点到其叶子节点的路径中，如果总是包含同样数目的黑色节点，（这里的同样数目是啥意思），则该路径相比其他路径是最短的。

上述条件，保证了最深的叶子节点的深度不会大于两倍的最浅叶子节点的深度（这就是半平衡的定义），所以红黑树总是半平衡的，为什么它具有如此神奇的特点，

首先第五个属性，一个红色节点不能是其他红色节点的子节点或者父节点。

第六个属性保证了，从树的任何节点到其叶子节点的路径都具有相同数目的黑色节点。树里的最长路径则是红黑交替节点路经，所以最短路径必然是具有相同数量黑色节点的只包含黑色节点的路径。于是从根节点到叶子节点的最长路径不会超过最短路径的两倍。这句话不理解

如果插入和删除操作可以遵循上述六个要求，那这棵树始终保持是一个半平衡树，看起来不容易理解，为什么插入和删除动作都需要服从这些特别的约束，为什么不能用一些简单的规则去维持平衡树，其实，实践证明这些规则准寻起来还是相对简单的，而且在保证半平衡树前提下，这些柴和删除动作并不会增加额外负担。

至于如何让插入和删除动作都能遵循这些规则，已经超出了本书范围。哈？所以上面这些规则果然不是现在能理解的。

##### 2.rbtree

Linux实现的红黑树称为rbtree，其定义在文件lib/rbtree.c中，声明在文件<linux/retree.h>中，除了一定的优化外，Linux的rbtree类似于前面所描述的经典红黑树，即保持了平衡性，所以插入效率和树中节点数目呈对数关系。

rbtree的根节点由数据结构rb_root描述。创建一个红黑树，要分配一个新的rb_tree结构，并且需要初始化为特殊值RB_ROOT：

```c
struct rb_root root = RB_BOOT;//这是二叉树的根节点
```

树里的其他节点由结构rb_node描述。给定一个rb_node，我们可以通过跟踪同名节点指针来找到他的左右子节点。

rbtree的实现并没有提供搜索和插入例程，这些例程希望由rbtree的用户自己定义。这是因为C语言不容易进行泛式编程，同时内核开发者们相信最有效的搜索和插入方法需要每个用户自己去实现。你可以使用rbtree提供的辅助函数，但自己要实现比较操作算子。就是说没有红黑树操作的API

搜索操作和插入操作最好的范例就是展示一个实际场景:我们先来 看搜索，下面的函数实现了在页告诉缓存中搜索一个文件区也就是在TAB中搜索一个文件去，这里的页面号用红黑树实现的吗？（由一个i节点和一个偏移量共同描述），每个i节点都有自己的rbtree，已关联在文件中的页偏移。该函数将搜索给定i节点的rbtree，以寻找匹配的偏移值：

```c
struct page * rb_search_page_cache(struct inode * inode,unsigned long offset)
{
    struct rb_node *n = inode->i_rb_page_cache.rb_node;//这里又出现了inode类型不知道这代表着什么,n是一个临时变量存放树节点的
    while(n){
        struct page *page = rb_entry(n,struct page,rb_page_cache);//可以看到同样采用了数据结构包含链表的操作，这里包含了红黑树，通过该红黑树节点找到父结构
        if(offset<page->offset)//偏移值就是红黑树用于判断遍历的方向到哪里
            n = n->rb_left;
        else if(offset > page->offset)
            n = n->rb_right;
        else
            return page;//如果偏移值等于当前页面包含红黑树节点的偏移值返回
    }
    return NULL;
}
```

这个例子中，在while循环中遍历了整个rbtree，offset将决定是向左还是向右，if和else条件实际上实现了rbtree的比较方法，从而确保了树的有序性（往大了找还是往小了找）。如果循环中找到了一个匹配offset的节点，则搜索完成，并返回对应的page结构，如果循环查找了全树也没有找到一个匹配项，说明在树中不存在匹配项，则函数返回NULL

插入操作要相对复杂一些，因为必须实现搜索和插入逻辑，下面并非一个了不起的函数，但可以作为实现自己插入操作的一个指导：

```c
struct page * rb_insert_page_cache(struct inode * inode,unsigned long offset,struct rb_node *node)
{
    struct rb_node ** p = &inode->i_rb_page_cache.rb_node;
    struct rb_node *parent = NULL;
    struct page *page;
    while(*p){
        parent = *p;//让parent变成当前二叉树节点的父节点
        page = rb_entry(parent,struct page,rb_page_cache);//找到父节点被包含的父结构
        if(offset<page->offset)
            p = &(*p)->rb_left;//如果偏移值小于父节点的话，就作为父节点的左子树
        else if(offset>page->offset)
            p = &(*p)->rb_right;
        else
            return page;//此时不希望找到相同的节点
    }//啥时候退出啊，父节点为空是什么意思
    rb_link_node(node,parent,p);//parent和p就是我们找到的位置，将node插入其中
    rb_insert_color(node,&inode->i_rb_page_cache);//执行复杂的再平衡动作，如果页被加入到页高速缓存中，则返回NULL，如果页已经在高速缓存中了，返回这个已存在的页结构地址
    return NULL;
}
```

红黑树的搜索就是给一个offset和inode然后遍历二叉树就可以了，插入的话需要先找到该结点的位置，然后将节点插入其中，其实也是先搜索再插入，具体的插入和再平衡动作（着色）太复杂了。

### 6.5数据结构以及选择

我们已经详细讨论了Linux中最重要的四种数据结构：链表、队列、映射和红黑树。在本节中，我们将教下如何再代码中具体选择使用哪种数据结构。

如果对数据集合的主要操作是遍历数据，就使用链表。事实上没有数据结构可以提供比线性算法复杂度更好的算法遍历元素，所以应该用最简单的数据结构完成简单工作。就是说怎么遍历复杂度很大还不如使用一个简单的数据结构实现。

另外当性能并非首要考虑因素时，或者当你需要存储相对较少的数据项时，或者当需要和内核中其他使用链表的代码交互时，也应该优先选择链表。

如果代码符合生产者/消费者模式，则使用队列，特别是如果想要一个定长缓冲。队列会使得添加和删除项的工作简单有效。同时队列页提供了先入先出的语义。而这也正是生产者/消费者用例的普遍需求。另一方面，如果需要存储一个大小不明的数据结构，链表更适合，毕竟离散的内存分配会导致其可以动态添加任何数量的数据项。

如果需要映射一个UID到一个对象，就使用映射，映射结构使得映射工作简单有效，而且映射可以帮助维护和分配UID。Linux的映射接口是针对UID到指针的映射，它并不适合其他场景。如果再处理发给用户空间的描述符，就考虑下映射。

如果需要大量存储数据，并且检索迅速，那么红黑树最好，红黑树可确保搜索时间复杂度是对数关系，即使节点很多最后遍历需要的时间也不会太多，同时也能保证按序遍历时间复杂度是线性关系。虽然比其他数据结构复杂一些，但其内存开销情况并不是太糟。但是如果你没有执行太多次时间紧迫的查找操作，那没必要使用红黑树来导致内存开销过大，使用链表吧。

要是上述数据结构还不能满足你的需求，内核还实现了一些较少使用的数据结构，也许它们能帮你，比如基树（tric类型）和位图。位图啊通常用来表示系统资源的使用情况。只有当寻遍所有内核提供的数据结构都不能满足时，你才需要自己设计数据结构。经常在独立的源文件中实现的一种常见数据结构是散列表。因为散列表无非是一些桶和一个散列函数，而且这个散列函数是针对每个用例的，因此用非泛型编程语言比如C语言实现内核范围内的统一散列表没有什么价值。泛型编程指的就是调用函数的时候可以接受不同类型的参数，实现统一接口的作用，这应该需要重载函数才能实现，C没有这个功能，如果单独为每个用例创建一个散列表太麻烦了。

### 6.6算法复杂度

在计算机科学和相关学科中，很有必要把算法的复杂度量化的表示出来。虽然存在各种各样表示伸缩度的方法，但是最常用的技术还是研究算法的渐变行为（asymptotic behavior）渐变行为是指当算法的输入变得非常大或者接近于无限大时算法的行为。渐变行为充分显示了当一个算法的输入逐渐变大时，该算法的伸缩度如何。就是给这个算法的输入取最大值看看啥情况。可以帮助我们以特定基准抽象出算法模型，从而更好地理解算法的行为。

#### 6.6.1算法

算法就是一系列的指令，他可能有一个或多个输入，最后产出一个结果或输出。比如计算一个房间中的人数的步骤就是一个算法，输入就是人，技术结果是输出，在Linux内核中，页换出和进程调度都是算法的例子。其实进程调度就是在CFS就绪队列（红黑树）中查找vruntime最小值，找到其最左叶子节点，然后进而找到该结点对应的进程。这里的时间调度实体就是包含了红黑树的父结构。rb_entry就可以找到了，进而找到进程描述符。

从数据角度讲，一个算法好比一个函数，比如我们称人数统计算法为f，要统计的人数为x，可以写成下面形式：

```c
y = f(x)//人数统计函数
```

这里y是统计x个人所需的时间。

#### 6.6.2大O符合

一种很有用的渐变表示法就是上限，它是一个函数，其值自从一个起始点之后总是超过我们所研究的函数的值，也就是说上限增长等于或者快于我们研究的函数。一个特殊符号，大o符号用来描述这种增长率。函数f(x)可写作O(g(x))，读为f是g的大o，数学定义形式为：

换成自然语言就是，完成f(x)的时间总是短于或等于完成g(x)的时间和任意常量的乘积。

从根本上讲，我们需要寻找一个函数，他的行为和我们的算法一样差或者更差，这样我们就可以通过给该函数送入非常大的输入，然后观察该函数的结果，从而了解我们算法的执行上限。

#### 6.6.3大o符号

当大多数人谈论大o符号时，更准确地讲他们谈论的更接近Donald Knuth所描述的大o符号，从技术角度讲，大o符号适合描述上限，比如7是6的上限，同样道理，9、12和65也都是6的上限。但更多说的是最小上限，或一个抽象出具有上限和下限的函数。

#### 6.6.4时间复杂度

比如，再次考虑计算房间里的人数，假设你一秒钟数一个人，那么如果有7个人在房间里，你需要花7秒数他们。显然如果有n个人，需要花n秒去数他们，我们称该算法复杂度为O(n)，如果任务是在房间里的所有人面前跳舞，那么时间复杂度就是O(1)。

让房间里的所有人相互介绍的时间复杂度是多少，有多少函数抽象这个算法呢，所以理解一个算法在提高工作负载时的表现，是为给定工作选择最好算法的关键。

显然，应避免使用复杂度为O(n!)或者O(2n)的算法，另外，用复杂度为O(1)的函数代替复杂度为O(n)的函数通常都会提高执行性能。但是情况并非总是如此，不能仅仅依靠算法复杂度来判断哪个函数在实际使用中性能更高。因为在比较算法性能时，还需要考虑输入规模。

不赞成使用复杂的算法，但是时刻要注意算法的负载和典型输入集合大小的关系，不要为了根本不需要支持的伸缩度要求，盲目地去优化算法。

### 6.7小结

本章我们讨论了许多Linux内核开发者们用于实现从进程调度到设备驱动等内核代码的通用数据结构，随着学习的深入，会发现这些数据结构的妙用，自己写内核代码时，记住总是应该重用已经存在的内核基础设施，别去重复造轮子。

也介绍了算法复杂度以及测量和标识算法复杂度的工具，其中最值得注意的是大o，贯穿本书，以及Linux内核，大o都是我们评价算法和内核组件在多用户、处理器、进程、网络连接、以及其他环境下伸缩度的重要指标。

其实吧，本章学完就是了解下每种数据结构存在方式，以及操作函数的使用，数据结构的特点，在内核中的实例。

## 第七章中断和中断处理

任何操作系统内核的核心任务，都包含有对连接到计算机上的硬件设备进行有效管理，如硬盘、蓝光碟机、键盘、鼠标、3D处理器，以及无线电等。而想要管理这些设备，首先要能和他们互通信息才行。众所周知，处理器的速度和外围硬件设备的速度往往不是一个数量级的，因此，如果内核采取让处理器向硬件发出一个请求，然后专门等待回应的方法，效果不好，在计组中提到过。既然硬件的相应这么慢，那么内核就应该在此期间处理其他事务，等到硬件真正完成了请求的操作之后，再回过头对他进行处理。（中断分成处理器+中断控制器，后者负责向前者发送中断信号）

那么到底如何让处理器和这些外部设备能协同工作，且不会降低机器的整体性能呢，轮询polling可能是一个解决办法这是检查下当前文件描述符中有无可用资源需要配合死循环使用，可以让内核定期对设备的状态进行查询，然后做出相应的处理，不过这种方法很可能会让内核做不少无用功，因为无论硬件设备是正在忙碌还是大功告成，轮询总是周期性地重复执行。更好的办法是由我们提供一种机制，让硬件在需要的适合再向内核发出信号，这就是中断机制。在本章中国，先讨论中断，进而讨论内核如何使用所谓的中断处理函数处理对应的中断。

### 7.1中断

中断使得硬件得以发出通知给处理器，例如，在你敲击键盘的时候，键盘控制器（控制键盘的硬件设备）会发送一个中断，通知操作系统有键按下。中断本质是一种特殊的电信号，由硬件设备法向处理器。处理器接收到中断后，会马上向操作系统反映此信号的到来，然后就由操作系统负责处理这些新到来的数据。硬件设备生成中断的时候并不考虑与处理器的时钟同步，换句话说就是中断随时可以产生，因此内核随时可能因为新到来的中断而被打断。

从物理学的角度看，中断是一种电信号，由硬件设备生成，并直接送入中断控制器的输入引脚中，中断控制器是个简单的电子芯片，其作用是将多路中断管线，采用复用技术只通过一个和处理器相连接的管线与处理器通信。当接收到一个中断后，中断控制器会给处理器发送一个电信号。处理器一经检测到此信号，便中断自己当前的工作转而处理中断，此后处理器会通知操作系统已经产生中断，这样，操作系统就可以对这个中断进行适当地处理了。

不同的设备对应的中断不同，而每个中断都通过一个唯一的数字标志，因此，来自键盘的中断就有别于来自硬盘的中断，从而使得操作系统能够对中断进行区分，这里的中断指的就是中断号了，并知道哪个硬件设备产生了哪个中断。这样，操作系统才能给不同的中断提供对应的中断处理程序。

这些中断值通常被称为中断请求（IRQ）线。每个IRQ线都会被关联一个数值量(这里的IRQ值就是中断号，这个中断号在操作系统上很好理解，就是说申请的IRQ号，但是对于裸机而言是什么，应该是中断向量表中的排序号吧)，例如，再经典的PC机上，IRQ0是时钟中断，而IRQ1是键盘中断，但并非所有的中断号都是这样严格定义的，例如对于连接在PCI总线上的设备而言，中断是动态分配的。而且其他非PC的体系结构也具有动态分配可用中断的特性。重点在于特定的中断总是与特定的设备相关联，并且内核要知道这些信息。实际上，硬件发出中断是为了引起内核的关注。这里的中断号是要申请的，这里说什么特定的设备相关联。（对于裸机开发而言，EXTI外部中断控制器中的exti_line表示的就是中断线，NVIC中断控制器负责中断优先级的选择）

##### 异常

在操作系统中，讨论中断就不能不提及异常，异常与中断不同，他在产生时必须考虑与处理器时钟同步。实际上，异常也常常称为同步中断(毕竟异常没有外部设备支持电信号的发出，只能依靠处理器的时钟发出信号，所以需要与处理器时钟同步)。在处理器执行到由于编程失误而导致的错误指令的时候，或者是在执行期间出现特殊情况，比如缺页。必须靠内核来处理的时候，就由处理器产生一个异常，因为许多处理器体系结构处理异常与处理中断的方式类似，因此，内核对他们的处理也很类似。本章对中断（由硬件产生的异步中断）的讨论，大部分也适合于异常（由处理器本身产生的同步中断），可以看出这里的异步/同步是相对于处理器时钟而言的，对于异步通知这是由信号、信号处理函数实现的，这里处理器的内存管理单元MMU发出了缺页中断，这属于硬中断，但确实是处理器发出的，，，应该说同步中断并不全是软中断。（这里其实学完ARM体系结构就会发现很好理解，异常就是各种错误，导致处理器切换中止模式、未定义模式。和软中断不一样，那是管理模式）

### 7.2中断处理程序

在响应一个特定中断的时候,内核会执行一个函数,该函数叫做中断处理程序(interrupt handler)或中断服务程序(interrupt service routine,ISR).产生中断的每个设备都有一个相应的中断处理程序.例如,由一个函数专门处理来自系统时钟的中断,而另一个函数专门处理由键盘产生的中断,一个设备的中断处理程序是它设备驱动程序的一部分，设备驱动程序是用于对设备进行管理的内核代码．（这两个不是一回事。中断处理会执行各个中断服务例程）

在Linux中，中断处理程序就是普普通通的Ｃ函数，只不过这些函数必须按照特定的类型声明，以便内核能够以标准的方式传递处理程序的信息，在其他方面，他们与一般的函数别无二致．中断处理程序与其他内核函数的真正区别在于，中断处理程序是被内核调用来响应中断的，而他们运行于我们称之为中断上下文的特殊上下文中．（中断处理程序、中断服务例程都属于中断上下文。）

需要指出的是，中断偶尔也被称作原子上下文，该上下文中的执行代码不可阻塞，其实就是说执行起来不可被打断，正如原子操作不可再分一样．

中断可能随时发生，因此中断处理程序也就随时可能执行．所以必须保证中断处理程序能够快速执行，这样才能保证尽可能快地恢复中断代码的执行．哪怕是裸机也不可以长时间停留在中断上下文，更何况操作系统有着更多的功能要维护，不会允许长时间停留在一个优先级非常高的进程中．所以对系统的其他部分而言，让中断处理程序在尽可能短的时间内完成运行也同样重要．

中断号与特定的设备关联，而中断处理函数与特定的中断关联，也就是说，一个设备可以产生多种不同的中断，那么该设备就可以对应多个中断处理程序，响应的，该设备的驱动程序也就需要准备多个这样的函数．比如裸机中的中断处理函数名称其实都是特定的，哪些名称是在中断向量表中就被定义了，比如某个引脚的中断函数就可以对根到中断向量表中，这说明确实是特定的中断对应于特定的设备．

最起码，中断处理程序要负责通知硬件设备中断已被接收：嗨，硬件，我听到你了，现在回去工作吧，但是中断处理程序往往还要完成大量其他的工作．例如，我们可以考虑下网络设备的中断处理程序面临的挑战．该处理程序除了要对硬件应答，还要把来自硬件的网络数据包拷贝到内存，对其进行处理后再交给合适的协议栈或应用程序．显而易见，这种工作量不会太小，尤其对于如今的千兆比特和万兆比特以太网卡而言．

### 7.3上半部和下半部的程序

又想中断处理程序运行的快，又想中断处理函数完成的工作量多，这两个目的显然有所抵触．鉴于两个目的之间存在此消彼长的矛盾关系，所以我们一般把中断处理切割为两个部分，中断处理程序是上半部，接收到一个中断，他会立即开始　执行，但只做有严格时限的工作，例如对接收的中断进行应答或复位硬件，这些工作都是在所有中断被禁止的情况下完成，能够被允许稍后完成的工作会推迟到下半部（bottom　ｈａｌｆ）去，此后在合适的时机，下半部会被软中断执行．以IIO触发器的虚拟中断为例,上半部仅仅是获取时间戳，下半部是将数据推送到缓冲区中．

让我们考察下上半部和下半部分割的例子，还是以我们的老朋友，网卡作为实例，当网卡接收来自网络的数据包时，需要通知内核数据包到了，网卡需要立即完成这件事，从而优化网络的吞吐量和传输周期，以避免超时，因此，网卡立即发出中断：嗨，内核，我这里有最新数据包了，内核通过执行网卡已注册的中断处理程序来做出应答．

中断开始执行，通知硬件，拷贝最新的网络数据包到内存，然后读取网卡更多的数据包．这些都是重要／紧迫而又与硬件相关的工作．内核通常需要快速的拷贝网络数据包到系统内存，因为网卡上接收网络数据包的缓存大小固定．相比系统内存也要小很多，所以上述拷贝动作一旦被延迟，必然造成缓存溢出，进入的网络包占满了网卡的缓存，后续的入包只能被丢弃．当网络数据包被拷贝到系统内存后，中断的任务算是完成了，这时他将控制权交还给系统被中断前原先运行的程序，这就是从中断上下文切换到进程上下文了．

处理和操作数据包的其他工作在随后的下半部中进行．

### 7.4注册中断处理程序

中断处理程序是管理硬件的驱动程序的组成部分，每一设备都有相关的驱动程序，如果设备使用中断，那么相应的驱动程序就注册一个中断处理程序．

驱动程序可以通过request_irq()函数注册一个中断处理程序(他被声明在文件<linux/interrupt.h>中),并且激活给定的中断线,以处理中断:

```c
int request_irq(unsigned int irq,irq_handler_t handler,unsigned long flags,const char *name,void *dev)
```

第一个参数irq表示要分配的中断号，对于某些设备，如传统PC设备商的系统市政或者键盘，这个值通常是预先确定的，而对于大多数其他设备，这个值要么是可以通过探测获取，要么可以通过编程动态确定。这个和之前的特定的中断对应特定的设备是一样的，我认为中断号和中断是一个东西。

第二个参数handler是一个指针，只想处理这个中断的实际中断处理程序。只要操作系统一接收到中断，该函数就被调用。

```c
typedef irqreturn_t (*irq_handler_t)(int ,void *);
```

注意handler函数的原型，他接受两个参数，并有一个类型为irqreturn_t的返回值。

这里确实确实需要注意下，虚拟中断是由iio_poll_func结构体作为consumer申请的，所以形参中的万能指针指向该结构体，而这个万能指针在我们自己使用的时候，一般不会使用。int类型是irq中断号

-----

聊下中断处理程序和中断服务例程的区别

一个中断处理程序对应一个中断线，比如EXTI_5这条中断线。

但是当中断线EXTI_5由五个设备共享，那么此时中断处理程序就对应五个中断服务例程。此时发生中断后，由中断处理程序决定执行哪条ISR。

request_irq函数中传入的函数指针指向的是一个中断服务例程。通过do_IRQ()执行各个中断服务例程。中断处理程序内核自带的。驱动模块中只有中断服务例程。

中断向量表中存放的是中断处理程序还是ISR取决于具体的操作系统和架构，ARM中存放的是中断处理程序，因为中断线与中断处理程序是一一对应的。至于说为什么使用M3核的时候直接注册中断服务例程，这是因为没有遇到过共享中断线的情况。

----

#### 7.4.1中断处理程序标志

第三个参数flags可以为0，也可能是下列一个或者多个标志的位掩码。其定义在文件<linux/interrupt.h>，在这些标志中最重要的是：

- IRQF_DISABLED，该标志被设置后，意味着内核在处理中断处理程序期间，禁止其他所有的中断。这就是为了防止中断嵌套的问题，如果不设置，中断处理程序可以与除本身外的其他任何中断同时运行（这里的同时运行指的是宏观上），多数中断处理程序是不会去设置该位的，因为禁止所有中断是一种野蛮的行为。这种用法留给希望快速执行的轻量级中断。这一标志是SA_INTERRUPT标志的当前表现形式，在过去的中断中用以区分快速和慢速中断。
- IRQF_SAMPLE_RANDOM，这标志表明这个设备产生的中断对内核熵池有贡献，内核熵池负责提供从各种随机事件导出的真正的随机数。如果指定了该标志，那么来自该**设备的中断间隔时间就会作为熵填充到熵池**中，如果设备以预知的速率产生中断比如系统定时器，或者可能收到外部攻击者的影响。那么就不要设置这个标志，相反有其他很多硬件产生中断的速率是不可预知的，能够成为一种较好的熵源。
- IRQF_TIMER，该标志是特别为系统定时器的中断处理而准备的。
- IRQF_SHARED，此标志表明可以在多个中断处理程序之间共享中断线。在同一个给定线上注册的每一个处理程序必须指定这个标志；否则，在每条线上只能有一个处理程序，其实就是共享中断号。
- 一般都是设置为NULL

第四个参数name是与中断相关的设备的ASCII文本表示，就是字符形式表示的，iio_trigger_poll结构体申请虚拟中断的时候就是以“ad7606_consumer2”好像是这个。

这些名字会被/proc/irq和/proc/interrupts文件使用，后一个常用来看看中断是否成功申请了，以便于用户通信。

第五个参数dev用于共享中断线，当一个中断处理程序需要释放时，dev讲提供唯一的标志信号（cookie），以便从共享中断线的诸多中断处理程序中删除指定的那一个。如果没有这个参数，name内核不可能知道在给定的中断线上到底要删除那一个处理函数，如果无需共享中断线，将该参数设置为NULL就可以了，之前设置为了indio_dev结构体，我以为这是要输入申请中断的所有者。原来是提供唯一标志信号。另外内核每次调用中断处理程序时，都会把这个指针传递给他，实践中往往会通过它传递驱动程序的设备结构：这指针是唯一的，而且有可能在中断处理程序内被用到。和我想的一样了。好像是申请中断的时候传入的void*指针是什么，在中断处理程序的形参就是对应的void *指针。

request_irq()成功执行返回0，如果返回非零值，就表示有错误发生，这种情况下，指定的中断处理程序不会被注册，最常见的错误是-EBUSY，他表示给定的中断线（中断号）已经在使用。或者是没有指定IRQF_SHARED

注意，request_irq()函数可能会睡眠，因此，不能在中断上下文或者其他不允许阻塞的代码中调用该函数，天真地在睡眠不安全的上下文中调用request_irq()函数，是一种常见错误。造成这种错误的部分原因是为什么request_irq()会引起阻塞。这确实让人费解，在注册的过程中，内核需要在/proc/irq文件中创建一个与中断对应的项。函数proc_mkdir()就是用来创建这个新的procfs项的，proc_mkdir()通过调用函数proc_create()对这个新的profs项进行设置，而proc_create()会调用函数kmalloc()来请求分配内存，而这个函数就是request_irq函数睡眠的原因。

#### 7.4.2一个中断例子

在一个驱动程序中请求一个中断线，并在通过request_irq()安装中断处理程序：

```c
request_irq():
if(request_irq(irqn,my_interrupt,IRQF_SHARED,"my_device",my_dev))//这里传入的my_dev指针就是用来区分IRQF_SHARED情况下共享中断线的独特标志
{
    printk(KERN_ERR "my_device:cannot register IRQ %d\n",irqn);
    return -EIO;
}
```

在这个例子中，irqn是请求的中断线；my_interrupt是中断处理程序；我们通过标志设置中断线可以共享；设备命名为my_device，最后传递my_dev变量给dev形参，如果请求失败，那么这段代码将打印出一个错误并返回。如果调用返回0，则说明处理程序已经成功安装。此后，处理程序就会在响应该中断的时候被调用了，有一点很重要，初始化硬件和注册中断处理程序的顺序必须正确，以防止中断处理程序在设备初始化完成之前就开始执行。

#### 7.4.3释放中断处理程序

卸载驱动程序时，需要注销相应的中断处理程序，并释放中断线。

```c
void free_irq(unsigned int irq,void *dev)
```

如果指定的中断线不是共享的，那么该函数删除处理程序的同时将禁用这条中断线。这就意味着需要重新申请该中断号了。如果中断线是共享的，则仅删除dev所对应的处理程序，而这条中断线本身只有在删除了最后一个处理程序时才会被禁用。由此可以看出为什么唯一的dev如此重要了，对于共享的中断线，需要有一个唯一的信息来区分其上面的多个处理程序，并让free_irq()仅仅删除指定的处理程序。

中断处理程序都是预先在内核进行注册的回调函数callback function，而不同的函数位于不同的驱动程序中，所以在这些函数共享同一个中断线时，内核必须准确地为他们创造执行环境，这里指的其实就是执行中断处理程序的时候内部会调用不同的设备结构体，此时就可以通过void *指针，将有用的环境信息传递给他们了。所以传入的void *指针不仅仅是应用于共享中断线时的唯一标志，还可以传递有用的环境信息。

不管是哪种情况下（共享或不共享），如果dev非空，他都必须与需要删除的处理程序相匹配，必须从进程上下文中调用free_irq()。

下面给出了中断处理函数的注册和注销函数

| 函数          | 描述                                                         |
| ------------- | ------------------------------------------------------------ |
| request_irq() | 在给定的中断线上注册一给定的中断处理程序                     |
| free_irq()    | 如果在给定的中断线上没有中断处理程序，则注销响应的处理程序，并禁用其中断线 |

### 7.5编写中断处理程序

以下是一个中断程序声明：

```c
static irqreturn_t intr_handler(int irq,void * dev)
```

注意，他的类型与request_irq()参数中handler所要求的的参数类型相匹配，第一个参数irq就是这个处理程序要响应的中断的中断号。如今这个参数已经没有太大作用了，确实我也是这么感觉的，可能只是在打印日志信息时会用到。而在2.0版以前的Linux内核中，由于没有dev这个参数，必须通过irq才能区分使用相同驱动程序（用中断号区分注册相同中断处理程序），因此也可以使用相同的中断处理程序的多个设备。例如具有多个相同类型硬盘驱动控制器的计算机。

第二个参数dev是一个通用指针，他与在中断处理程序注册时传递给request_irq()的参数dev必须一致。如果该值有唯一确定性，那么他就相当于一个cookie，可以用来区分共享同一中断处理程序的多个设备，另外dev也可能指向中断处理程序使用的一个数据结构，因为对每个设备而言，设备结构都是唯一的，而且可能在中断处理程序中也用得到，因此，他也通常被看做dev。

中断处理程序的返回值是一个特殊类型：irqreturn_t，中断处理程序可能返回两个特殊值：IRQ_NONE和IRQ_HANDLED，当中断处理程序检测到一个中断，但该中断对应的设备并不是在注册处理函数期间指定的产生源时，返回IRQ_NONE，这个一般不会进入中断处理函数吧，，

当中断处理程序被正确调用，且确实是它所对应的设备产生了中断时，发货IRQ_HANDLED，另外，也可以使用宏IRQ_RETVAL(val)，如果val为非零值，那么该宏返回IRQ_HANDLED；否则返回IRQ_NONE，利用这些特殊值，内核可以知道设备发出的是否是一种虚假的中断，其实我们定义的中断处理函数返回值都是IRQ_HANDLED。只不过返回值的利用与驱动开发者无关，这是内核开发者思考的事情。

如果给定中断线上所有中断处理程序返回的都是IRQ-NONE，那么内核就可以检测出问题了，注意，irqreturn_t这个返回类型实际上就是一个int型。之所以使用这些特殊值是为了与早期的内核保持兼容，2.6版之前的内核并不支持这种特性，中断处理程序只需返回void就行了。如果要和2.4甚至更早的内核上使用中断，需要将typedef irqreturn_t 改为void，屏蔽掉此特性，并给no-ops定义不同的返回值（这个之前并没有提过），其他用不着做什么大的修改。中断处理程序通常会标记为static，因为它从来不会被别的文件中的代码直接调用。

中断处理函数扮演什么角色取决于产生中断的设备好该设备为什么要发送中断，即使什么其他工作也不做，绝大部分的中断处理程序至少需要知道产生中断的设备，告诉他已经收到中断了，复杂一些的其他设备，可能还需要在中断处理程序中发送好接收数据，以及执行一些扩充的工作。尽可能将扩充的工作退给下半部处理程序。

##### 重入和中断处理程序

Linux中的中断处理程序是无须重入的。当一个给定的中断处理程序正在执行时，相应的中断线在所有处理器都会被屏蔽掉，（哪怕接收中断也是其他中断线），以防止在同一中断线上接收另一个新的中断。通常情况下，所有其他的中断都是打开的，所以这些不同中断线上的其他中断都能被处理，但是当前中断线总是被禁止的。由此可以看出，同一个中断处理程序绝对不会被同时调用以处理嵌套的中断，所以不需要考虑重入的问题（并发或者并行的调用同一个函数导致出现不可预测的问题）

#### 7.5.1共享的中断处理程序

共享的处理程序与非共享的处理程序在注册和运行方式上比较相似，但差异主要有以下三处：

- request_irq()的参数flags必须设置IRQF_SHARED标志
- 对于每个注册的中断处理程序来说，dev参数必须唯一。指向任一设备结构就可以了，通常选择设备结构是因为它是唯一的，而且中断处理程序可能会用到它。不能给共享的处理程序传递NULL值。
- 中断处理程序必须能够区分他的设备是否真的产生了中断。这既需要硬件的支持，也需要处理程序中有相关的处理逻辑。如果硬件不支持这一功能，那中断处理程序肯定会束手无策，他根本没法知道到底是与它对应的设备发出了这个中断，还是共享这条中断线的其他设备发出了这个中断。

所有共享中断线的驱动程序都必须满足以上要求，只要有任何一个设备没有按规则进行共享，那么中断线就无法共享了。指定IRQF_SHARED标志以调用request_irq()时，只有在以下两种情况才能成功：中断线当前未被注册，或者在该线上的所有已注册处理程序都指定了IRQF_SHARED。

内核接收一个中断后，他将依次调用在该中断线上注册的每一个处理程序。因此，一个处理程序必须知道他是否应该为这个中断负责，如果与它相关的设备并没有产生中断，那么处理程序应该立即退出。这需要硬件设备提供状态寄存器，以便中断处理程序进行检查。（这里所谓的依次调用中断线上的ISR，这就属于中断处理程序的工作）

#### 7.5.2中断处理程序实例

让我们考察一个实际的中断处理程序，他来自real-time clockRTC驱动程序，可以再drivers/char/rtc.c中找到。很多机器（包括PC）都可以找到RTC，他说一个从系统定时器中独立出来的设备，用于设置系统时钟，提供报警器或周期性的定时器。对大多数体系结构而言，系统时钟的设置，通常只需要向某个特定的寄存器或I/O地址写入想要的时间就可以了。

然而报警器或周期性定时器通常就得靠中断来实现，这种中断与生活中的闹铃差不多：中断发出时，报警器或定时器就会启动。

RTC驱动程序装载时，rtc_init()函数会被调用，对这个驱动程序进行初始化。他的职责之一就是注册中断处理程序：

```c
//对rtc_irq注册rtc_interrupt
if(request_irq(rtc_irq,rtc_interrupt,IRQF_SHARED,"rtc",(void*)&rtc_port)){
    printk(KERN_ERR "rtc:cannot register IRQ%d\n",rtc_irq);
    return -EIO;
}
```

从中我们看到，中断号由rtc_irq指定，这个变量用于为给定体系结构指定RTC中断。在PC上RTC位于IRQ8，这种中断号就需要共享了，我们常用的那外部中断一般都不需要共享中断线。第二个参数是我们的中断处理程序rtc_interrupt，他将与其他中断处理程序共享中断号，因为设置了IRQF_SHARED标志。由第四个参数我们看出，驱动程序的名称为rtc，因为这个设备允许共享中断线，所以它给dev型参传递了面向每个设备的实参数。

最后展示的是处理程序本身：

```c
static irqreturn_t rtc_interrupt(int irq,void *dev)
{
    /*可以是报警器中断、更新完成的中断或周期性中断
    我们把状态保存在rtc_irq_data的低字节中，
    而把从最后一次读取之后所接收的中断号保存在其余字节中*/
    
    spin_lock(&rtc_lock);//中断中只能使用自旋锁，不可以导致自身休眠，第一次调用是为了保证rtc_irq_data不被SMP机器上的其他处理器同时访问。
    rtc_irq_data += 0x100;
    rtc_irq_data &=~0xff;
    rtc_irq_data |=(CMOS_READ(RTC_INIR_FLAGS)&0xF0);
    if(rtc_status & RTC_TIMER_ON)//如果设置了RTC周期性定时器，就要通过函数mod_timer()对其更新
        mod_timer(&rtc_irq_timer,jiffies + HZ/rtc_freq + 2*HZ/100);
    spin_unlock(&rtc_lock);//临界区结束
    spin_lock(&rtc_task_lock);//第二次调用是为了避免rtc_callback出现相同的情况。
    if(rtc_callback)//这里就是单纯的回调函数并不是中断处理，本身已经处于中断中了
        rtc_callback->func(rtc_callback->private_data);//这里就是执行预先设定好的回调函数了
    spin_ublock(&rtc_wait);
    wake_up_interruptible(&rtc_wait);
    kill_fasync(&rtc_async_queue,SIGIO,POLL_IN);
    return IRQ_HANDLED;//因为不支持共享，所以总是返回IRQ_HANDLED，我们自己编写的也是如此，同样是不支持共享，而不是由内核实现返回IRQ_NONE值
}
```

### 7.6中断上下文

当执行一个中断处理程序时，内核处于中断上下文（interrupt context）中。回忆下进程上下文。进程上下文是一种内核所处的操作模式，此时内核代表进程执行，例如执行系统调用或者运行内核线程，在进程上下文中，可以通过current宏（current_thread_info()->task）关联当前进程，此外，因为进程是以进程上下文的形式连接到内核中的，因此，进程上下文可以睡眠，也可以调用调度程序。

与之相反，中断上下文和进程并没有什么瓜葛。与current宏也是不相干的（但是这个宏会指向被中断的进程）。**因为没有后备进程**，所以中断上下文不可以睡眠（这里说的比较清楚，就是中断没有后备进程，睡眠后无法切换。进程可以由schedule函数切换），因此，不能在中断上下文忠调用某些函数。如果一个函数睡眠，就不能在你中断处理程序中使用它，这是对中断处理程序中的函数类型进行了限制。

中断上下文具有较为严格的时间限制，因为它打断了其他代码。中断上下文中的代码应当迅速、简洁、尽量不要使用循环去处理繁重的工作。有一点很重要：中断处理程序打断了其他的代码，甚至可能是 打断了在其他中断线上的另一个处理程序。正是因为这种异步执行的特性，所以所有的中断处理程序必须尽可能的迅速、简洁。尽量把工作从中断处理程序中分离出来，放在下半部来执行，因为下半部可以在更合适的时间运行。不会具有上半部那么高的优先级。

中断处理程序栈的设置是一个配置选项。曾经，中断处理程序并不具有自己的栈。相反，他们共享所中断进程的内核栈（这句可以理解了，进程本身拥有两个栈，中断运行在内核空间，所以以前版本是占用进程的内核栈），内核栈的大小是两页，具体的说在32位体系结构上是8KB，在64位体系结构上是16KB。因为在这种设置中，中断处理程序共享别人的堆栈，所以他们在栈中获取空间时必须非常节约。当然，内核栈本身就很有限，因此，所有的内核代码都应该谨慎利用它。

在2.6版早期的内核中，增加了一个选项，把栈的大小从两页减到一页，也就是在32位的系统上只提供4KB的栈，内核栈属于内核内存的一部分，这就减轻了内存的压力，因为系统中每个进程原先都需要两页连续，且不可换出的内核内存。为了应对栈大小的减少，中断处理程序拥有了自己的栈，**每个处理器一个**，大小为一页。这个栈就成为中断栈了，尽管中断栈的大小是原先共享栈的一半，但平均可用栈空间却变大了，毕竟这回中断处理程序把这一整页占为己有。（每个处理器一个中断内核栈，当发生中断嵌套的时候就会保存当前内核栈中的数据转移到SVC模式下的堆栈中成为一个堆栈帧，然后返回IRQ模式，执行新的中断）

中断处理程序不必关心栈如何设置，或者内核栈的大小是多少，尽量节约内核栈空间。

### 7.7中断处理机制的实现

中断处理系统在Linux中的实现是非常依赖于体系结构的，其实就是依赖于处理器、所使用的的中断控制器的类型、体系结构的设计以及机器本身。

-----

下图的do_IRQ()函数就是由中断处理程序执行去检测线上是否有中断服务例程ISR。如果有的话执行ISR----handle_IRQ_event()

![1670464856447](C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1670464856447.png)

上图是中断从硬件到内核的路由，设备产生中断，通过总线把电信号发送给中断控制器。如果中断线是激活的（中断可以被屏蔽，这是由硬件实现的，中断优先级就是这么来的），那么中断控制器就会把中断发往处理器。在大多数体系结构中，这个工作就是通过电信号给处理器的特定管脚发送一个信号。除非在处理器上禁止该中断，否则，处理器会立即停止它正在做的事，关闭中断系统，然后跳到内存中预定义的位置开始执行哪里的代码。这个预定义的位置是由内核设置的（因为其异步通知的特点，开发者无法预知在哪里被执行，所以由内核设置），是中断处理程序的入口点。

在内核中，中断的旅程开始于预定义入口点，这类似于系统调用通过预定义的异常句柄进入内核（这里是同步中断）。对于每条中断线，处理器都会跳到对应的一个唯一的位置（这里指的就是中断向量表中的中断源位置）。这样，内核就可知道所接收中断的IRQ号了（所以并不是根据中断号来确定入口地址的，而是根据中断线这是硬件层决定的）初始入口点只是在栈中保存这个号，并存放当前寄存器的值；然后，内核调用函数do_IRQ()。从这里开始，大多数中断处理代码是用C编写的，但他们依然与体系结构相关。

do_IRQ()的声明如下：

```c
unsigned int do_IRQ(struct pt_regs regs);
```

因为C的调用惯例是要把函数参数放在栈的顶部，pt_regs结构包含原始寄存器的值，这些值是以前在汇编入口例程中保存在栈中的。中断的值也会得以保存，所以，do_IRQ()可以将它提取出来。

计算出中断号后，do_IRQ()对所接收的中断进行应答，但是在中断处理程序期间是否响应其他中断是由handle_IRQ_event()函数决定，禁止这条线上的中断传递，在普通的PC上，这些操作是由mask_and_ack_8259A()来完成的。

接下来，do_IRQ()需要确保在这条中断线上有一个有效的处理程序，并且这个程序已经启动，但当前并没有执行（指的是进入就绪态的意思吗）如果是这样的话，do_IRQ()就调用handle_IRQ_event()来运行为这条中断线所安装的中断处理程序，handle_IRQ_event()方法被定义在文件kernel/irq/handler.c中。

这个do_IRQ函数更像是为正式执行中断处理程序做准备。下面就是用到了中断处理程序的返回值，这里可以看出并不是给驱动开发者准备的。

```c
irqreturn_t handle_IRQ_event(unsigned int irq,struct irqaction *action)
{
    irqreturn_t ret,retval = IRQ_NONE;
    unsigned int status = 0;
    if(!(action->flags &IRQF_DISABLED))//如果处理器没有禁止中断的话执行下面的函数
        local_irq_enable_in_hardirq();//好好理解下这句话，如果注册中断处理函数的时候并没有要求禁止其他中断，允许中断嵌套的话，就使能所有中断，不再屏蔽他们触发，但是这是有限制的，不可能一直取消禁止中断，当中断处理程序执行完毕之后，继续禁止其他中断触发。
    do{//接下来每个潜在的处理程序在循环中依次执行，并对所有执行结果做出处理
        trace_irq_handler_entry(irq,action);
        ret = action->handler(irq,action->dev_id);//这里就是执行共享中断线上的各个中断服务例程ISR了，传入的是int ,void *类型实参，dev_id顾名思义就是设备识别，这里是中断的所有者。
        trace_irq_handler_exit(irq,action,ret);
        switch(ret){
            case IRQ_WAKE_THREAD:
                //把返回值设置为已处理，以便可疑的检查不再触发
                ret=IRQ_HANDLED;
                //但此时并没有结束，要捕获返回值为WAKE_THREAD的中断处理程序，但并不创建一个线程函数
                if(unlikely(!action->thread_fn)){
                    warn_no_thread(irq,action);
                    break;
                }
                if(likely(!test_bit(IRQTF_DIED,&action->thread_flags))){
                    set_bit(IRQTF_RUNTHREAD,&action->thread_flags);
                    wake_up_process(action->thread);
                }
            case IRQ_HANDLED:
                status |=action->flags;
                break;
            default:
                break;
        }
        retval |= ret;
        action = action->next;//执行下一个动作也就是下一个中断，这是在共享中断线的情况下，如果非共享直接退出，因为ISR是驱动开发者创建的
    }while(action);//直到找到好使的一个ISR
    if(status & IRQF_SAMPLE_RANDOM)//如果注册中断的时候还设置了该标志，调用下面的函数
        add_interrupt_randomness(irq);//函数使用中断间隔时间为随机数产生器，产生熵
    local_irq_disable();//最后将中断禁止，函数返回到do_IRQ()中
    return retval;
}
```

do_IRQ()函数做清理工作并返回到初始入口点，然后再从这个入口点跳到函数ret_from_intr()。入口点就是当初中断导致处理器跳转到的那个入口地址

ret_from_intr()例程类似于初始入口代码，以汇编语言编写。这个例程检查重新调度是否正在挂起，这里就意味着设置了need_resched为真，这是隐式调用schedule函数的方法。如果重新调度正在执行，而且内核正在返回用户空间（说明中断的是用户进程），那么schedule()被调用，如果内核正在返回内核空间（说明被中断的是内核进程）只有当preempt_count为0（表示内核进程已经没有锁资源了，属于安全抢占）schedule()才会被调用，否则抢占内核不安全，在schedule()返回之后，或者如果没有挂起的工作，那么，原来的寄存器被恢复，内核回复到曾经中断的点。

在x86上，初始的汇编例程位于arch/x86/kernel/entry_64.S文件中，C方法位于arch/x86/kernel/irq.c，其他所支持的结构与此类似。

中断处理机制是硬件触发到处理器执行入口地址->do_IRQ->handle_IRQ_event（这里执行并判断中断处理程序的返回值）->do_IRQ->ret_from_intr(这里检查是否需要重新调度)

### 7.8/proc/interrupts

procfs是一个虚拟文件系统，只存在于内核内存，一般安装于/proc目录，在procfs中读写文件都要调用内核函数，这些函数模拟从真实文件中读或写。其实就是将内核中的属性展示给用户层的一个接口。类似于sysfs。下面从单处理器PC上输出的信息：

第一列是中断线，在这个系统中，现有的中断号为0~2、..，显示的都是安装处理程序的中断线。第二列是一个接收中断数目的计数器。事实上，系统中的每个处理器都存在这样的列，但是，这个机器只有一个处理器，时钟中断已接收3602371次中断，而声卡没有接收一次中断。这表明机器启动以来没有使用它，通过/proc/irq可以看出来我们申请的外部中断到底触发了多少次，看下第二次没进入外部中断到底是什么原因。第三列是处理这个中断的中断控制器。XT-PIC对应于标准的PC可编程中断控制器。在具有IOAPIC的系统上，大多数中断会列出IO-APIC-level或IO-APIC-edge，作为自己的中断控制器。最后一列是与这个中断相关的设备名字，这个名字是通过参数devname提供给函数（void*dev_id）request_irq()的，前面讨论过了，如果中断是共享的话，会将这个中断线上的所有设备都显示出来。

对于想深入探究procfs内部的人来说，procfs代码位于fs/proc中，提供/proc/interrupts的函数是与体系结构相关的，叫做show_interrupts()。

### 7.9中断控制

Linux内核提供了一组接口用于操作机器上的中断状态，这些接口为我们提供了能够禁止当前处理器的中断系统，或屏蔽了整个机器的一条中断线的能力，这些例程都是与体系结构相关的，可以在<asm/system.h>和<asm/irq.h>中找到。

一般来说，控制中断系统的原因归根结底是需要提供同步。通过禁止中断，可以确保某个中断处理程序不会抢占当前的代码。（这里的同步指的是不会发生抢占，与异步通知，而不是同步时钟了）此外，禁止中断还可以禁止内核抢占。然而，不管是禁止中断还是禁止内核抢占，都没有提供任何保护机制来防止来自其他处理器的并发访问。Linux支持多处理器，因此内核代码一般都需要获取某种锁，防止来自其他处理器的并发访问，获取这些锁的同事也伴随着禁止本地中断，**锁提供保护机制，防止来自其他处理器的并发访问**，而**禁止中断提供保护机制，则是防止来自其他中断处理程序的并发访问**。

因此必须理解内核中断的控制接口。

#### 7.9.1禁止和激活中断

用于禁止当前处理器上的本地中断，随后又激活他们的语句为：

```c
local_irq_disable();//常用于中断处理程序结束之后
local_irq_enable();//根据注册中断的时候flags来决定是否使能所有中断
```

这两个函数通常以单个汇编指令来实现，涉及到汇编了就依赖于体系结构了。实际上，在x86中，local_irq_disable()仅仅是cli指令，而local_irq_enable()是sti指令。cli好sti分别是对clear和set允许中断（allow interrupt）标志的汇编调用，换句话说，在发出中断的处理器上，他们将禁止和激活中断的传递。即使外部中断信号来了，也不会向处理器传递

如果在调用local_irq_disable()例程之前已经禁止了中断，那么该例程会带来潜在的危险；同样对应的local_irq_enable()例程也存在潜在危险，因为他将无条件地激活中断，尽管这些中断可能在开始时是关闭的。所以我们需要一种机制把中断恢复到以前的状态而不是简单的激活或禁止。内核普遍关心这点是因为，内核中一个给定代码路径既可以在中断激活的情况下达到，也可以在中断禁止的情况下达到，这取决于具体的调用链。例如，想象一下前面的代码片段是一个大函数的组成部分。这个函数被另外两个函数调用：（代码路径其实指的就是函数调用关系，之前画流程图的时候就是）其中一个函数禁止中断，而另一个函数不禁止中断，其实这些都是废话

简单的说就是随着内核的不断增长，要想知道到达这个函数的所有代码路径将变得越来越困难，因此，在禁止中断之前保存中断系统状态更加安全一些，相反，在准备激活中断时，只需将中断恢复到原来状态即可。

```c
unsigned long flags;
local_irq_save(flags);//禁止中断
local_irq_restore(flags);//中断被恢复到他们原来的状态
```

这些方法至少有一部分要以宏的形式实现，因此表面上flags参数是以值传递，也就是表面上的只传入一个flags就可以了，但实际上这并不是一个简单的数值，而是具体体系结构的数值，也就是包含中断系统的状态。至少有一种体系结构把栈信息和值相结合（这里的体系结构值得就是SPARC了，这是一种可扩展的处理器架构），这里的栈信息指的就是当前函数所处的内存地址，反正就是说flags这个值已经与函数内存地址相关联了，所以flags不能传递给另一个函数，基于这个原因，对local_irq_save()和对local_irq_restore()的调用必须在同一个函数中进行。因为二者的实参flags与这两位的当前内存地址相关联，只有这样才能匹配成功，恢复与禁止成双成对。其实这也和保存中断状态有关吧，同一个函数具有相同的栈帧，这应该就是对栈的更细致的划分

这些函数都可以在中断中调用，也可以在进程上下文中调用。不会导致进程睡眠就可以在中断中使用了。

不再使用全局的cli()

以前的内核中提供了一种能够禁止系统中所有处理器上的中断方法。而且，如果另一个处理器调用这个方法，那么他就不得不等待，直到中断重新被激活才能继续执行。这个函数就是cli()，相应的激活中断函数为sti()，虽然适用于所有体系结构，但完全以x86为中心。这些接口在2.5版本开发期间被取消了，相应地，所有的中断同步现在必须结合使用本地的中断控制和自旋锁。这意味着，为了确保对共享数据的互斥访问，以前代码仅仅需要通过全局禁止中断达到互斥，而现在还需要自旋锁了。

以前，驱动程序编写者可能假定在他们的中断处理程序中，任何访问共享数据地方都可以使用cli()提供互斥访问。cli()调用将确保没有其他的中断处理程序（只有他们特定的处理程序会运行），此外如果另一个处理器进入了cli保护区，那么该处理器就无法继续运行了，直到原来的处理器退出他们的cli()保护区，并调用了sti()后才能继续运行。

现在驱动开发者无法只通过cli()实现互斥访问了。

取消全局cli()有不少优点，首先，强制驱动程序编写者实现真正的加锁，要知道具有特定目的细粒度锁比全局锁要快很多，而且也完全吻合cli()的使用初衷。其次，这也使得很多代码更具流线型，避免了代码的成簇布局。由此得到的中断系统更简单也更易于理解。

#### 7.9.2禁止指定中断线

前面介绍了禁止整个处理器上所有中断的函数，在某些情况下，只禁止整个系统中的一条特定的中断线就够了，这就是所谓的屏蔽掉（masking out)一条中断线。作为例子，可能想在对中断的状态操作之前禁止设备中断的传递。为此，Linux提供了四个接口：（之前看这个sync很眼熟，在应用篇中学到了是用来刷新内核缓冲区的）

```c
void disable_irq(unsigned int irq);//禁止中断控制器上指定的中断线，只有在当前正在执行的所有处理程序完成后，disable_irq()才会返回。因此调用者不仅要确保不再指定线上传递新的中断，同时还要确保所有已经开始执行的处理程序已全部退出。只有这样才能保证不会被阻塞
void disable_irq_nosync(unsigned int irq);//非同步模式，其实就是不会等待当前中断执行完毕就返回了。
void enable_irq(unsigned int irq);//这里就是不需要等待这条中断线上的处理程序退出就会返回。
void synchronize_irq(unsigned int irq);//等待一个特定的中断处理程序的退出，如果该处理程序正在执行，那么该函数必须退出后才能返回。
```

对这些函数的调用可以嵌套，可以理解为上多把锁，但要记住在一条指定的中断线上，对disable_irq()或者disable_irq_nosync()的每次调用，都需要相应地调用一次enable_irq()。只有在对enable_irq()完成最后一次调用后，才真正重新激活了中断线。

禁止多个中断处理程序共享的中断线是不合适的。禁止中断线也就禁止了这条线上所有设备的中断传递，因此用于新设备的驱动程序不应该使用这些接口。根据规范PCI设备必须支持中断线共享，因此，他们根本不应该使用这些接口。所以，disable_irq()及其相关函数在老式传统设备的驱动程序中更容易找到。我没见过使用这些的。

#### 7.9.3中断系统的状态

通常有必要了解中断系统的状态（如中断是禁止的还是激活的），或者你当前是否正处于中断上下文的执行该状态中。

宏irqs_disable()定义在<asm/system.h>中，如果本地处理器上的中断系统被禁止，则它返回非零，否则返回零。也许可以看下是否是因为中断控制器被禁止的缘故才导致该中断线无法实现第二次响应

在<linux/hardirq.h>中定义的两个宏提供一个用来检查内核的当前上下文接口：

```c
in_terrupt();//如果内核处于任何类型的中断处理中，他返回非零，说明内核此刻正在执行中断处理程序，或者正在执行下半部处理程序。
int_irq();//只有在内核确实正在执行中断处理程序时返回非零。
```

通常情况下，要检查自己是否处于进程上下文中。也就是说，希望确保自己不在中断上下文中。这种情况很常见，（其实所谓的中断上下文并没有后备函数，意思就是中断处理程序如果被阻塞休眠了，调度器没有替换的程序，所以中断处理程序并不可以被休眠）。因为代码要做一些像睡眠这样只能从进程上下文做的事。如果in_interrupt()返回0则此刻内核处于进程上下文中。下面给出中断控制方法的列表

| 函数                 | 说明                                                         |
| -------------------- | ------------------------------------------------------------ |
| local_irq_disable()  | 禁止本地中断传递，这是全部都禁止                             |
| local_irq_enable()   | 激活本地中断传递                                             |
| local_irq_save()     | 保存本地中断传递的当前状态，然后禁止本地中断传递，知道保存中断状态了，但还是全部中断都禁止 |
| local_irq_restore()  | 恢复本地终端传递到给定的状态，知道恢复成原来的状态了这很好   |
| disable_irq()        | 禁止给定中断线，并确保该函数返回之前在该中断线上没有处理程序在运行 |
| disable_irq_nosync() | 禁止给定中断线                                               |
| enable_irq()         | 激活给定中断线                                               |
| irqs_disabled()      | 如果本地中断传递被禁止，则返回非零，否则返回0                |
| in_interrupt()       | 如果在中断上下文返回非零，如果在进程上下文中，返回0          |
| in_irq()             | 如果当前正在执行中断处理程序，则返回非零，否则返回0          |

问题是这些函数在哪里应用呢

### 7.10小结

本章介绍了中断，这是一种由设备使用的硬件资源异步项处理器发信号，实际上中断就是由硬件来打断操作系统。

大多数现代硬件都通过中断与操作系统通信。对给定硬件进行管理的驱动程序注册中断处理程序，是为了响应并处理来自相关硬件的中断，中断过程所做的工作包括应答并重新设置硬件，从设备拷贝数据到内存以及反之，处理硬件请求，并发送新的硬件请求。

内核提供的接口包括注册和注销中断处理程序、禁止中断、屏蔽中断线以及检查中断系统的状态。

因为中断打断了其他代码的执行（进程，内核本身，甚至其他中断处理程序），他们必须赶快执行完。但通常是还有很多工作要做。为了在大量的工作必须快速执行之间求得一个平衡，内核把处理中断的工作分为两半。中断处理程序，也就是上半部在本章讨论。下一章研究下半部。

## 第八章下半部好推后执行的工作

在第七章，我们讨论了内核为处理中断而提供的中断处理程序机制。中断处理程序在内核中很有用的部分。但是由于本身存在一些局限，所以只能完成整个中断处理流程的上半部分，这些局限包括：

- 中断处理程序以异步方式（异步指的是让CPU暂时搁置当前请求的响应,处理下一个请求 ）执行，并且他有可能会打断其他重要代码（甚至包括其他中断处理程序）的执行。因此，为了避免被打断的代码停留时间过长，中断处理程序应该执行的越快越好
- 如果当前有一个中断处理程序正在执行，在最好的情况下也就是IRQF_DISABLED没有被设置，与该中断同级的其他中断会被屏蔽，在最坏的情况下也就是设置了IRQF_DISABLED，当前处理器上所有其他中断都会被屏蔽。此时因为执行这个中断处理程序而导致了部分中断被屏蔽，这影响了硬件与操作系统之间的通信，因此中断处理程序执行的越快越好。
- 由于中断处理程序往往需要对硬件进行操作，比如给硬件提供一个时序图之类的，通常有很高的时限要求，要求一定时间内完成操作。
- 中断处理程序不再进程上下文运行，所以他们不能被阻塞，这就限制了他们做的事情比如阻塞访问文件。

现在，为什么中断处理程序只能作为整个硬件中断处理流程一部分的原因很明显了。操作系统必须有一个快速、异步、简单的机制负责对硬件做出迅速响应并给完成那些时间要求很严格的操作。中断处理程序很适合于实现这些功能，可是对于那些其他的、对时间要求相对宽松打断任务，就应该推后到中断被激活以后再去运行。也就是说中断上半部的时候会屏蔽掉部分中断，但是下半部因为是软中断实现的（优先级相对比较高的普通进程），在这里是已经激活中断了，上半部结束后就激活中断了。

这样，整个中断处理流程就被分为了两个部分，第一部分是中断处理程序（上半部），内核通过对他的异步执行完成对硬件中断的即时响应。在本章中，要研究的是中断处理流程中的下半部。

### 8.1下半部

下半部的任务就是执行与中断处理密切相关但中断处理程序本身不执行的工作 。在理想的情况下，最好是中断处理程序将所有工作都交给下半部执行，因为我们希望在中断处理程序中完成的工作越少越好也就是越快越好。我们期望中断 处理程序能够尽可能快地返回。

但是中断处理程序注定要完成一部分工作。例如，中断处理程序几乎都需要通过操作硬件对中断的到达进行确认，有时还会从硬件拷贝数据。因为这些工作对时间非常敏感，所以只能靠中断处理程序自己去完成。

剩下的几乎所有其他工作都是在下半部执行的目标。例如，如果你在上半部中吧数据从硬件拷贝到了内存，那么当然应该在下半部中处理内存中的数据。遗憾的是，并不存在严格明确的规定来说明到底什么任务应该在哪个部分完成，如何做决定完全取决于驱动开发者的判断，尽管在理论上不存在什么错误，但轻率的实现效果往往不会很理想，中断处理程序会异步执行，并且在最好的情况下他也会锁定当前的中断线。因此在中断处理程序持续执行的时间缩短到最小程度显得非常重要。对于在上半部和下半部之间划分工作，尽管不存在某种严格的规则，但还是有一些提示可供借鉴。

- 如果一个任务对时间非常敏感，将其放在中断处理程序中执行
- 如果一个任务和硬件相关，将其放到中断处理程序中执行
- 如果一个任务要保证不被其他中断打断，放到中断处理程序执行
- 其他所有任务，放到下半部执行

当开始尝试写自己的驱动程序的时候，读一下别人的中断处理程序和响应下半部可能会让你受益匪浅。在决定怎样把你的中断处理流程中的任务划分到上半部和下半部中去的时候，问问自己什么必须放进上半部而什么可以放到下半部。

#### 8.1.1为什么要用下半部

----

三刷：目前对于中断上下部我关注点在于处理器模式是否需要发生改变，这牵扯到页表项的访问权限问题。中断嵌套的问题和中断模式无关，堆栈帧就是嵌套导致的。也许不同的下半部实现方式会导致处理器模式不同。目前感觉上下文只和调度有关，和处理器模式无关。

---

理解为什么要让工作推后执行以及在什么时候推后执行非常关键。希望尽量减少中断处理程序中需要完成的工作量。

下半部并不需要指明一个确切时间，只要把这个任务推迟一些，让他们在系统不太繁忙并且中断恢复后执行就可以了。通常下半部在中断处理程序一返回就会马上运行。下半部执行的关键在于当他们运行的时候，允许响应所有的中断。

不仅仅是Linux，许多操作系统也把处理硬件中断的过程分为了两部分。这么说起来我是没进入下半部中，那上半部进入了，是不是因为上半部执行了但下半部没有执行呢。

#### 8.1.2下半部的环境

和上半部只能通过中断处理程序实现不同，下半部可以通过多种机制实现。这些用来实现下半部的机制分别由不同的接口和子系统组成。本章中会发现，实现一个下半部会有许多不太的方法。实际上，在Linux发展的过程中曾经出现过多种下半部机制。让人备受困扰的是，其中不少机制名字起的很像甚至是词不达意。这就需要专门的程序员来给下半部起名。

本章中将要讨论2.6版本的内核中的下半部机制是如何设计和实现的。同时也会讨论怎么在自己编写的内核代码中使用过它们，而那些过去使用的、已经废除了一段时间的机制，由于曾经很有名气，所以也会提到。

##### 1.下半部的起源

最早的Linux只提供bottom half这种机制用于实现下半部。这个名字没问题，也成为了BH，BH接口也非常简单，提供了一个静态创建、由32个bottom halves组成的链表。上半部通过一个32位整数中的一位来标识出哪个bottom half可以执行。每个BH都在全局范围内同步。即使分属于不同的处理器，也不允许任何两个bottom half同时执行。这种机制使用方便却不够灵活，简单但是有性能的瓶颈。

##### 2.任务队列

不久，内核开发者们引入了任务队列机制来实现工作的推后执行，并用它来代替BH机制。内核为此定义了一组队列，其中每个队列都包含一个由等待调用的函数组成链表。根据其所处队列的位置，这些函数会在某个时刻执行。驱动程序可以把它们自己的下半部注册到合适的队列上去。这种机制表现得不错，但是没法代替整个BH接口。对于一些性能要求比较高的子系统，比如网络部分，这个也不能胜任。

##### 3.软中断和tasklet

在2.3这个开发版本中，内核开发者引入 了软中断（softirqs）和tasklet。如果无须考虑和过去开发的驱动程序兼容的话，软中断和tasklet完全可以代替BH接口。软中断是一组静态定义的下半部接口，有32个。可以在所有处理器上同时执行，即使两个类型相同也可以。tasklet这一名称起的很糟糕，让人费解，他们是一种基于软中断实现的灵活性强，动态创建的下半部实现机制。两个不同类型的tasklet可以在不同的处理器上同时执行，但类型相同的tasklet不能同时执行。tasklet其实是一种在性能和易用性之间寻求平衡的产物。对于大部分下半部处理来说，用tasklet就足够了，像网络这种对性能够要求非常高的情况才需要使用软中断。可是，使用软中断需要特别小心，因为两个相同的软中断有可能同时被执行，此外软中断还必须在编译期间就进行静态注册。与此相反，tasklet可以通过代码进行动态注册。

有些人被这些概念彻底搞糊涂了，他们吧所有的下半部都当成是软件产生的中断或者软中断了，我就是这样，，其实是BH和软中断和tasklet并驾齐名。

这里的软中断和实现系统调用所提到的软件中断并不是一个概念，后者是异常，同步中断由处理器时钟发出的信号实现的中断。可以理解为内部中断。而这里的软中断指的是类似于优先级很高的普通线程。

把BH转化为软中断或者tasklet并不是轻而易举的事情，因为BH是全局同步的，因此，在执行期间假定没有其他BH执行，这种转化在内核2.5中实现了。

在开发2.5版本的内核时，BH接口最后全部被弃置了，所有的BH使用者必须转而使用其他下半部接口。此外，任务队列接口也被工作队列取代了，工作队列是一种简单而非常有用的方法，他们先对要推后执行的工作排队，然后在进程上下文中执行他们。

综上所述，2.6版本以后，内核提供了三种不同形式的下半部实现机制：软中断、tasklets、和工作队列。

##### 内核定时器

另外一个可以用来将工作推后执行的机制是内核定时器。不像本章到目前为止介绍到的所有这些机制，内核定时器把操作推迟到某个确定的时间段之后执行。

##### 4.混乱的下半部概念

这些东西确实把人搅得很混乱，但是他们其实只不过是一些起名的问题，让我们再来梳理下

下半部（bottom half）是一个操作系统通用词汇，用于指代中断处理流程中推后执行的那一部分，所有用于实现将工作推后执行的内核机制都被称为下半部机制。一些人错误的把所有的下半部机制都叫做软中断。下表揭示了下半部机制的演化过程。

| 下半部机制              | 状态          |
| ----------------------- | ------------- |
| BH                      | 在2.5中去除   |
| 任务队列（task queues） | 在2.5中去除   |
| 软中断（Softirq）       | 从2.3开始引入 |
| tasklet                 | 从2.3开始引入 |
| 工作队列（Work queues） | 从2.5开始引入 |

### 8.2软中断

我们的讨论从实际的下半部实现，软中断方法开始，软中断使用的比较少，而tasklet下半部更常用，但是由于tasklet是通过软中断实现的，所以先来研究软中断。软中断的代码位于kernel/softirq.c文件中。

#### 8.2.1软中断的实现

软中断是在编译期间静态分配，他不像tasklet那样能够被动态地注册或注销。软中断由softirq_action结构表示，他定义在<linux/interrupts.h>中

```c
struct softirq_aciton{
void (*action)(struct softirq_action *);//这是结构体成员是函数指针形参是该结构体，有点this的味道
}
```

而在kernel/softirq.c中定义了一个包含有32个该结构体的数组。

```c
static struct softirq_action softirq_vec[NR_SOFTIRQS];
```

每个被注册的软中断都占据该数组的一项，因此最多可能有32个软中断。注意，这是一个定制，注册的软中断数目最大值没法动态改变，所以这不是一个链表。在2.6版本的内核中这32个项只用到了9个。

##### 1.软中断处理程序

软中断处理程序action的函数原型如下：

```c
void softirq_handler(struct softirq_action *)
```

党内和运行一个软中断处理程序的时候，就会执行这个action函数，唯一的参数为指向响应结构体的指针

```c
my_softirq->action(my_softirq);
```

当你看到内核把整个结构体都传递给软中断处理程序而不是仅仅传递数据值的时候，这个小技巧可以保证将来在结构体中加入新的成员时，无须对所有的软中断处理程序都进行变动，软中断处理程序可以方便的解析他的参数从数据成员中提取数值。

一个软中断不会抢占另外一个软中断。实际上，唯一可以抢占软中断的是中断处理程序。不过，其他的软中断可以在其他处理器上同时执行。好奇的就是为啥不能在同一个处理器上执行。

##### 2.执行软中断

一个注册的软中断必须在被标记后才会执行。这被称为触发软中断，通常，中断处理程序会在返回前标记他的软中断，使其稍后被执行。于是，在合适的时刻软中断就会运行。在下列地方，待处理的软中断会被检查和执行：

- 从一个硬件中断代码处返回时
- 在ksoftirqd内核线程中
- 在那些显式检查和执行待处理的软中断的代码中

不管是用什么办法换起，软中断都要在do_softirq()中执行。该函数很简单。如果有待处理的软中断，do_softirq()会循环遍历每一个，调用它们的处理程序。（这和do_IRQ函数一样，遍历每一个中断服务例程，但是软中断应该没有共享中断线的概念）

```c
u32 pending;//这其实就是一个32位的位图
pending = local_softirq_pending();
if(pending){//如果成功赋值的话
    struct softirq_action *h;//定义一个软中断结构体指针
    //重设待处理的位图
    set_softirq_pending(0);
    h = softirq_vec;//要注意这里是一个结构体数组
    do{
        if(pending & 1)//如果位图最低位被置一的话
            h->action(h);//执行结构体数组中的第一个结构体的软中断处理程序
        h++;//将h+1此时指向了第二个结构体
        pending >>=1;//位图也要右移一位，这样丢弃第一位，第二位变成第一位
    }while(pending);
}
```

一直重复下去，直到pending变为0，这表明已经没有待处理的软中断了，我们的任务也就完成了。注意这种歌检查足以保证h总是指向结构体数组softirq_vec的有效项，因为pending最多只可能设置32位，循环最多也只能执行32次。

执行遍历软中断位图的时候也要禁止本地中断。

#### 8.2.2使用软中断

软中断保留给系统中对时间要求最严格以及最重要的下半部使用。目前，只有两个子系统（网络和SCSI，网络之前就讲过了，因为网络芯片内部缓存不够，还要把接受到的数据发送给内存，这就非常考验时间了，不然就会出现丢包现象）是直接使用软中断。此外，内核定时器和tasklet都是建立在软中断上的。如果想要加入一个新的软中断，首先应该问问自己为什么用tasklet实现不了，tasklet可以动态生成，由于他们对加锁的要求不高，所以使用起来很方便，而且他们的性能也非常不错。当然，对于时间要求严格并能自己高效地完成加锁工作的应用，软中断就是正确的选择。

##### 1.分配索引

在编译期间，通过<linux/interrupt.h>中定义的一个枚举类型来静态地声明软中断。内核用这些从0开始的索引来表示一种相对优先级。索引号小的软中断在索引号大大大大软中断之前执行。

建立一种新的软中断必须在此枚举类型中加入新的项。而加入时，你不能像其他地方一样，简单地把新项加到列表的末尾。相反，你必须根据希望赋予他的优先级来决定加入的位置。习惯上，HI_SOFTIRQ通常作为第一项，而RCU_SOFTIRQ作为最后一项。新项一般是插在BLOCK_SOFTIRQ好TASKLET_SOFTIRQ之间。

| tasklet         | 优先级 | 软中断描述                                                 |
| --------------- | ------ | ---------------------------------------------------------- |
| HI_SOFTIRQ      | 0      | 优先级高的tasklets                                         |
| TIMER_SOFTIRQ   | 1      | 定时器的下半部                                             |
| NET_TX_SOFTIRQ  | 2      | 发送网络数据包，之前说过网络设备会使用软中断作为中断下半部 |
| NET_RX_SOFTIRQ  | 3      | 接收网络数据包                                             |
| BLOCK_SOFTIRQ   | 4      | BLOCK装置                                                  |
| TASKLET_SOFTIRQ | 5      | 正常优先权的tasklets                                       |
| SCHED)SOFTIRQ   | 6      | 调度程度                                                   |
| HRTIMER_SOFTIRQ | 7      | 高分辨率定时器                                             |
| RCU_SOFTIRQ     | 8      | RCU锁定                                                    |

##### 2.注册你的处理程序

接着，在运行时通过调用open_softirq()注册软中断处理程序，该函数有两个参数：软中断的索引号和处理函数。如网络子系统，在ne/coreldev.c通过下面方式注册软中断：

```c
open_softirq(NET_TX_SOFTIRQ,net_tx_action);
open_softirq(NET_RT_SOFTIRQ,net_rx_action);
```

软中断处理程序执行的时候，允许响应中断这里指的是软中断，但他自己不能休眠。在一个处理程序运行的时候，当前处理器上的软中断被禁止。但其他的处理器仍可以执行别的软中断。实际上，如果同一个软中断在它被执行的同时再次被触发了，那么**另外一个处理器可以同时运行其处理程序**。这意味着任何共享数据（甚至是仅在软中断处理程序内部使用的全局变量）都需要严格的锁保护。这点很重要，他也是tasklet更受喜欢的原因。tasklet当同一个软中断在他被执行的同时再次被触发了，是不会在其他处理器上执行的。

如果仅仅通过互斥的加锁方式来防止它自身的并发执行，那是用软中断也没啥意义了，所以大部分软中断处理程序，都通过采取单处理器数据（仅属于某一个处理器的数据，因此根本不需要加锁，其他处理器无法执行该软中断）或其他一些技巧来避免显式地加锁，从而提供更出色的性能。

引入软中断的主要原因是其可扩展性。如果不需要扩展到多个处理器（也就是在多个处理器上执行）就是用tasklet，tasklet本质上也是软中断，只不过同一个处理程序的多个实例不能在多个处理器上同时运行，这就很好的避免了数据的共享。

##### 3.触发你的软中断

通过在枚举类型的列表中添加新项以及调用open_softirq()进行注册以后，新的软中断处理程序就能够运行。rasie_softirq()函数可以将一个软中断设置为挂起状态（这里的挂起状态其实就是触发软中断的意思），让他在下次调用do_softirq()函数时投入运行。也就是开始遍历32位的位图的每一个位

```c
raise_softirq(NET_TX_SOFTIRQ);
```

这会触发NET_TX_SOFTIRQ软中断。他的处理程序net_tx_action()就会在内核下一次执行软中断时投入运行。该函数在触发一个软中断之前先要禁止中断（软中断执行前是中断在执行，等到执行完毕就会禁止中断，这是因为只有在中断处理程序期间可以被打断，否则其他时刻不可以被打断），触发后再恢复原来的状态。软中断处理程序执行的时候可以被中断打断的，为啥这里要禁止中断呢。如果中断本来就已经被禁止了，可以调用另一函数raise_softirq_irqoff()，这会带来一些优化效果：

```c
//中断已经被禁止
raise_softirq_irqoff(NET_TX_SOFTIRQ);
```

在中断处理程序中触发软中断是最常见的形式。在这种情况下，中断处理程序执行硬件设备的相关操作，然后触发相应的软中断，最后退出，内核在执行完中断处理程序以后，马上就会调用do_softirq()函数。于是软中断开始执行中断处理程序留给他的剩余任务。

为啥软中断执行的时候也要禁止中断，其实执行完中断后就禁止中断了，然后执行软中断。流程上确实是对的，但是为什么呢，应该是只有在中断处理程序期间可以被另一个中断打断，执行完软中断以后应该会激活中断吧。所到底，中断+软中断才是宏观上的中断处理程序。

### 8.3tasklet

tasklet是利用软中断实现的一种下半部机制，之前提到过，他和进程没有关系，现在其实也知道了软中断和进程也没有关系，软中断被调用的理由也不是schedule调用的，而是由中断执行结束以后调用的。所以并**不是所谓的“优先级比较高的普通进程”。**

tasklet和软中断在本质上很相似，行为表现也相近，但是，他的接口更简单，锁保护也要求较低。

选择到底是软中断还是tasklet其实很简单：

通常你应该用tasklet，就像之前看到的软中断的32位只用了9位一样，软中断的使用者屈指可数。只在那些执行频率很高好连续性要求很高的情况下才需要使用。而tasklet却有更广泛的用途。大多数情况下用tasklet效果都不错，而且他们非常容易使用。

#### 8.3.1tasklet的实现

因为tasklet是通过软中断实现的，所以他们本身也是软中断。

##### 1.tasklet结构体

tasklet由tasklet_struct结构表示，每个结构体单独代表一个tasklet，他在<linux/interrupt.h>中定义为：

```c
struct tasklet_struct{//这个名称一下子想起了task_struct结构体了
    struct tasklet_struct *next;//链表中的下一个tasklet，注意这里是将tasklet_struct作为链表节点类型了，而不是将链表包含进父结构。list_head
    unsigned long state;//tasklet的状态
    atomic_t count;//引用计数器，这是原子类型
    void(*func)(unsigned long);//tasklet处理程序
    unsigned long data;//给tasklet处理函数的参数
};
```

结构体中的func成员是tasklet的处理程序（像软中断中的action一样），data是它唯一的参数。

state成员只能在0、TASKLET_STATE_SCHED和TASKLET_STATE_RUN之间取值，TASKLET_STATE_SCHED表明tasklet已被调度，正准备投入运行，TASKLET_STATE_RUN表明该tasklet正在运行。这是只有在多处理器的系统上才会作为一种优化来使用，单处理器任何时候都清楚单个tasklet是不是正在执行。

count成员是tasklet的引用计数器，如果不为零，则tasklet被禁止，不允许执行，表示当前正在执行一个tasklet，只有当count==0的时候，tasklet才被激活，并且在被设置为挂起状态时，该tasklet才能够执行。这就是当正在执行一个tasklet的时候会禁止在任一处理器上执行该tasklet。

##### 2.调度tasklet

已调度的tasklet等同于被触发的软中断。（所谓的本质上就是软中断，区别再在于不允许共享数据而已，也就是同时在多个处理器上执行同一个tasklet）。已调度的tasklet存放在两个单处理器数据结构：tasklet_vec(普通tasklet)和tasklet_hi_vec(高优先级的tasklet)。这两个数据结构都是由tasklet_struct结构体构成的链表。链表中的每个tasklet_struct代表一个不同的tasklet。其实就是将tasklet结构体加入链表。

tasklet由tasklet_schedule()和tasklet_hi_schedule()函数进行调度，他们接受一个执行tasklet_struct结构体的指针作为参数，两个函数非常类似（区别在于一个使用TASKLET_SOFTIRQ而另一个用HI_SOFTIRQ）。只不过是对应的软中断优先级不同而已。

tasklet由两类软中断代表：HI_SOFTIRQ和TASKLET_SOFTIRQ。

接下来的内容中我们将仔细研究怎么编写和使用tasklets。现在，让我们考察下tasklet_schedule()的细节：

1. 检查tasklet的状态是否为TASKLET_STATE_SCHED。如果是，说明tasklet已经被调度过了但还没来得及执行，函数立即返回。
2. 调用_tasklet_schedule()。
3. 保存中断状态，然后禁止本地中断。在我们执行tasklet代码时，这么做可以保证当tasklet_schedule()处理这些tasklet时，处理器上的数据不会弄乱。这里禁止本地中断并不是防止tasklet执行的时候被中断抢占，而是防止执行调度程序的时候处理器上的数据不会弄乱。
4. 把需要调度的tasklet加到每个处理器的tasklet_vec链表或者tasklet_hi_vec链表。
5. 唤起TASKLET_SOFTIRQ或HI_SOFTIRQ软中断，这样在下一次调用do_softirq()时会被执行该tasklet，这里其实就能看出来了，基于软中断实现的tasklet，只不过在其外面增加了tasklet的操作。
6. 恢复中断到原状态并返回。

所以整个流程是使能中断->执行中断上半部->禁止中断->唤醒中断下半部->保存中断并禁用中断->执行中断下半部->恢复中断

在前面的内容中我们曾经提到过挂起，do_softirq()会尽可能早地在下一个合适的时机执行，由于大部分tasklet和软中断都是在中断处理程序中被设置成待处理状态，所以最近一个中断返回的时候看起来就是执行do_softirq()的最佳时机。因为TASKLET_SOFTIRQ和HI_SOFTIRQ已经被触发了（这两个软中断就是tasklet的基础）。所以do_softirq()会执行相应的软中断处理程序。而这两个处理程序，tasklet_action()和tasklet_hi_action()这就是tasklet处理的核心。下面是两个处理程序具体工作

1. 禁止中断（我真的是搞不懂了，这个系统中断状态到底是怎么回事，没必要首先保存其状态，因为这里的代码总是作为软中断被调用，而且中断总是被激活的），并为当前处理器检索tasklet_vec或tasklet_hig_vec链表。毕竟在中断中就挂起了会导致将相应地 tasklet添加到两个链表中。
2. 将当前处理器上的该链表设置为NULL，达到清空的效果。
3. 允许响应中断。（看得出来这个临界区也是防止数据被变乱的，等到执行处理函数的时候就允许响应中断了）没必要再恢复成以前的状态，因为这段代码本身就是作为软中断处理程序被调用的，所以中断是应该被允许的。
4. 循环遍历获得链表上的每一个待处理的tasklet，之前是循环遍历每一个软中断组成的结构体数组，现在是循环遍历那两个软中断为基础创建的tasklet链表
5. 如果是多处理器系统，通过检查TASKLET_STATE_RUN来判断这个tasklet是否正在其他处理器上运行，如果他正在运行，那么现在就不要执行了，跳到下一个待处理的tasklet去。在tasklet_schedule()的时候也会检查一次
6. 如果当前这个tasklet没有执行，将其状态设置为TASKLET_STATE_RUN，这样别的处理器就不会再执行了。是在tasklet处理程序中设置状态的
7. 检查count值是否为0，确保tasklet没有被禁止。如果tasklet被禁止了，则跳到下一个挂起的tasklet，执行的顺序是链表的顺序
8. 此时已经清楚的知道了这个tasklet没有在其他地方执行，并且被我们设置成执行状态，这样他在其他地方也不会执行了，并且引用计数为0，现在可以执行tasklet的处理程序了。
9. tasklet运行完毕，清除tasklet的state的TASKLET_STATE_RUN状态标志。
10. 重复执行下一个tasklet，直到没有剩余的等待处理的tasklet

tasklet和软中断一样并没有用到schedule()所以这也是他们不同于进程的地方，也是中断执行完之后直接调用。

tasklet的实现非常简单，也非常巧妙。可以看到，所有的tasklet都通过重复运用HI_SOFTIRQ和TASKLET_SOFTIRQ这两个软中断实现，当一个tasklet被调度时，内核就会唤起这两个软中断中的一个。随后，该软中断会被特定的函数处理，执行所有已调度的tasklet。这个函数保证同一时间只有一个给定类型的tasklet会被执行。所有复杂性都被一个简洁的接口隐藏了。确实挺复杂，先执行do_softirq函数毕竟是基于软中断实现的tasklet，在这个do_softirq函数中执行tasklet的处理程序，tasklet_action函数。最后在这个函数中执行处理程序。

#### 8.3.2使用tasklet

大多数情况下，为了控制一个寻常的硬件设备，tasklet机制都是实现自己的下半部的最佳选择，tasklet可以动态创建，使用方便，执行起来也快。

##### 1.声明自己的tasklet

你既可以静态地创建tasklet，也可以动态地创建它。选择哪种方式取决于你到底想要对一个tasklet的直接引用还是间接引用。如果准备静态地创建一个tasklet也就是直接引用，使用下面 <linux/interrupt.h>中定义的两个宏中的一个：

```c
DECLARE_TASKLET(name,func,data);
DECLARE_TASKLET_DISABLE(name,func,data);
```

这两个宏根据给定的名称静态地创建一个tasklet_struct结构。当该tasklet被调度以后，给定的函数func会被执行，他的参数由data给出。这两个宏的区别在于引用计数器的初始值设置不同。前面一个宏把创建的tasklet的引用计数器设置为0，这样初始值就是处于激活状态。另一个把引用计数器设置为1，到时候执行处理程序的时候就会检测count是否为零，所以该tasklet处于禁止状态。下面是一个例子

```c
DECLARE_TASKLET(my_tasklet,my_tasklet_handler,dev);
struct tasklet_struct my_tasklet = {//传入的name就是实例化结构体的变量名称
    NULL,//指向的下一个节点
    0,//state
    ATOMIC_NINT(0),//count
    my_tasklet_handler,
    dev,//data
};
```

这样就创建了一个名为my_tasklet，处理程序为tasklet_handler并且是已被激活的tasklet，当处理程序被调用的时候，dev就会被传递给他。

还可以通过将一个间接引用（一个指针）赋值给一个动态创建的tasklet_struct结构的方式来初始化一个tasklet_init();其实一直讲的动态创建指的就是定义一个指向结构体的指针，然后使用函数初始化它，而静态创建就是直接创建一个结构体成员都已经被赋值的结构体。其实就是宏，创建IIO通道那样的。之前创建kfifo也是分成了静态和动态，这一次把静态创建和动态创建的代码看明白了

##### 2.编写你自己的tasklet处理程序

tasklet处理程序必须符合规定的函数类型：

```c
void tasklet_handler(unsigned long data)
```

因为是靠软中断实现的，所以tasklet不可以睡眠，（归根结地其实是由中断中调用的软中断，而不是schedule调用的，所以和中断一样没有后备队列）意味着不能在tasklet中使用信号量或者其他什么阻塞式函数。（没有后备进程这就导致了中断被阻塞了以后调度器无法调度其他进程只能等待）由于tasklet运行时允许响应中断，所以你必须做好预防工作（比如屏蔽中断然后获取一个自旋锁），如果你的tasklet和中断处理程序之间共享了某些数据的话，两个相同的tasklet决不能同时执行。这就涉及到共享数据的问题了，

##### 3.调度自己的tasklet

通过调用tasklet_schedule()函数并传递给他相应的tasklet_struct的指针，该tasklet就会被调度以便执行：

```c
tasklet_schedule(&my_tasklet);//将my_tasklet标记为挂起，其实就是设置他的状态为TASKLET_STATE_SCHED，执行该函数的第一步就是检查状态，如果已经被挂起了直接退出
```

tasklet被调度以后，只要有机会就会尽可能早地运行，在他没有得到运行机会之前如果有一个相同的taasklet被调度了，那么仍然只会运行一次。

如果这次tasklet已经开始运行了，比如说在另外一个处理器上，那么这个新的tasklet会被重新调度并再次运行。

作为一种优化措施，一个tasklet总是在调度他的处理器上执行，毕竟调度他的话说明处理器的高速缓存中应该有中断下半部的页，可以更好地利用处理器的告诉缓存。

你可以调用tasklet_disable()函数来禁止某个指定的tasklet。如果该tasklet当前正在执行，这个函数会等到它执行完毕再返回。你也可以调用tasklet_disable_nosync()函数，这也是用来禁止指定的tasklet，之前禁止中断上半部的时候也是类似这种的函数。无须在返回前等待tasklet制定完毕。不过这种做法就是无法估计该tasklet是否仍在执行。调用tasklet_enable()函数可以激活一个tasklet。

如果希望激活DECLARE_TASKLET_DISABLED()创建的tasklet，这种创建的tasklet就是不能执行的，count初始的时候就是1.

```c
tasklet_disable(&my_tasklet);//tasklet现在被禁止了
tasklet_enable(&my_tasklet);//现在被激活了
```

可以通过调用tasklet_kill()函数从挂起的队列中去掉一个tasklet，该函数的参数是一个指向某个tasklet的tasklet_struct的长指针。在处理一个经常重新调度他自身的tasklet的时候，从挂起的队列中移去已调度的tasklet会很有用。这个函数首先会等待该tasklet执行完毕，然后再讲它移去。当然，没有什么可以组织其他地方的代码重新调度该tasklet。由于该函数需要等待tasklet执行完毕，所以可能造成休眠，禁止在中断上下文包括下半部中使用它。

##### 4.ksoftirqd

每个处理器都有一组辅助处理软中断（和tasklet）的内核线程。当内核中出现大量软中断的时候，这些内核进程就会辅助处理他们。因为tasklet通过软中断实施的，所以同样适用于软中断和tasklet。

之前曾经阐述过，对于软中断，内核会选择在几个特殊时机进行处理。而在中断处理程序返回时处理是最常见的。软中断被处罚的频率有时可能很高，更不利的是，处理函数有时还会自行重复触发。也就是说，当一个软中断执行的时候，他可以重新触发自己以便再次得到执行。

如果软中断本身出现的频率就高，再加上他们又有将自己重新设置为可执行状态的能力，那么就会导致用户空间进程无法获得足够的处理器时间，因而处于饥饿状态。而且单纯的对重新触发的软中断采取不立即处理的策略，也无法让人接受。

当软中断最初提出的时候，就是一个让人进退维谷的问题，而直观的解决方案又都不理想。首先看下两种最容易想到的直观的方案。

第一种方案是，只要还有被触发并等待处理的软中断，本次执行就要负责处理，重新触发的软中断也在本次执行返回前被处理。这样做可以保证对内核的软中断采取即时处理的方式，关键在于，对重新触发的软中断也会立即处理。当负载很高的时候这样做就会出问题，（就是说需要处理的软中断太多了！！）会导致 只有软中断和中断处理程序轮流执行。

第二种方案选择不处理重新触发的软中断，再从中断返回的时候，内核和平时一样，也会检查所有挂起的软中断并处理他们。但是，任何自行重新触发的软中断都不会马上处理，他们被放到下一个软中断执行时去处理，而这个时机通常也就是下一次中断返回的时候，也就是说要等一段时间，上一次重新触发的软中断才能被执行。可是，在比较空闲的系统中，立即处理软中断才是比较好的做法。这种方式能保证系统不处于饥饿状态，但却要让软中断忍受饥饿的痛苦，根本没有好好利用闲置的系统资源。这就是为什么说下半部不是进程了，只能在中断返回后执行，而不能像普通进程那样被调度器调度。

在设计软中断的时候，开发者就意识到需要一些折中。最终在内核中实现的方案是不会立即处理重新触发的软中断。而作为改进，当大量软中断出现的时候，内核会唤醒一组内核线程来处理这些负载。这些线程在最低的优先级上运行（nice = 19），这能避免他们跟其他重要的任务抢夺资源。但最后也会被执行，所以，这个折中方案能够保证在软中断负担很重的时候，用户不会饥饿，而在空闲系统中，软中断处理的非常迅速，毕竟仅存的内核线程哪怕优先级再低也会被调度。

每个处理器都有一个这样的线程。所有线程的名字都叫做ksofirqd/n，区别在于n，这对应的是处理器的编号，在双CPU的机器上就会有两个这样的线程，分别是ksoftirqd/0好ksoftirqd/1，为了保证只要有空闲的处理器，他们就会处理软中断，这样无论哪个空闲了都可以执行内核线程从而执行软中断了。一旦该线程被初始化了，他就会执行类似下面这样的死循环：

```c
for(;;)
    if(!softirq_pending(cpu))//返回真表示有待处理的软中断
        schedule();
set_current_state(TASK_RUNNING);//设置内核线程状态是正在执行毕竟这是内核线程而不是tasklet了
while(softirq_pending(cpu))
{
    do_softirq();//执行软中断
    if(need_resched())//当执行完以后看下是否有比当前进程优先级更高的进程如果有的话就重新调度去执行优先级更高的进程
        schedule();
}
set_current_state(TASK_INTERRUPTIBLE);//设置进程状态是被阻塞了，可中断的休眠
}
```

其实本质上来讲这些重新触发的软中断还是由do_softirq函数执行，只不过不再是由中断触发软中断执行了，而是由内核线程判断是否执行do_softirq，如果不执行就执行schedule函数。

#### 8.3.3老的BH机制

因为已经被放弃了，不学了。

### 8.4工作队列

工作队列（work queue）是另外一种将工作推后执行的形式，他和我们讨论的软中断好tasklet完全不同。工作队列可以把工作推后，交由一个内核线程去执行，这个下半部分总是会在进程上下文中执行。这样，通过工作队列执行的代码能占尽进程上下文的所有优势。最重要的是工作队列允许重新调度甚至是睡眠。

通常，在工作队列好软中断/tasklet中做出选择非常容易。如果推后执行的任务需要睡眠，那么就选择工作队列。如果推后执行的任务不需要睡眠，那么就选择软中断或tasklet。实际上，工作队列通常可以用内核线程替换。反正都是线程了，但是由于内核开发者非常反对创建新的内核线程，所以推荐使用工作队列。当然，这种接口也的确很容易使用。

如果需要用一个可以重新调度的实体来执行你的下半部处理，应该使用工作队列。这是唯一能在进程上下文中运行的下半部实现机制。也只有它可以睡眠。这意味着在需要获得大量的内存时，因为内核是运行在一个连续的大段内存上，需要获得信号量的时候，需要执行阻塞式的IO操作时，他都会非常有用。如果不需要用一个内核线程来推后进程执行工作，就使用tasklet吧。

#### 8.4.1工作队列的实现

工作队列子系统是一个用于创建内核线程的接口。通过它创建的进程负责执行由内核其他部分排到队列里的任务。他创建的这些内核线程成为工作者线程（worker thread）。工作队列可以让驱动程序专门创建一个专门的工作者线程来处理需要推后的工作。不过，工作队列子系统提供了缺省的工作者线程来处理这些工作。因此，工作队列最基本的表现形式，就转变成了一个把需要推后执行的任务交给特定的通用线程的这样一个接口。

缺省的工作者线程叫做events/n，这里的n是处理器的编号；每个处理器对应一个线程。例如，单处理器的系统只有events/0这样一个线程，就和tasklet的内核线程一样，而双处理器的系统就会多一个events/1线程。

默认的工作者线程会从多个地方得到被推后的工作。许多内核驱动程序都把他们的下半部交给默认的工作者线程去做。除非一个驱动程序或者子系统必须建立一个属于他自己的内核线程，否则最好使用默认线程。

到现在我都不知道自己的中断下半部是有什么实现的。不过并不存在什么东西能够阻止代码创建属于自己的工作者线程。如果你需要在工作者线程中执行官大量的处理操作，使用自己创建的工作者线程会好一些。处理器密集型和性能要求严格的任务会因为拥有自己的工作者线程而获得好处。此时这么做也有助于减轻缺省线程的负担，避免工作队列中其他需要完成的工作处于饥饿状态。

##### 1.表示线程的数据结构

工作者线程用workqueue_struct结构表示：其实这个结构体表示的是系统中同一种类型的每个CPU的工作者线程。这里的类型指的是默认的event和我们自己创建的falcon类型的工作者线程。

```c
//外部可见的工作队列抽象是由每个CPU的工作队列组成的数组
struct workqueue_struct{
    struct cpu_workqueue_struct cpu_wq[NR_CPUS];
    struct list_head list;//这回是一个包含链表的父结构了
    const char *name;//events/n
    int sinqlethread;
    int freezeable;
    int rt;
}
```

workqueue_struct结构体内是有一个cpu_workqueue_struct结构组成的数组，它定义在kernel/workqueue.c中，数组的每一项对应系统中的一个处理器。由于系统中每个处理器对应一个工作者线程，所以对于给定的某台计算机来说，就是每个处理器，每个工作者线程对应一个这样的cpu_workqueue_struct结构体。毕竟cpu_workqueue_struct结构体数组中每个成员对应一个处理器也就是对应一个工作者线程。

毫无疑问，cpu_work_queue_struct是kernel/workqueue.c中的核心数据结构：

```c
struct cpu_workqueue_struct{
	spinlock_t lock;//锁保护这种结构
	struct list_head worklist;//工作列表，这里是用来连接工作者需要执行的工作的链表
    wait_queue_head_t more_work;//这是等待队列
    struct work_struct *current_struct;//指向当前工作
    struct workqueue_struct *wq;//关联工作队列结构，其实就是指向自己父结构
    task_t *thread;//关联线程，但是进程描述符是task_struct
}
```

每个工作者线程类型关联一个自己的workqueue_struct，在该结构体内，给每个线程分配一个cpu_workqueue_struct，因而也就是给每个处理器分配一个。

要注意workqueue_struct这是工作者线程的结构体，就好像进程描述符一样

##### 2.表示工作的数据结构

所有的工作者线程都是用普通的内核线程实现的，他们都要执行worker_thread()函数。在他初始化完成以后，这个函数执行一个死循环并开始休眠，当有操作被插入到队列里的时候，线程就会被唤醒，其实就和tasklet的内核线程一样。也是有死循环这样才能时刻检测

一边执行这些操作，当没有剩余的操作也就是工作队列空了的时候就继续休眠。

工作用<linux/workqueue.h>中定义的work_struct结构体表示：

```c
struct work_struct{
	atomic_long_t data;//原子类型的long类型，这个其实就是该工作结构体的待处理标志位，每次执行完都要清0
    struct list_head entry;//又是一个链表
    work_func_t func;
}
```

这些结构体被连接成链表，在每个处理器上的每种类型的队列都对应这样一个链表。比如，每个处理器上用于执行被推后的工作的那个通用线程就有一个这样的链表。当一个工作者线程被唤醒的时候，他会执行他的链表上的所有工作。工作被执行完毕一个，就会将相应的work_struct对象从链表上移去。当链表上不再有对象的时候，就会继续休眠，实际上就是通过cpu_workqueue_struct->worklist中移除一个。

可以看下worker_thread()函数的核心流程，简化如下：

```c
for(;;)
{
    prepare_to_wait(&cwq->more_work,&wait,TASK_INTERRUPTIBLE);//之前就见过这个函数，将wait进程加入到等待队列more_work中，并设置进程状态为TASK_INTERRUPTIBLE。这里的wait就是工作者线程自己。
    if(list_empty(&cwq->worklist))//判断下工作队列是否清空，如果是就重新调度
        schedule();
    finish_wait(&cwq->more_work,&wait);//将wait进程从等待队列中移除。本线程还需要执行并将自身设置为TASK_RUNNING
    run_workqueue(cwq);//执行被推后的工作
}
```

这里神奇的是这是将自己这个线程设置进等待队列了，当然真正的休眠是由schedule函数引起的，正常的等待队列应用其实是将那些想要获取资源而得不到的线程给进入等待队列，而不是自身。

下一步，由run_workqueue()函数来实际完成推后到此的工作：

```c
while(!list_empty(&cwq->worklist)){
    struct work_struct *work;//创建一个指向工作结构体类型的指针
    work_func_t f;
    void *data;
    work = list_entry(cwq->worklist.next,struct work_struct,entry);//这个函数也很熟悉了，这是直接返回包含链表的父结构的地址。entry指的是在父结构中链表节点的实例化名称。想要获得的是工作链表的下一项，也就是获得下一项工作
    f = work->func;//将该工作要做的操作拿出来
    list_del_init(cwq->worklist.next);//从工作链表中删除该工作，并初始化也就是初始化该工作的上下节点指针
    work_clear_pending(work);//将待处理标志位pending清零
    f(work);//执行该工作
}
```

##### 3.工作队列实现机制的总结

其实我看懂了这些数据结构之间的关系。

![1670592141304](C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1670592141304.png)

位于最高一层的是工作者线程。系统允许有多种类型的工作者线程存在。对于指定的一个类型，系统的每个CPU上都有一个该类的工作者线程。内核中有些部分可以根据需要来创建工作者线程，而在默认情况下内核只有event这一种类型的工作者线程。每个工作者线程都有一个cpu_workqueue_struct结构体表示。而workqueue_struct结构体则表示给定类型的所有工作者线程。这里说透了。

例如，除系统默认的通用events工作者类型外，自己还加入了一种falcon工作者类型。并且使用的是拥有四个处理器的计算机，那么系统中现在有四个event类型的线程和自己创建的falcon类型的四个线程，其实就是有四个cpu_workqueue_struct结构体和另外四个cpu_workqueue_struct结构体。同时有一个对应event类型的workqueue_struct和一个对应ffalcon类型的workqueue_struct。

工作处于最底层，驱动程序创建这些需要推后执行的工作。他们用work_struct结构来表示。这个结构体中最重要的部分是一个指针，指向一个函数，而这个函数正是负责处理需要推后执行的具体任务。（work_func_t func）工作会被提交给某个具体的工作者线程，在这种情况下，就是特殊的falcon线程。然后这个工作者线程会被唤醒并执行这些拍好的工作。

大部分驱动程序都使用的是现存的默认工作者线程。他们使用起来简单、方便。可是在有些要求更严格的情况下，驱动程序就需要自己的工作者线程了。比如xfs文件系统就位自己创建了两种新的工作者线程。

#### 8.4.2使用工作队列

工作队列的使用非常简单，先来看下默认的events任务队列，然后再看看创建新的工作者线程。

##### 1.创建推后的工作

首先要做的是实际创建一些需要推后完成的工作。可以通过DECLARE_WORK在编译时静态地建该结构体：

```c
DECLARE_WORK(name,void(*func)(void*),void *data);
```

这样就会静态地创建一个名为name，处理函数为func，参数是data的work_struct结构体。

同样，也可以在运行时通过指针创建一个工作：

```c
INIT_WORK(struct work_struct *work,void(*func)(void*),void*data);
```

这会动态地初始化一个由work指向的工作，处理函数为func，参数为data。

##### 2.工作队列处理函数

工作队列处理函数的原型是：

```c
void work_handler(void *data)//这就是工作的处理函数
```

这个函数会由一个工作者线程执行，因此，函数会运行在进程上下文中。就是f(work)，默认情况下是允许响应中断，并且不持有任何锁。如果需要，函数可以睡眠，这就是工作队列与软中断/tasklet的区别了后两种是运行在中断上下文的。是由中断处理程序最后调用的，而不是由schedule函数调用的。

需要注意的，尽管操作处理程序运行在进程上下文中，但他不能访问用户空间，因为内核线程在用户空间没有相关的内存映射。只有在发生系统调用时，内核会代表用户空间的进程运行，此时他才能访问用户空间，也只有在此时他才会映射用户空间的内存。这还是用户空间调用系统调用的时候传入的地址。

在工作队列和内核其他部分之间使用锁机制就像在其他的进程上下文中使用锁机制一样。

##### 3.对工作进行调度

现在工作已经被创建，我们可以调度他了。想要把给定工作的处理函数提交给默认的events工作线程，只需调用：

```c
schedule_work(&work);
```

work马上就会被调度，一旦其所在的处理器上的工作者线程被唤醒，他就会被执行。

有时候并不希望工作马上就被执行，而是希望他经过过一段延迟之后再被执行。这种情况下，可以调度他在指定的时间执行：

```c
schedule_delayed_work(&work,delay);
```

这时，&work指向的work_struct知道delay指定的时钟节拍用完之后才会执行。

##### 4.刷新操作

排入队列的工作会在工作者线程下一次被唤醒的时候执行。有时，再继续下一步工作之前，你必须保证一些操作已经执行完毕。这一点对模块来说就很重要，在卸载之前，有可能需要调用下面的函数，而在内核的其他部分，为了防止竞争条件的出现，也可能需要确保不再有待处理的工作。

出于以上目的，内核准备了一个用于刷新指定工作队列的函数：

```c
void flush_scheduled_work(void);
```

函数会一直等待，直到队列中所有对象都被执行以后才返回。其实这个和用户层调用sync函数很像，就是用来刷新内核缓冲区的，只有当刷新完成以后才会返回。

在等待所有待处理的工作执行的时候，该函数会进入休眠状态，所以只能在进程上下文中使用他们。

注意，该函数并不取消任何延迟执行的工作，也就是说，任何通过schedule_delayed_work()调度的工作，如果其延迟时间未结束，他并不会因为调用flush_scheduled_work()而被刷新掉。取消延迟执行的工作应该调用：

```c
int cancel_delayed_work(struct work_struct *work);
```

这个函数可以取消任何与work_struct相关的挂起工作。

##### 5.创建新的工作队列

如果缺省的队列不能满足需求，应该创建一个新的工作队列和对应的工作者线程，由于这么做会在每个处理器上都创建一个工作者线程，所以只有在明确了必须要靠自己的一套线程来提高性能的情况下，再创建自己的工作队列。

创建一个新的任务队列和与之相关的工作者线程，需要调用一个简单的函数：

```c
struct workqueue_struct *create_workqueue(const char *name);
```

其中的name是用于该内核线程的命名，返回值是工作者线程的描述结构体。不是还要创建工作队列吗？，工作队列就是cpu_workqueue_struct[]数组，之前认为的是工作列表那个是存放工作节点的链表。比如缺省的events队列的创建就调用的是：

```c
struct workqueue_stuct *keventd_wq;
keventd_wq = create_workqueue("events");
```

这样就会创建所有的工作者线程（也就是说系统中的每个处理器都有一个该类型的工作者线程），并且做好所有开始处理工作之前的准备工作。

创建一个工作的时候无需考虑工作队列的类型。在创建之后，可以调用下面列举的函数。这些函数与schedule_work()以及schedule_delayed_work()相近，唯一的区别在于他们针对的是给定的工作队列，而不是默认的events队列进行操作。

```c
int queue_work(struct workqueue_struct *wq,struct work_struct *work)//传入了一个工作者线程，以及一个指向工作的列表
    
int queue_delayed_work(struct workqueue_struct *wq,struct work_struct *work,unsigned long delay)
```

最后，可以调用下面的函数刷新指定的工作队列，之前介绍刷新函数的时候提到的工作队列其实是工作列表，反正就是把工作列表刷新完，防止出现待处理的任务，这是在卸载驱动模块的时候用到的操作：

```c
flush_workqueue(struct workqueue_struct *wq);//
```

该函数和前面讨论过的flush_scheduled_work()作用相同，只是他在返回前等待清空的是给定的队列。

#### 8.4.3老的任务队列机制

像BH接口被软中断好tasklet代替一样，由于任务队列接口存在的种种缺陷，他也被工作队列接口取代了。像tasklet一样，任务队列接口（内核中称为tq）其实也和进程没有什么相关之处！任务队列接口的使用者在2.5开发版中分为两部分，其中一部分转向了使用tasklet，还有另外一部分继续使用任务队列接口。而目前任务队列接口剩余的部分已经演化成了工作队列接口。由于任务队列在内核中曾经使用过一段时间，出于了解历史的目的，我们对它进行一个大体回顾。

任务队列机制通过定义一组队列来实现其功能，每个队列都有自己的名字，比如调度程序队列、立即队列和定时器队列。不同的队列在内核中的不同场合使用。keventd内核线程负责执行调度程序队列的相关任务。他是整个工作队列接口的先驱。定时器队列会在系统定时器的每个时间节拍时执行，而立即队列能够得到双倍的运行机会，以保证他能立即执行。所谓的双倍机会什么意思，反正就是执行机会更多。

这些听起来都挺有用，但任务队列接口实际上是一团乱麻。这些队列基本上都是些随机创建的抽象概念，散落在内核各处，就像飘散在空气中。唯有调度队列有点意义，他能把工作推后到进程上下文完成。不看了，被淘汰的东西

### 8.5下半部机制的选择

在各种不同的下半部实现机制之间做出选择是很重要的。在2.6版内核中，有三种可能的选择：软中断、tasklet和工作队列。tasklet基于软中断实现，所以两者很相近。工作队列机制与他们完全不同，它靠内核线程实现。

从设计的角度考虑，软中断提供的执行序列化的保障最少。要求软中断处理函数必须格外小心地采取一些步骤确保共享数据的安全，两个甚至更多相同类别的软中断有可能在不同的处理器上同时执行。如果被考察的代码本身安全工作做得很好，比如网络子系统 ，他完全使用单处理器变量，那么软中断就是非常好的选择。对于时间要求严格和执行频率很高的应用来说，它执行的也最快。

如果代码安全问题考虑的不充分，选择tasklet意义更大，他的接口非常简单，而且由于两个同种类型的tasklet不能同时执行，所以实现起来简单一些。tasklet是有效的软中断，但不能并发运行。驱动程序开发者应当尽可能选择tasklet而不是软中断。（中断下半部的实现机制是由驱动开发者决定的，这可能就icm20606驱动由cpu0执行外部中断和虚拟中断，而ad7606就是分别在两个cpu上执行中断的原因，但我没找到在哪里），当然，如果准备利用每一个处理器上的变量或者类似的情形，以确保软中断能安全地在多个处理器上并发地运行，那么还是选择软中断。

如果讲到易于使用，工作队列就当仁不让了，使用默认的events队列很容易。接下来是tasklet，接口也很简单，最后才是软中断

下表是对三种下半部接口的比较

| 下半部   | 上下文 | 顺序执行保障           |
| -------- | ------ | ---------------------- |
| 软中断   | 中断   | 没有                   |
| tasklet  | 中断   | 同类型不能同时执行     |
| 工作队列 | 进程   | 和进程上下文一样被调度 |

简单的说，一般的驱动程序的编写者需要做两个选择。首先，是不是需要一个可调度的实体来执行需要推后完成的工作，如果休眠的需要，工作队列，否则选择tasklet，如果必须专注于性能的提高，就考虑软中断吧。

### 8.6在下半部之间加锁

到目前为止，还没讨论过锁机制，这是一个非常有趣且广泛的话题，在这里还是应该对它的重要性有所了解：在使用下半部机制时，即使是在一个单处理器的系统上，避免共享数据被同时访问也是至关重要的。一个下半部实际上可能在任何时间执行。

使用tasklet的一个好处在于，他自己负责执行的序列化保障：两个相同类型的tasklet不允许同时执行，即使在不同的处理器上也不行 。意味着无须为共享数据的问题操心。

tasklet之间同步（两个不同类型的tasklet共享同一数据时）需要正确使用锁机制。

如果进程上下文和一个下半部共享数据，在访问这些数据之前，需要禁止下半部的处理并得到锁的使用权。为了本地和SMP的保护，其实就是当前处理器和其他处理器的保护，防止出现死锁。

如果中断上下文和一个下半部共享数据，在访问数据之前，需要禁止中断并得到锁的使用权

任何在工作队列中被共享的数据也需要使用锁机制。其中有关锁的要点和在一般内核线程中没什么区别，因为工作队列本来就是在进程上下文中执行的。

### 8.7禁止下半部

一般单纯禁止下半部的处理是不够的。为了保证共享数据的安全，更常见的做法是，先得到一个锁，然后再禁止下半部处理。驱动程序中通常使用的都是这种方法，其实ad7606就考虑到了这个问题，在下半部中判断下缓冲区是否使能，如果没有使能就是唤醒被阻塞的进程然后返回。如果编写的是内核的核心代码，可能仅需要禁止下半部就可以了。

如果需要禁止所有的下半部处理（就是禁止所有的软中断和所有的tasklet），可以调用local_bh_disable()函数。允许下半部进行处理，可以调用local_bh_enable()函数。没错，这些函数的命名也有问题；但是BH接口让位给软中断了，所以无所谓了。

下半部机制控制函数的清单

| 函数                    | 描述                                  |
| ----------------------- | ------------------------------------- |
| void local_bh_disable() | 禁止本地处理器的软中断和tasklet的处理 |
| void local_bh_enable()  | 激活本地处理器的软中断和tasklet的处理 |

这些函数有可能被嵌套使用，最后被调用的local_bh_enable()激活下半部。

函数通过preempt_count（这个计数器很眼熟了，内核抢占的时候需要判断下当前抢占进程是否安全，也就是被抢占的进程是否携带了锁）为每个进程维护一个计数器。当计数器变为0的时候，下半部才能够被处理。因为下半部的处理已经被禁止，所以local_bh_enable()还需要检查所有现存的待处理的下半部并执行他们。

这些函数与硬件体系结构相关，位于<asm/softirq.h>中，通常由一些复杂的宏实现。下面是为那些好奇的人准备了C语言的近似描述：

```c
//通过增加preempt_count禁止本地下半部
void local_bh_disable(void)
{
    struct thread_info *t = current_thread_info();//获取当前进程描述结构体的第一个成员。
    t->preempt_count += SOFTIRQ_OFFSET;//增加计数器，从而禁止本地下半部的执行，此时表示调用函数的进程不可以安全的被抢占，但这和禁止本地下半部有啥关系。
}
//减少preempt_count如果该返回值为0，将导致自动激活下半部，是不是调用函数的进程被抢占与否和下半部的激活有关系
void local_bh_enable(void)
{
    struct threa_inffo *t = current_thread_info();
    t->preempt_count -= SOFTIRQ_OFFSET;
   if(unlikely(!t->preempt_count && softirq_pending(smp_processor_id())))
        do_softirq();//所谓的激活下半部不就是通过计数器作为标志位判断是否执行下半部而已
}
```

这些函数并不能禁止工作队列的执行。因为工作队列是在进程上下文中运行的，不会涉及异步执行的问题，所以也就没有必要禁止他们执行。由于软中断和tasklet是异步发生的这里是因为中断是异步的而他们是在中断处理返回的时候调用的，所以，内核代码必须禁止他们。另一方面，对于工作队列来说，它保护共享数据所做的工作和其他进程上下文差不多。

### 8.8小结

-----

下半部侧重于软件、和处理器架构本身关系不大了，也没有提使用的是谁的内核栈。

----

在本章中，我们涵盖了用于延迟Linux内核工作的三种机制：软中断、tasklet和工作队列。考察了其设计与实现，讨论了如何把这些机制应用到代码中，以及他们易于混淆的命名。为了完整起见，也考察了曾经的下半部机制：BH好任务队列（但我没学这两个）

因为下半部中很多地方用到了同步和并发，所以本章谈了很多相关的话题。也讨论了下禁止下半部的问题。第九章将从理论上讨论内核同步和并发，为理解这一问题的本质打下基础。第十章讨论内核为解决这一问题所提供的具体接口。以这两章为基石。

## 第九章内核同步介绍

在使用共享内存（不限于共享内存，准确说是共享资源）的应用程序中，程序员必须特意留意保护共享资源，防止共享资源并发访问。内核也不例外。共享资源之所以要防止并发访问，是因为如果多个执行线程同时访问和操作数据，可能发生各线程之间相互覆盖共享数据的情况，造成被访问数据处于不一致状态。并发访问共享数据是造成系统不稳定的一类隐患，就和可重入问题一样，也是并发访问全局变量导致的，而且这种错误一般难以跟踪好调试，所以首先应该认识到这个问题的重要性。

要做到对共享资源的恰当保护往往很困难。多年之前，在Linux还未支持对称对处理器SMP的时候，避免并发访问数据的方法相对来说比较简单。在单一处理器的时候，只有在中断发生的时候，或在内核代码明确地请求重新调度、执行另一个任务的时候，数据才可能被并发访问。因此早期内核开发工作相比如今要简单很多。

但当年的太平日子一去不复返了，从2.0版开始，内核就开始支持对称多处理器了，而且从那以后对它的支持不断的加强和完善。支持多处理器意味着内核代码可以同时运行在两个甚至更多的处理器上。因此，如果不加保护，运行在两个不同处理器上的内核代码完全可能在同一时刻里并发访问共享数据。随着2.6版内核出现，Linux内核已发展成抢占式内核，这意味着调度程序可以在任何时刻抢占正在运行的内核代码，重新调度其他的进程执行。现在，内核代码中有不少部分都能够同步执行，而他们都必须妥善保护起来。

本章我们先提纲指令地讨论操作系统内核中的并发和同步问题，第十章详细介绍Linux内核为解决同步问题和防止产生竞争条件而提供的机制及接口。

### 9.1临界区和竞争条件

所谓临界区就是访问和操作共享数据的代码段。多个执行线程并发访问同一个资源是不安全的，为了避免在临界区中并发访问，编程者必须保证这些代码原子地执行，也就是说，操作在执行结束前不可被打断，就如同整个临界区是一个不可分割的指令一样。

如果两个执行线程有可能处于同一个临界区中 同时执行，那么这就是程序包含的一个bug。称为竞争条件，这样命名是因为这里会存在线程竞争。这种情况出现的机会往往非常小，就是因为竞争引起的错误非常不易重现，所以调试起来非常麻烦。

 什么是并发与竞争？ 并发： 多个“用户”同时访问一个共享的内存。 竞争： 多个“用户”同时访问一段共享的内存并对其修改，就会造成数据混乱，甚至程序崩溃，这就是竞争。 

 什么是同步？ 显然，我们不希望这种情况发生，就有了对临界资源的保护，让每个时间只能有一个进程/线程访问，甚至有序访问，这就叫同步 （这里的同步和同步中断不是一个概念，同步中断值得是要和处理器时钟同步，因为是内部中断），中断是异步执行的，就是说无法做到有序访问。

术语执行线程指任何正在执行的代码实例，比如，一个在内核执行的进程、一个中断处理程序或者一个内核线程。

#### 9.1.1为什么我们需要保护

为了认清同步的必要性，我们首先要明白临界区无处不在。作为第一个例子，让我们考察一个现实世界的情况：ATM(自动柜员机，或叫自动提款机)。

自动提款机所进行的主要操作就是从个人银行账户取钱。某人走到机器前，插入ATM卡，输入密码作为验证，选择取款，输入金额，敲确认，取出钱，然后发信息通知本人。

当用户要求取某一特定的金额后，提款机需要确保在其账户上的确有那么多钱。如果有，取款机就要从现有的总额中扣除取款额。实现这一描述的代码如下：

```c
int total = get_total_from_account();//账户总额
int withdrawal = get_withdrawal_amount();//用户要求的取款额
if(total < withdrawal){
    error("You do not have that much money");
    return -1;//检查下账户余额是否足够
}
//如果余额足够的话
total -= withdrawal;
update_total_funds(total);//扣除提取的钱，将剩余的钱更新给银行
//然后把提取的钱交给用户
spit_out_money(withdrawal);
```

现在让我们假定在用户账户上的另一个扣款操作同时发生。看看扣款是如何同时发生的：假定用户的配偶在另一台ATM上开始另外的取款，这和上面的同时进行。

正在取款的两个系统都会执行刚才看到的代码，必须在某些操作期间对账户加锁，确保每个事物相对其他任何事物的操作是原子性的，这样的事物必须完整地发生，要么干脆不发生，但是决不能被打断。

#### 9.1.2单个变量

现在，让我们看一个特殊计算的例子。考虑一个非常简单的共享资源：一个全局整形变量和一个简单的临界区，其中的操作仅仅是将整型变量的值增加1

```c
i++
```

该操作可以转换成类似于下面动作的机器指令序列：

```c
//得到当前变量i的值并且拷贝到一个寄存器
//将寄存器中的值加1
//把i的新值写回到内存中
```

拷贝到寄存器的原因是，数据需要进行运算，这是由cpu实现的，所以要拷贝到cpu中特定的寄存器中，突然疑惑了

现在假定有两个执行线程同时进入这个临界区，两个原子操作交错执行就不会发生，因为处理器会从物理上确保这种不可能。使用这样的指令会缓解这种问题，内核也提供了一组实现这些原子操作的接口。

学过的内容，跳过了

9.2加锁

现在讨论一个更为复杂的竞争条件，相应的解决方法也更为复杂。假设需要处理一个队列上的所有请求。我们假定该队列是通过链表得以实现，链表中的每个节点都代表一个请求。有两个函数可以用来操作此队列：一个函数将新请求添加到队列尾部，另一个函数从队列头删除请求，然后处理它。内核各个部分都会调用这两个函数，所以内核会不断的从队列中删除和处理请求。对请求队列的操作无疑要用到多条指令。如果一个线程试图读取队列，而这时另一个线程正在处理该队列，那么读取线程就会发现队列此刻正处于不一致状态，这就是竞争，很明显，如果允许并发访问队列，就会产生危害。当共享资源是一个复杂的数据结构时，竞争条件往往会使该数据结构遭到破坏。

表面上看，这种情况好像没有一个好的方法来解决，一个处理器读取队列的时候，我们怎么能禁止另一个处理器更新队列呢，虽然有些体系结构提供了简单的原子指令（汇编指令），实现算术运算和比较之类的原子操作，但让体系结构提供专门的指令，对像上列中那样的不定长度的临界区进行保护，就强人所难了。这就是锁机制和原子操作的区别。后者是定义原子类型的数据，在数据被操作的时候不会被打断这是因为汇编指令具有原子性。

现在需要一种方法确保一次有且只有一个线程对数据结构进行操作，或者当另一个线程在对临界区标记时，就禁止其他访问。（临界区指的是共享资源的区域）

锁提供的就是这种机制：他就如同一把门锁，门后的房间可想象成一个临界区。在一个指定时间内，房间里只能有一个执行线程存在，线程持有锁，而锁保护了数据。

前面例子中讲到的请求队列，可以使用一个的单独的锁进行保护。每当有一个新请求要加入队列，线程首先占住锁，然后就可以安全地将请求加入到队列中，结束操作后再释放该锁。任何要访问队列的代码首先需要占住相应的锁，这样该锁就能阻止来自其他执行线程的并发访问。

请注意锁的使用是自愿的、非强制的，他完全属于一种编程者自选的编程手段。没有什么可以强制编程者必须使用锁。

锁有多种多样的形式，而且加锁的粒度范围也各不相同，Linux自身实现了几种不同的锁机制。各种锁机制之间的区别主要在于：当锁已经被其他线程持有，因而不可用时的行为表现，一些锁被抢用的时候会简单地执行忙等待，而另外一些锁会使当前任务睡眠直到锁可用为止。第十章将讨论Linux中不同锁之间的行为差别以及他们的接口。

一些人会认为锁根本解决不了什么问题，只不过是把临界区缩小到加锁和开锁之间的代码。其实就是把锁当成一个共享资源，大家都来抢占他。所幸的是，锁是采用原子操作实现的，而原子操作不存在竞争。这句话说的很好，就是把临界区缩小到加锁和解锁上，有可能出现加锁的时候别人给解锁了，结果锁没加上这种情况，但是原子操作又避免了这种情况。

在流行的x86体系结构中，锁的实现也不例外，他使用了称为compare和exchange的类似指令。

#### 9.2.1造成并发执行的原因

用户空间之所以需要同步，是因为用户程序会被调度程序抢占好重新调度。由于用户金成功可能跟在任何时刻被抢占，而调度程序完全可能选择另一个高优先级的进程到处理器上执行，所以就会使得一个程序正处于临界区时，被非自愿的抢占了。如果新调度的进程随后也进入同一个临界区，前后两个进程相互之间就会产生竞争，另外，因为信号处理是 异步发生的，所以即使是单线程的多个进程共享文件，或者在一个程序内部处理信号，也可能产生竞争条件。这种类型的并发操作，这里其实两者并不真是同时发生（同时发生指的是真正意义上的并行），但是他们相互交叉进行，所以可称为伪并发执行。

这两种其实就是并行与并发之间的关系，只不过并行称为真并发。

如果有一台SMP的机器，那么两个进程就可以真正地在临界区中同时执行了，这种类型称为真并发，虽然真并发好伪并发的原因好含义不同，但他们同样会造成竞争条件，需要保护。

内核中有类似可能造成并发执行的原因，他们是;

- 中断，中断几乎可以在任何时刻异步发生（不按照顺序发生），也就可能随时打断当前正在执行的代码。
- 软中断和tasklet，内核能在任何时刻唤醒和调度软中断和tasklet，这是因为中断会调度他们，而中断是任何时刻都能发生的，打断当前正在执行的代码。
- 内核抢占，因为内核具有抢占性，所以内核中的任务可能会被另一任务抢占。
- 睡眠以及用户空间的同步，在内核执行的进程可能会睡眠，这就会唤醒调度程序，从而导致调度一个新的用户进程执行。
- 对称多处理，并行执行代码。

对内核开发者来说，必须理解上述这些并发执行的原因，并且为他们实现做足准备工作。如果在一段内核代码操作某资源的时候系统产生了一个中断，而且该中断的处理程序还要访问这一资源，这就是一个bug，类似的如果一段内核代码在访问一个共享资源期价可以被强占也是一个bug，如果内核代码在临界区里睡眠了，那任何一个代码只要接触到这个临界区，睡眠的代码就竞争了。最后还要注意，两个处理器绝对不能同时访问同一共享数据，当我们清楚什么样的数据需要保护时。提供锁来保护系统稳定也就不难做到了。真正的困难是发现潜在竞争条件的可能，并有意识的采取某些措施来防止并发执行。

要重申这点，因为它实在是很重要，真正用锁来保护共享资源并不困难，尤其是在设计代码的早期，（锁和原子操作区别在于锁是将临界区缩小到加锁解锁上面，而原子操作就是用汇编指令 实现数据操作的不可再分，并且锁并不是加锁解锁就完事了，还有的加锁失败导致后对进程的处理），真正难的是辨认出真正需要共享的数据的相应的临界区，才是真正有挑战性的地方。要记住，最开始设计代码的时候就要考虑加入锁，而不是事后才想到。如果代码已经写好了，再在其中找到需要上锁的部分并向其中追加锁，这就是非常困难的，所以在编写代码的开始阶段就要设计恰当的锁。

在中断处理程序中能 避免并发访问的安全代码称为中断安全代码（interrupt-saft），在对称多处理的机器中能避免并发访问的安全代码称为SMP安全代码（SMP-safe），在内核抢占时能避免并发访问的安全代码称为抢占安全代码（preempt-safe），在第十章会重点讲述为了提供同步和避免所有上述竞争条件，内核所使用的的实际方法。

#### 9.2.2了解要保护些什么

找出哪些数据需要保护是关键所在。由于任何可能被并发访问的代码都几乎无例外地需要保护，所以寻找哪些代码不需要保护反而更容易些，我们也就从这里入手。执行线程的局部数据仅仅被它本身访问，显然不需要保护，比如，局部自动变量（还有动态分配的数据结构，其地址仅存放在堆栈中）不需要任何形式的锁，因为他们独立存在于执行线程的栈中。类似的如果数据只会被特定的进程访问，那么也不需要加锁（因为进程一次只在一个处理器上执行）。

到底什么数据需要加锁？大多数内核数据结构都需要加锁，有一条很好的经验可以帮助我们判断：如果有其他执行线程可以访问这些数据，就给这些数据加锁，如果任何其他什么东西都可以看到他，就锁住，是给数据而不是给代码加锁。

##### 配置选项：SMP和UP

因为Linux内核可在编译时配置，所以你可以针对指定机器进行内核裁剪。更重要的是，CONFIG_SMP配置选项控制内核是否支持SMP。许多加锁问题在单处理器上是不存在的，因而当CONFIG_SMP没被设置时。不必要的代码就不会被编入针对单处理器的内核映像中。这样做可以使单处理器机器避免使用自旋锁带来的开销。同样的技巧也适用于CONFIG_PREEMPT（允许内核抢占的配置选项）。这种设计真的很优越，内核只用维护一些简洁的基础资源，各种各样的锁机制当需要时可随时被编译进内核使用。在不同的体系结构上，CONFIG_SMP好CONFIGI_PREEMPT设置不同，实际编译时包含的锁就不同。

在代码中，要为大多数糟糕的情况提供适当的保护，例如具有内核抢占的SMP，并且要考虑到所有的情况。

在编写内核代码时，要问自己下面问题：

- 这个数据是不是全局的，除了当前线程外，其他线程 能够不能访问它
- 这个数据会不会在进程上下文和中断上下文共享？他是不是要在两个不同的中断处理程序中共享？
- 进程在访问数据时可不可能被抢占，被调度的新程序会不会访问同一数据
- 当前进程是不是会睡眠在某些资源上，如果是，他会让共享数据处于何种状态？
- 怎样防止数据失控
- 如果这个函数又在另一个处理器上被调度将会发生什么
- 如何确保代码远离并发威胁

简而言之，几乎访问所有的内核全局变量和共享数据都需要某种形式的同步方法，具体加锁方法在第十章进行讨论。

### 9.3死锁

死锁的产生需要一定条件：要有一个或多个执行线程和一个或多个资源，每个线程都在等待其中的一个资源，但所有的资源都已经被占用了，所有线程都在互相等待，但他们永远不会释放已经占有的资源，所以任何线程都无法继续，意味着死锁的发生。

最简单的死锁例子是自死锁：如果一个执行线程试图去获得一个自已已经持有的锁，他将不得不等待锁被释放，但因为它正在忙着等待这个锁，所以自己永远也不会有机会释放锁，最终结果就是死锁，有的锁可以被加锁多次。

同样道理，考虑有n个线程和n个锁，如果每个线程都持有一把其他人需要得到的锁，那么所有的线程都将阻塞地等待他们希望得到的锁重新可用。最常见的例子是有两个线程好两把锁，他们通常被叫做ABBA死锁。

每个线程都在等待其他线程持有的锁，但是绝没有一个线程会释放他们持有的锁。

有些内核提供递归锁来防止自死锁现象，递归锁可以被一个执行线程多次请求，幸好Linux没有提供这样的递归锁。不用递归锁是一个好事，虽然递归锁缓解了自死锁问题，但他们很容易使加锁逻辑变得杂乱无章。

预防死锁的发生非常重要，虽然很难证明代码不会发生死锁，但是可以写出避免死锁的代码，一些简单的规则对避免死锁大有帮助：

- 按顺序加锁。使用嵌套的锁时必须保证以相同的顺序获取锁，这样可以阻止致命拥抱类型的死锁。最好能记录下锁的顺序，以便其他人也能照此顺序使用。
- 防止发生饥饿，代码的执行必须会结束，不能一直把持着锁
- 不要重复请求同一个锁，自死锁
- 设计应力求简单，越复杂的加锁方案越有可能造成死锁。

最值得强调的第一点，它最为重要，如果有两个或多个锁曾在同一时间里被请求，那么以后其他函数请求他们也必须按照前次的加锁顺序进行。如果不按照前次加锁顺序有可能发生死锁现象。

比如线程一：获取锁a,b,试图获取c，等待锁c

线程二：获取锁c，试图获取锁b，等待锁b

这就导致了两个线程需要彼此获得的锁而不得导致死锁了。

只要嵌套地使用多个锁，就必须按照相同的顺序去获取他们，在代码上使用锁的地方，对锁的获取顺序加上注释是个良好的习惯。

```c
cat_lock用于保护访问cat数据结构的锁，总是要在获取dog锁前先获取
```

尽管释放锁的顺序好死锁是无关的，但最好还是以获取锁的相反顺序来释放锁。

### 9.4争用和扩展性

锁的争用（lock contention），或简称争用，是指当锁正在被占用的时候，有其他线程试图获取该锁，一个锁处于高度争用状态，就是指有多个其他线程在等待获得该锁。由于锁的作用是使程序以串行的方式对资源进行访问，所以使用锁无疑会降低系统的性能。被高度争用（频繁被持有或者长时间持有，两者都很糟糕）的锁会成为系统的瓶颈，严重降低系统性能。即使是这样，相比于被几个相互抢夺共享资源的线程撕成碎片，搞得内核崩溃要好，不过最好还是解决高度争用的问题。

扩展性（scalability）是对系统可扩展程度的一个量度。对于操作系统，我们在谈及可扩展性时就会和大量进程、大量处理器或是大量内存联系起来。其实任何可以被计量的计算机组件都可以涉及到可扩展性。理想情况下，处理器的数量加倍应该会使系统处理性能翻倍。但实际上，这是不可能达到的。

自从2.0版内核引入多处理支持后，Linux对集群处理器的可扩展性大大提高了。在Linux刚加入对多处理器支持的时候，一个时刻只能有一个任务在内核中执行；在2.2版本中，当加锁机制发展到细粒度加锁后，便取消了这种限制，而在2.4和后续版本中，内核加锁的粒度变得越来越精细。如今在Linux2.6版内核中，内核加的锁是非常细的粒度，可扩展性也很好。

加锁粒度用来描述加锁保护的数据规模。一个过粗的锁保护大块数据，比如一个子系统用到的所有的数据结构，那粒度太粗了；相反，一个过于精细的锁保护很小的一块数据，比如，一个大数据结构中的一个元素。在实际使用中，绝大多数锁的加锁范围都处于上述两种极端之间，保护的既不是完整的子系统也不是独立元素，而是一个单独的数据结构。许多锁的设计在开始阶段都很粗，但当锁的争用问题变得严重时，设计就向更加精细的加锁方向进化。

在第四章中讨论过的运行队列，就是一个锁从粗到精细化的实例。在2.4和更早的内核中，调度程序有一个单独的调度队列，在2.6版内核版本的早期版本中，O(1)调度程序为每个处理器单独配备一个运行队列，每个队列拥有自己的锁，于是加锁由一个全局锁进化到了每个处理器拥有各自的锁这是一种重要的优化，因为运行队列锁在大型机器上被争着用，本质上就是要在调度程序中每次都把整个调度进程下放到处理器上运行。在2.6版内核系列的新进版本中，CFS调度器进一步提升了锁的可扩展性。

一般来说，提高可扩展性是件好事，因为它可以提高Linux在更大型的、处理能力更强大的系统上的性能。但是一味地提高可扩展性，却会导致Linux在小型SMP和UP机器上的性能降低，这是因为小型机器可能用不到特别精细的锁，锁过于精细只会增加复杂度，并加大开销。考虑一个链表，最初的加锁方案可能就是用一个锁来保护链表，后来发现，在拥有集群处理器机器上，当各个处理器需要频繁访问该链表的时候，只用单独一个锁却成了扩展性的瓶颈。为解决这个瓶颈，我们将原来加锁的整个链表变成为链表中的每一个节点都加入自己的锁，这样以来，如果要对节点进行读写，必须先得到这个节点对应的锁。将加锁粒度变细后，多处理器访问同一个节点是，只会争用一个锁。但这是锁的争用仍然没有避免，能不能为每个节点中国的每个元素都提供一个锁呢，不能，严格来讲，即使这么细的锁可以在大规模SMP机器上执行的很好，但他在双处理器上表现的不是那么明显，多余的锁会加大系统开销，造成很大的浪费。

不管怎么说，可扩展性都是很重要的，需要慎重考虑。关键在于，在设计锁的开始阶段就应该考虑到要保证良好的扩展性。（总结来说，可扩展性指的就是Linux在处理能力更强的系统上保持良好的性能，而关键在于锁的细粒度性）。

因为即使在小型机器上，如果对重要资源锁的太粗，也很容易造成系统性能瓶颈。锁加的过粗或者过细，差别往往只在一线之间。当锁争用严重时，加锁太粗会降低可扩展性；而锁争用不明显时，加锁过细会加大系统开销，带来浪费，这两种情况都会造成系统性能下降。

重点是设计初期加锁方案应该力求简单，仅当需要时再进一步细化加锁方案。

### 9.5小结

要编写SMP安全代码，不能等到编码完成后才考虑如何加锁，恰当的同步需要从头到尾，在整个编码过程中不断考虑与完善。无论你在编写哪种内核代码，是新的系统调用也好，还是重写驱动程序也好，首先应该考虑的就是保护数据不被并发访问，记住加锁数据。

第十章将讨论如何为SMP、内核抢占和其他各种情况提供充分的同步保护，确保数据在任何机器和配置上安全。了解了同步、并发和加锁的基本原理之后，让我们潜心钻研Linux内核提供的实际工具，以确保你的代码有竞争力但免于死锁。

## 第十章内核同步方法

第九章讨论了竞争条件为何会产生以及怎么去解决，幸运的是，Linux内核提供了一组相当完备的同步方法，这些方法使得内核开发者们能编写出高效而又自由竞争的代码。本章讨论的是这些方法，包括他们的接口、行为和用途。

### 10.1原子操作

我们首先介绍同步方法中的原子操作，因为他是其他同步方法的基石。原子操作可以保证指令以原子的方式执行，执行过程不被打断。众所周知，原子操作也就是不能够被切割的指令。例如，第九章提到的原子方式的加操作，他通过把读取和增加变量的行为包含在一个单步中执行，从而防止了竞争的发生保证了操作结果总是一致的。

内核提供了两组原子操作接口，一组针对整数进行操作，一组针对单独的位进行操作。在Linux支持的所有体系结构中都实现了这两组接口。大多数体系结构会提供支持原子操作的简单算法指令。而有些体系结构确实缺少简单的原子操作指令，但是也为单步执行提供了锁内存总线的指令，这就确保了其他改变内存的操作不能同时发生。

#### 10.1.1原子整数操作

针对整数的原子操作只能对atomic_t类型的数据进行处理。这里之所以引入了一个特殊数据类型，而没有直接使用c语言的int类型，主要是出于两个原因：首先，让原子函数只接收atomic_t类型的操作数，可以确保原子操作只与这种特殊类型数据一起使用。同时，这也保证了该类型的数据不会被传递给任何非原子函数。实际上，对一个数据一会采用原子操作，一会又不用原子操作了，这有什么好处呢？

其次，使用atomic_t类型确保编译器不对相应的值进行**访问优化**，这点使得原子操作最终接收到正确的内存地址，而不是一个别名。（编译器还会把数据给个别名，这属于编译器原理部分了，无法深入）。最后，在不同体系结构上实现原子操作的时候，使用atomic_t可以屏蔽其中的差异。Atomic_t类型定义在文件<linux/types.h>中：

```c
typedef struct{
	volatile int counter;
}atomic_t;
```

尽管Linux支持的所有机器上的整形数据都是32位的，但是使用atomic_t的代码只能将该类型的数据当做24位来使用。这个限制完全是因为在SPARC体系结构上，原子操作的实现不同于其他体系结构：32位int类型的低八位被嵌入了一个锁，因为SPARC体系结构对原子操作缺乏指令级的支持，所以只能利用该锁避免对原子类型数据的并发访问。

![1670722747359](C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1670722747359.png)

一目了然，低八位嵌入一个锁，从而实现对该整形数据的原子操作。锁是把临界区缩小到加锁解锁行为上，而加锁解锁具有原子性，而现在操作锁就是操作这个数据，所以变成了对这个数据操作具有原子性了。根据嵌入的锁的不同，其他试图操作原子类型数据的进程也会自旋或者阻塞。

所以在SPARC机器上就只能使用过24位了，虽然其他机器上的代码完全可以使用全部的32位，但在SPARC机器上却可能造成一些奇怪和微妙的错误，目前已经允许SPARC提供全32位的atomic_t。

使用原子整形操作需要的声明都在<asm/atomic.h>文件中。有些体系结构会提供一些只能在该体系结构上使用的额外原子操作方法，但所有的体系结构都能保证内核使用到的所有操作的最小集。在写内核代码时，可以肯定这个最小操作集在所有体系结构上都已经实现了，也就是说无论是哪个体系结构，内核的操作都是合法的。

定义一个atomic_t类型的数据方法很平常，还可以在定义时给它设定初值：

```c
atomic_t v;
atomic_t u = ATOMIC_INIT(0);
```

操作也都非常简单：

```c
atomic_set(&v,4);//v = 4
atomic_add(2,&v);//v = v+2 = 6
atomic_inc(&v);//v = v + 1 = 7
```

如果需要将atomic_t转换成int类型，可以使用atomic_read()来完成：

```c
printk("%d\n",atomic_read(&v));
```

原子整数操作最常见的用途就是实现计数器。使用复杂的锁机制来保护一个单纯的计数器显然杀鸡用了宰牛刀，所以，开发者最好使用atomic_inc()和atomic_dec()这两个相对来说轻便一些的操作。

还可以用原子整数操作原子地执行一个操作并检查结果。一个常见的例子就是原子地减操作和检查。

```c
int atomic_dec_and_test(atomic_t *v);
```

这个函数将给定的原子变量减1，如果结果为0，就返回真；否则返回假，下表列出了所有的标准原子整数操作，某种特定的体系结构上实现的所有操作可以在文件<asm/atomic.h>中找到。

原子操作通常是内联函数，往往是通过内嵌汇编指令实现的。正是因为使用汇编指令所以才具有原子性，内联函数也称为字里行间展开函数，毕竟使用很频繁，而函数本身占空间很小，编译的时候编译成内联函数反而可以减少跳转次数，对内存开销也不大，有效的提高系统效率。

如果一个函数本来就是原子的，往往会被定义成一个宏。例如，在大部分体系结构上，读取一个字本身就是一种原子操作，也就是说，在对一个字进行写入操作期间不可能完成对该字的读取。这样，把atomic_read()定义成一个宏，只需返回atomic_t类型的整数值就可以了。

```c
static inline int atomic_read(const atomic_t *v)
{
return v->counter;//其实atomic_t是结构体类型，只有一个成员counter这是int类型
}
```

##### 原子性和顺序性的比较

关于原子读取的上述讨论引发了原子性和顺序性之间差异的讨论，正如所讨论的，一个字长的读取总是原子地发生，绝不可能对同一个字交错地进行写；读总是返回一个完整的字，这货在发生在写之前或者之后，但绝不可能发生在写的过程中。

也许代码有着更高的要求，要求读必须在待定的写之前发生，这种需求其实不属于原子性的要求，而是顺序要求。原子性确保指令执行期间不被打断，要么全部执行完，要么根本不执行。另一方面，顺序性确保即使两条或多条指令出现在独立的执行线程中、甚至独立的处理器上他们本该的执行顺序却依然要保持。

在本小节讨论的原子操作只保证原子性。顺序性通过屏蔽（barrier）指令来实施，以后讨论。

在编写代码的时候，能使用原子操作时，就尽量不要使用复杂的加锁机制。对多数体系结构来说，原子操作与更复杂的同步方法相比，给系统带来的开销小，对高效缓存行（cache-line）的影响也小。这个高速缓存行是个啥（高速缓存是一组称之为缓存航的固定大小的数据块），但是，对于那些有高性能要求的代码，对多种同步方法进行测试比较，不失为一种明智的做法。

#### 10.1.2 64位原子操作

随着64位体系结构越来越普及，内核开发者确实在考虑原子操作除32位atomic_t类型外，也应引入64位的atomic64_t，因为移植性原因，atomic_t变量大小无法在体系结构之间改变，所以，atomic_t类型即便在64位体系结构下也是32位的，若要使用64位的原子变量，则要使用atomic64_t类型，其功能和其32位的兄弟没区别，使用方法完全相同，不同的只有整型变量大小从32位变成了64位这里的整型变量指的就是atomic64_t->counter是64位的。几乎所有的经典32位原子操作都有64位的实现，他们被冠以atomic64前缀，而32位实现冠以atomic前缀。关于64位原子操作列表就不展示了，有些体系结构实现的方法更多，但是没有移植性。和atomic_t一样，atomic64_t类型其实是对长整型的一个简单封装类。

```c
typedef struct {
	volatile long counter;
}atomic64_t;
```

所有64位体系结构都提供了atomic64_t类型，以及一组对应的算数操作方法。但是多数32位体系结构不支持atomic64_t类型，不过x86-32是一个众所周知的例外。

为了便于在Linux支持的各种体系结构之间移植代码，开发者应该使用32位的atomic_t类型，把64位atomic64_t类型留给那些特殊体系结构和需要64位的代码。

#### 10.1.3原子位操作

除了原子整数操作外，内核也提供了一组针对位这一级数据进行操作的函数。定义在文件<asm/bitops.h>

令人感到奇怪的是位操作函数是对普通的内存地址进行操作的。他的参数是一个指针和一个位号，第零号是给定地址的最低有效位。在32位机上，第31位是给定地址的最高有效位而第32位是下一个字的最低有效位。虽然使用原子位操作在大多数情况下是对一个字长的内存进行访问，因而位号应该位于0~31，但是，对位号的范围并没有限制。

由于原子位操作是对普通的指针进行的操作，所以不像原子整型对应atomic_t，这里没有特殊的数据类型。相反只要指针指向了任何你希望的数据，就可以对它进行操作。之前的原子操作很清晰，内部实现就是汇编将数据存入寄存器然后运算在提取出来。

这里是使用了指针，和寄存器、汇编怎么联系的呢。

```c
unsigned long word = 0;
set_bit(0,&word);//第0位被设置
set_bit(1,&word);//第一位被设置
printk("%u1\n",word);//打印成二进制的11也就是3
clear_bit(1,&word);//清空第一位
change_bit(0,&word);//翻转第0位的值，这里被清空
//原子地设置第0位并且返回设置前的值
if(test_and_set_bit(0,&word)){
    //永远不为真
}
word = 7;//这个变量被原子位操作了，可以吧原子位指令与一般的C语句混在一起
```

为方便起见，内核还提供了一组与上述操作对应的非原子位函数。非原子位函数与原子为函数的操作完全相同，但是，前者不保证原子性，且其名字前缀多了两个下划线。例如test_bit()对应的非原子形式是__test_bit()。如果不需要原子操作，那么这些非原子的为函数相比原子的位函数可能会执行的更快些。

##### 非原子位操作是什么？

咋一看，非原子位操作没有任何意义。因为仅仅涉及一个位，所以不存在发生矛盾的可能。

原子性意味着，指令完整地成功执行完，不被打断，或者根本不执行。所以，如果连续执行两个原子位操作，你会希望两个操作都成功，在操作都完成后，位的值应该是第二个操作所赋予的。但是，在最后一个操作发生前的某一个时间点，位的值应该维持第一个操作赋予的。换句话说，真正的原子操作需要的是，所有中间结果都正确无误。

假如，假定给出两个原子位操作：先对某位置位，然后清零。如果没有原子操作，那么这一位可能的确清零了，可能根本没有清零，置位操作可能与清除操作同时发生，但没有成功。清除操作可能成功了。

现在，有了原子操作，置位会真正发生，可能有那么一刻，读操作显示所置的位，然后清除操作才执行，该位变为0了。

这种行为可能是重要的，尤其当顺序性开始起作用的时候，

他妈是这里不知道再说什么，说是解释下非原子位操作存在的意义，但是还是重复了下原子性在位操作上的重要性。

内核还提供了两个例程用来从指定的地址开始搜索第一个被设置的位或未被设置的位

```c
int find_first_bit(unsigned long *addr,unsigend int size)
int find_first_zero_bit(unsigned long *addr,unsigned int size)
```

这两个参数中第一个参数是一个指针，第二个参数是要搜索的总位数，返回值分别是第一个被设置或没被设置的位的位号。如果搜索范围仅限于一个字，使用_ffs()和ffz()这两个函数更好，他们只需要给定一个要搜索的地址做参数。

与原子整数操作不同，代码一般无法选择是否使用位操作（这句话不懂啥意思），他们是唯一的、具有可移植性的设置特定位方法，需要选择的是使用原子位操作还是非原子位操作。如果你的代码本身已经避免了竞争条件，你可以使用非原子位操作，这样执行更快，也要取决于具体的体系结构。

### 10.2自旋锁

如果每个临界区都能像增加变量这样简单就好了，可惜现实是残酷的。现实世界里临界区甚至可以跨越多个函数。（这也是锁机制和原子操作的区别，锁机制具有缩小临界区这个概念）。举个例子，我们经常能碰到这种情况：先得从一个数据结构中移出数据，对其进行格式转换和解析，最后再把它加入到另一个数据结构中。整个执行过程必须是原子的，在数据被更新完毕前，不能有其他代码读取这些数据。显然，简单的原子操作对此无能为力了，他们只能确保一个变量或者一个位实现原子性操作。这就需要使用更为复杂的同步方法，锁来提供保护。

Linux内核中最常见的锁是自旋锁（spin lock），自旋锁最多只能被一个可执行线程持有。如果一个执行线程试图获得一个被已经持有的自旋锁（此时这个锁就是高度争用的），那么该线程就会一直进行忙循环-等待锁重新可用。要是锁未被争用，请求锁的执行线程便能立刻得到它，继续执行。

在任意时间，自旋锁都可以防止多于一个的执行线程同时进入临界区。同一个锁可以用在多个位置，例如，对于给定数据的所有访问都可以得到保护和同步。

一个被争用的自旋锁使得请求他的线程在等待锁重新可用时自旋，但是特别浪费处理器时间，这种行为是自旋锁的要点。所以自旋锁不应该长期被持有。事实上，这点正是资源所的初衷，中断内使用自旋，因为中断属于运行在中断上下文中，没有后背进程那么此时调度器是找不到等待队列的就无法调度了，而且中断属于异步执行，所以无法休眠，而自旋锁即使得不到也不会导致中断休眠。

初衷：在短期内进行轻量级加锁。

还可以采用另外的方式来处理对锁的争用：让请求线程睡眠，直到锁重新可用时再唤醒它。这样处理器就不必循环等待，可以去执行其他代码。这种方式也会带来一定的开销，有两次明显的上下文切换，被阻塞的线程要换出和换入，与实现自旋锁的少数几行代码相比，上下文切换当然有更多的到吗，因此持有自旋锁的时间最好小于完成两次上下文切换的耗时。一般不会去测量上下文切换的耗时，所以让持有自旋锁的时间尽可能的短，这样使用自旋锁才有性价比。下面内容将讨论信号量，信号量提供了上述第二种锁机制，它使得在发生争用的时候，等待的线程能投入睡眠而不是旋转。

#### 10.2.1自旋锁方法

自旋锁的实现和体系结构密切相关，代码往往通过汇编实现。这些与体系结构相关的代码定义在文件<asm/spinlock.h>中，实际需要用到的接口定义在文件<linux/spinlock.h>。自旋锁的基本使用形式如下：

```c
DEFINE_SPINLOCK(mr_lock);
spin_lock(&mr_lock);
//临界区
spin_unlock(&mr_lock);
```

因为自旋锁在同一时刻至多被一个执行线程持有，所以一个时刻只能有一个线程位于临界区，这就为多处理器机器提供了防止并发访问所需的保护机制。注意在单处理器机器上，编译的时候并不会加入自旋锁。它仅仅被当做一个设置内核抢占机制是否被启用的开关。如果禁止内核抢占，那么在编译时自旋锁会被完全剔除出内核.

----

其实单处理器也不影响使用自旋锁吧,虽然无法实现并行，但是并发也可以使用自旋锁。

而且自旋锁跟内核抢占开关有什么关系，Linux支持 内核抢占，就是内核线程执行中被打断。

这里的之所以单处理器不使用自旋锁是防止中断处理程序获得自旋锁不成而进入自旋，此时将无法释放自旋锁，而中断处理程序也将一直自旋。并发也不可以，因为并发本质是时间片轮询，而这个其实是CFS完全公平调度器实现的，中断处理程序不是普通的进程，无法由调度器调度。

----

##### 警告：自旋锁是不可递归的

Linux内核实现的自旋锁是不可递归的，这点不同于自旋锁在其他操作系统中的实现，所以小心自旋锁。

自旋锁可以使用在中断处理程序中（这里不能使用信号量，因为中断处理程序不可以休眠，没有后备进程可供调度）。在中断处理程序中使用自旋锁时，一定要在获取锁之前，首先禁止本地中断（在当前处理器上的中断请求），否则中断处理程序机会打断正持有锁的内核代码，有可能去试图抢用这个已经被持有的自旋锁。这样一来，中断处理程序就会自旋，等待该锁重新可用，但是锁的持有者在这个中断处理程序执行完毕前不可能运行。（即使有时间片轮询，但这是中断处理程序啊，并不是CFS调度器调度的，而是异步执行的，是由硬件触发导致的，只有在中断处理程序全须全尾的执行完毕才会执行其他进程）。这就会导致前面内容提到的双重请求死锁，不过需要关闭的只有当前处理器上的中断。毕竟中断发生在不同的处理器上，即使中断处理程序在同一锁上自旋，也不会妨碍另一个处理器上的锁的持有者释放锁。

内核提供的禁止中断同时请求锁的接口，使用起来很方便，方法如下：

```c
DEFINE_SPINLOCK(mr_lock);
unsigned long flags;
spin_lock_irqsave(&mr_lock,flags);
//临界区
spin_unlock_irqrestore(&mr_lock,flags);
```

函数spin_lock_irqsave()保存中断的当前状态，并禁止本地中断，然后再去获取指定的锁。反过来spin_unlock_irqrestore()对指定的锁解锁，然后让中断恢复到加锁的状态。

这里其实就是上面的spin_lock函数的升级版。

在单处理器系统上，虽然在编译时抛弃掉了锁机制，但在上面例子中仍需要关闭中断，以禁止中断处理程序访问共享数据。

##### 锁什么？

使用锁的时候一定要对症下药，要有针对性，要知道需要保护的是数据而不是代码。尽管本章的例子讲的都是保护临界区的重要性，但是真正保护的其实是临界区中的数据，而不是代码。这个之前就提到过了，保护数据指的是临界段（其实临界段就是共享数据）。至于保护代码这个概念指的应该是防止这个代码段被打断吧。比如把一个线程扔到临界区内。

大原则：针对代码加锁会使得程序难以理解，并且容易引发竞争条件，正确的做法应该是对数据而不是代码加锁。

既然不是对代码加锁，就一定要用特定的锁来保护自己的共享数据。无论何时需要访问共享数据，一定要先保证数据是安全的。而保证数据安全往往就意味着在对数据进行操作前，首先占用恰当的锁，完成操作后释放它 。

如果能确定中断在加锁前是激活的，就不需要在解锁后恢复中断以前的状态了。无条件地在解锁时激活中断。这样使用

```c
DEFINE_SPINLOCK(mr_lock);
spin_lock_irq(&mr_lock);
//临界区
spin_unlock_irq(&mr_lock);
```

由于内核变得庞大而复杂，因此在内核的执行路线上，很难搞清楚中断在当前调用点上到底是不是处于激活状态。也正因为如此，不提倡使用spin_lock_irq()方法。

##### 调试自旋锁

配置选项CONFIG_DEBUG_SPINLOCK为使用自旋锁的代码加入了许多调试检测手段。激活了该选项，内核就会检查是否使用了未初始化的锁，是否在还没加锁的时候就要对锁执行开锁操作。在测试代码时，总是应该激活这个选项，内核就会检查是否使用了未初始化的锁，是否在还没加锁的时候就要对锁执行开锁操作。在测试代码时，总是应该激活这个选项。如果需要进一步全程调试锁，还应该打开CONNFIG_DEBUG_LOCK_ALLOC选项。

#### 10.2.2其他针对自旋锁的操作

可以使用spin_lock_init()方法来初始化动态创建的自旋锁，不过此时只有一个指向spinlock_t类型的指针，没有它的实体。（之前就一直对实体充满了疑问，打个比方的话，指的应该是一个是结构体内容，一个是结构体的地址），这里并不会对spinlock_t进行初始化，仅仅是提供一个指针而已。

spin_try_lock()试图获得某个特定的自旋锁，如果该锁已经被争用，那么该方法会立刻返回一个非0值，而不会自旋等待锁被释放；如果成功地获得了这个自旋锁，该函数返回0.同理spin_is_locked()方法用于检查特定的锁当前是否已被占用，如果已被占用，返回非0值；否则返回0。

#### 10.2.3自旋锁和下半部

在第八章的时候曾经提到过，在与下半部配合使用时，必须小心地使用锁机制。函数spin_lock_bh()用于获取指定锁，同时它会禁止所有下半部的执行。相应的spin_unlock_bh()函数执行相反的操作。

由于下半部可以抢占进程上下文中的代码，所以当下半部和进程上下文共享数据时，必须对进程上下文中的共享数据进行保护（软中断、tasklet、工作队列），所以需要加锁的同时还要禁止下半部执行。同样，由于中断处理程序可以抢占下半部，所以如果中断处理程序和下半部共享数据，必须在获取恰当的锁的同时还要禁止中断。（这里的下半部应该是需要根据其实现方式来判断是否有必要禁用，禁用中断是因为中断执行完以前一直把持着CPU，造成死锁，而下半部中的tasklet和软中断也是这个路子，并不是普通的进程，但是工作队列就是普通进程应该不会造成死锁吧，，）

同类的tasklet不可能同时运行，所以对于同类的tasklet中的共享数据不需要保护。但是当数据被两个不同种类的tasklet共享时，就需要在访问下半部中的数据前先获取一个普通的自旋锁，这里不需要禁止下半部，因为在同一个处理器上不会有tasklet相互抢占的情况。

对于软中断，无论是否同种类型，如果数据在软中断共享，那么它必须得到锁的保护。这是因为即使是同种类型的两个软中断也可以同时运行在一个系统中的不同处理器上。但是同一处理器上的软中断绝不会抢占另一个软中断，因此没必要禁止下半部。

这里特意没提工作列表我估计就是因为其本身就是一个普通进程并不具备“无时间片观念，直接执行到底的思想”

### 10.3读-写自旋锁

有时，锁的用途可以明确地分为读取和写入两个场景。例如，对一个链表可能既要更新又要检索。当更新链表时，不能有其他代码并发地写写链表或从链表中读取数据，写操作要求完全互斥。另一方面，当对其检索链表时，只要其他程序不进行写操作就行。只要没有写操作，多个并发的读操作都是安全的。任务链表的存取模式就非常类似这种情况，他就是通过读写自旋锁获得保护的（之前讲过么，记不住了）。

当对某个数据结构的操作可以像这样被划分为读/写或者消费者/生产者两种类别时，类似读/写锁这样的机制就很有帮助了。为此，Linux内核提供了专门的读写自旋锁。这种自旋锁为读和写提供了不同的锁。一个或多个读任务可以并发地持有读者锁；相反，用于写的锁最多只能被一个写任务持有，而且此时不能有并发的读操作。（有问题！Linux的自旋锁是不可以递归的，这里的读锁不应该使用自旋锁更像是信号量）。有时把读写锁叫做共享排斥锁，或者并发排斥锁，因为这种锁以共享和排斥的形式获得使用。

读/写自旋锁的使用方法类似于普通自旋锁，他们通过下面的方法初始化：

```c
DEFINE_RWLOCK(mr_rwlock);
read_lock(&mr_rwlock);
//临界区
read_unlock(&mr_rwlock);

//在写者的代码分支中使用如下函数：
write_lock(&mr_rwlock);
//临界区
write_unlock(&mr_rwlock);
```

通常情况下，读锁和写锁会位于完全分割开的代码分支中。毕竟不能一个线程即想要读取又想要写入，这会导致它自旋的概率大大增加。

注意，不能把一个读锁升级为写锁，这里的升级其实就是加上的意思

```c
read_lock(&&mr_rwlock);
write_lock(&mr_rwlock);
```

执行上述两个函数将会带来死锁，因为写锁会不断自旋，等待所有的读者释放锁，其中页包括它自己。所以当确实需要写操作时，要一开始就请求写锁，如果写和读不能清晰地分开的话，那么使用一般的自旋锁就行了，不要使用读-写自旋锁。

多个读者可以安全地获得同一个读锁，这就说明即使一个线程递归地获得同一读锁也是安全的。这个特性使得读写自旋锁真正称为一种有用并且常用的优化手段。如果在中断处理程序中只有读操作而没有写操作，就可以混合使用中断禁止锁，使用read_lock()而不是read_lock_irqsave()对读进行保护。不过，还是需要用write_lock_irqsave()禁止有写操作的中断，否则，中断里的读操作就有可能锁死在写锁上。（凡是需要中断禁止的都是担中断处于自旋的状态，这样的话中断永远不会释放CPU，永远无法获得锁，造成死锁）。而这里的意思是读锁并不会导致中断自旋，但是写锁会导致。

假如读者正在进行操作，包含写操作的中断发生了，由于读锁还没有全部被释放，所以写操作会自旋，而读操作只能在包含写操作的中断返回后才能继续，释放读锁，此时死锁就发生了。

在使用Linux读写自旋锁时，最多要考虑的一点是这种锁机制照顾读比写要多一点。当读锁被持有的时候，写操作为了互斥访问只能等待，但是，读者却可以继续成功地占用读锁。而自旋等待的写者在所有读者释放锁之前是无法获得锁的，所以大量读者必定会使挂起的写者处于饥饿状态，在设计锁的时候要注意这一点，读锁确实被照顾的多一些，但写者就有更大概率处于饥饿状态了。

自旋锁提供了一种快速简单的锁实现方法。如果加锁时间不长并且代码不会睡眠（比如中断处理程序），利用自旋锁是最佳的选择，如果加锁时间可能很长或者代码在持有锁的时候休眠了，最好使用信号量来完成加锁功能。这里的休眠有啥影响，带着自旋锁休眠了，而自旋锁往往是不可迭代的，这就导致了哪些想要获取锁的进程自旋占据CPU，而锁还因为持有进程休眠而无法释放，造成死锁。

### 10.4信号量

Linux中的信号量是一种睡眠锁，如果有一个任务试图获得一个不可用（也就是已被占用的）信号量时，信号量 会将其推进到一个等待队列，然后让其睡眠，这时处理器能重获自由，从而去执行其他代码。（这就是和内核抢占的区别了，内核抢占是通过进程描述符的一个成员是专门描述是否有进程的优先级高于当前进程的，当存在并且安全抢占的时候，调度器进行抢占，这属于调度器实现的）而这里其实是信号量实现的。

从信号量的睡眠特性得出一些有意思的结论：

- 由于争用信号量的进程在等待锁重新变为可用时会睡眠，所以信号量适用于锁会被长时间持有的情况。
- 相反，锁被短时间持有是，使用信号量就不适宜了。因为睡眠、维护等待队列以及唤醒所花费的开销可能比锁被占用的全部时间还要长。
- 由于执行线程在锁被争用时会睡眠，所以只能在进程上下文中才能 获取信号量锁，因为在中断上下文中是不能进行调用的。
- 可以在持有信号量时睡眠，因为当其他进程试图获得同一信号量时不会因此而死锁，因为最后会把CPU资源空出来
- 在你占用信号量的同时不能占用自旋锁。因为在你等待信号量时可能会睡眠，而在持有自旋锁时是不允许睡眠。

以上这些结论阐明了信号量和自旋锁在使用上的差异，在使用信号量的大多数时候，你的选择余地并不大。往往在需要和用户空间同步时，你的代码会需要睡眠，此时使用信号量是唯一的选择。由于不受睡眠的限制，使用信号量通常来说更容易写一些，如果需要在自旋锁和信号量中做选择，应该根据锁被持有的时间长短做判断。理想情况当然是所有的锁定操作都应该越短越好。但如果你用的是信号量，那么锁定的时间长一点也能够接受。另外，信号量不同于自旋锁，他不会禁止内核抢占（这是因为自旋锁的特性是自旋+中断的特性是执行到底导致的，最终自旋到底也就是死锁），所以持有信号量的代码可以被抢占。这意味着信号量不会对调度的等待时间带来负面影响。

#### 10.4.1计数信号量和二值信号量

最后要讨论的是信号量的一个有用特性，它可以同时允许任意数量的锁持有者，而自旋锁在一个时刻最多允许一个任务持有它。信号量同时允许的持有者数量可以在声明信号量时指定。这个值称为使用者数量（usage count）或简单地叫数量。通常情况下，信号量和自旋锁一样，在一个时刻仅允许有一个锁持有锁者。这时计数等于1，这样的信号量被称为二值信号量（因为它或者由一个任务持有，或者没有任务持有），也称为互斥信号量，因为它强制进行互斥，另一方面，初始化时也可以把数量设置为大于1的非0值。这种情况，信号量被称为计数信号量，它允许在一个时刻至多有count个锁持有者。计数信号量不能用来进行强制互斥，因为它允许多个执行线程同时访问临界区。相反，这种信号量用来对特定代码加以限制，内核中使用他的机会不多。在使用信号量时，基本上用到的都是互斥信号量。

信号量逐渐成为一种常用的锁机制。信号量支持两个原子操作P()和V()，down操作通过对信号量计数减1来请求获得一个信号量。如果结果为负数任务会被放入等待队列，如果结果是大于等于0，获得信号量锁，任务进入临界区。该函数如同一个动词，降低一个信号量就等于获取该信号量。相反，当临界区中的操作完成后，up操作用来释放信号量，该操作称为提升信号量，因为它会增加信号量的计数值。如果在该信号量上的等待队列不为空，那么处于队列中等待的任务在被唤醒的同时会获得该信号量。

#### 10.4.2创建和初始化信号量

信号量的实现是与体系结构相关的，具体实现定义在文件<asm/semaphore.h>中。struct semaphore类型用来表示信号量。可以通过以下方式静态地声明信号量，其中name是信号量变量名，count是信号量的使用数量：

```c
struct semaphore name;
sema_init(&name,count);
```

创建更为普通的互斥信号量使用以下快捷方式，不用说，name仍然是互斥信号量的变量名：

```c
static DECLARE_MUTEX(name);
```

更常见的情况是，信号量作为一个大数据结构的一部分动态创建。此时，只有指向该动态创建的信号量的间接指针，可以使用如下函数来对他进行初始化（想起来了，之前的那些链表的父结构中就经常会包含一个成员就是信号量）：

```c
sema_init(sem,count);
```

sem是指针，count是信号量的使用者数量。

与前面类似，初始化一个动态创建的互斥信号量时使用如下函数：

```c
init_MUTEX(sem);
```

我不明白为什么mutes在init_MUTEX中是大写，或者为什么init这个函数名中放在前面，而在sema_init的时候放在后面。

#### 10.4.3使用信号量

函数down_interruptible()试图获取指定的信号量，如果信号量不可用，他将把调用进程设置为TASK_INTERRUPTIBLE状态，也就是可中断睡眠。这种进程状态意味着任务可以被信号唤醒，当进程被唤醒的时候而函数返回-EINTR，另一个函数down会让进程在TASK_UNINTERRUPTIBLE状态下睡眠。你应该不希望这种情况发生，因为这样一来，进程在等待信号量的时候就不再响应信号了。因此，使用down_interruptible()比使用down更为普遍。也许你会觉得这两个函数名字起的不恰当。

使用down_trylock函数，你可以尝试以堵塞方式来获取指定的信号量。在信号量已被占用时，它立刻返回非零值；否则，他返回0，而且让你成功持有信号量锁。

要释放指定的信号量，需要调用up函数，例如：

```c
//定义并声明一个信号量，名字为mr_sem，用于信号量计数
static DECLARE_MUTEX(mr_sem);
//试图获取信号量
if(down_interruptible(&mr_sem))
    //信号被接收，信号量还未获取
}//判定为假表示跳出
//临界区
//最后释放给定的信号量
up(&mr_sem);
```

### 10.5读写信号量

与自旋锁一样，信号量也有区分读写访问的可能。与读写自旋锁好普通自旋锁之间的关系差不多，读写信号量也要比普通信号量更具优势。

读写信号量在啮合中是由rw_semaphore结构表示的，定义在文件<linux/rwsem.h>中。通过以下语句可以创建静态声明的读写信号量：

```c
static DECLARE_RWSEM(name);
```

其中name是新信号量名。

动态创建的读写信号量可以通过以下函数初始化：

```c
init_rwsem(struct rw_semaphore *sem)
```

所有的读写信号量都是互斥信号量，也就是说他们的引用计数等于1，虽然只对写者互斥，不对读者。只要没有写者，并发持有读锁的读者数不限。相反，只有唯一的写者（在没有读者时）可以获得写锁。所有读写锁的睡眠不会被信号打断，所以他只有一个版本的down操作。

```c
static DECLARE_RWSEM(mr_rwsem);
//试图获得信号量用于读
down_read(&mr_rwsem);
//临界区（只读）
//释放信号量 
up_read(&mr_rwsem);
//试图获得信号量用于写
down_write(&mr_rwsem);
//临界区
//释放信号量
up_write(&mr_sem);
```

与标准信号量一样，读写信号量也提供了down_read_trylock()和down_write_trylock()方法。这两个方法都需要一个指向读写信号量的指针作为参数。如果成功获得了信号量锁，他们返回非0值；如果信号量锁被争用，则返回0。这和普通信号量的情形完全相反。到最后也没有给我讲解信号量实现具体过程，以及读写内部实现，都是停留在API层。

读写信号量相比读写自旋锁多一种特有的操作：downgrade_write()，这个函数可以动态地将获取的写锁转换为读锁。

读写信号量和读写自旋锁一样，除非代码中的读和写可以明白无误地分割开来，否则最好不使用它。在强调一次，读写机制使用是有条件的，只有在你的代码可以自然地界定出读写时才有价值。

### 10.6互斥体

直到最近，内核中唯一允许睡眠的锁是信号量。多数用户使用信号量只使用计数1，说白了是把其作为一个互斥的排它锁使用，好比允许睡眠的自旋锁。不幸的是，信号量用途更通用，没多少使用限制。这点使得信号量适合用于那些较复杂的、未明情况下的互斥访问，比如内核于用户空间复杂的交互行为。但这也意味着简单的锁定而使用信号量并不方便，并且信号量也缺乏强制的规则来行使任何形式的自动调试，即便受限的调试也不可能。

为了找到一个更简单睡眠锁，内核开发者们引入了互斥提mutex。确实，这个名字容易好我们的习惯称呼混淆。所以这里我们澄清一下，互斥体mutex这个称谓所指的是任何可以睡眠的强制互斥锁，比如使用计数是1的信号量，但在最新的Linux内核中，互斥体mutex这个称谓现在也用于一种实现互斥的特定睡眠锁。也就是说，互斥提是一种互斥信号。

mutex在内核中对应数据结构mutex，其行为和使用计数为1的信号量类似，但操作接口更简单，实现也更高效，而且使用限制更强。静态地定义mutex，你需要做：

```c
DEFINE_MUTEX(name);
//动态初始化mutex，需要做：
mutex_init(&mutex);
//对互斥锁锁定好解锁并不难
mutex_lock(&mutex);
//临界区
mutex_unlock(&mutex);
```

可以将互斥体理解为简化版的信号量，因为不再需要管理任何使用计数，并且还可以让线程睡眠。

mutex的简洁性和高效性源自于相比使用信号量更多的受限性。他不同于信号量，因为mutex仅仅实现了设计初衷的最基本的行为。因此mutex的使用场景 相对而言更严格、更定向了。

- 任何时刻中只有一个任务可以持有mutex，也就是说，mutex的使用计数永远是1
- 给mutex上锁者必须负责给其再解锁，不能在一个上下文中上锁，另一个上下文中解锁，其实就是要在同一个线程中实现解锁和加锁。这个限制使得mutex不适合内核同用户空间之间复杂的同步场景。
- 递归的上锁和解锁是不允许的。
- 当持有一个mutex的时候，线程不可以休眠。为什么呢，不理解
- mutex不能在中断或者下半部中使用，即使使用mutex_trylock()也不行。
- mutex只能通过官方API管理：只能使用上节中描述的方法初始化，不可被拷贝、手动初始化或者重复初始化。

也许mutex结构最有用的特色是：通过一个特殊的调试模式，内核可以采用编程方式检查好警告任何见他其约束法则的不老实行为。当打开内核配置选项CONFIG_DEBUG_MUTEXES后，就会有多种检测来确保这些约束得以遵守。这些调试手段无疑能帮助你和其他mutex使用者们都能以规范式的、简单化的使用模式对其使用。

所谓的调试手段其实就是配置内核选项啊，这么说的话信号量确实没有调试手段

#### 10.6.1信号量和互斥体

互斥体和信号量很相似，内核中两者共存会令人混淆。所幸，他们的标准使用方式都有简单的规范：除非mutex的某个约束妨碍你使用，否则相比信号量要优先使用mutex。当你写新代码时，只有碰到特殊场合这里的特殊场合一般指的是很底层的代码才会需要使用信号量。因此建议首选mutex。如果发现不能满足其约束条件，且没有其他别的选择时，再考虑选择信号量。

#### 10.6.2自旋锁和互斥体

了解何时使用自旋锁，何时使用互斥体或者信号量对编写优良代码很重要，但是多数情况下，并不需要太多的考虑，因为在中断上下文中只能使用自旋锁，而在任务睡眠时只能使用互斥体。

| 需求             | 建议的加锁方法 |
| ---------------- | -------------- |
| 低开销加锁       | 优先使用自旋锁 |
| 短期锁定         | 优先使用自旋锁 |
| 长期加锁         | 优先使用互斥体 |
| 中断上下文中加锁 | 使用自旋锁     |
| 持有锁需要睡眠   | 使用互斥体     |

### 10.7完成变量

如果在内核中一个任务需要发出信号通知另一任务发生了某个特定事件，利用完成变量是使两个任务得以同步的简单方法。如果一个任务要执行一些工作时，另一个任务就会在完成变量上等待。当这个任务完成工作后，会使用完成变量去唤醒在等待的任务。这听起来很像一个信号量，思想是一样的。事实上，完成变量仅仅提供了代替信号量的一个简单的解决方法。例如，当子进程执行或者退出时，vfork系统调用使用完成变量唤醒父进程。然后父进程才会知道原来子进程凉了再去给其收尸。

完成变量由结构completion表示，定义在<linux/completion.h>中，这个完成变量之前学过啊，就是complete啊，起一个完成变量我都没认出来。

完成变量由结构completion表示，定义在<linux/completion.h>中。通过以下宏静态地创建完成变量并初始化它：

```c
DECLARE_COMPLETION(mr_comp);
```

通过init_completion()动态创建并初始化完成变量。

在一个指定的完成变量上，需要等待的任务调用wait_for_completion()来等待特定事件。当特定事件发生后，产生事件的任务调用complete()来发送信号唤醒正在等待的任务。

| 方法                                     | 描述                           |
| ---------------------------------------- | ------------------------------ |
| init_completion(struct completion *)     | 初始化指定的动态创建的完成变量 |
| wait_for_completion(struct completion *) | 等待指定的完成变量接收信号     |
| complete(struct completion *)            | 发信号唤醒任何等待任务         |

使用完成变量的例子可以参考文件，kernel/sched.c 和kernel/fork.c。完成变量的通常用法是将完成变量作为数据结构中的一项动态创建，而完成数据结构初始化工作的内核代码将调用wait_for_completion()进行等待，初始化完成后，初始化函数调用completion()唤醒 在等待的内核任务。

### 10.8BLK:大内核锁

欢迎来到内核的原始混沌时期。BLK(大内核锁)是一个全局自旋锁，使用它主要是为了方便实现从Linux最初的SMP过度到细粒度加锁机制。我们下面来介绍BLK的一些有趣的特性：

- 持有BLK的任务仍然可以睡眠。因为当任务无法被调度的时候，所加锁会自动被丢弃：当任务被调度时，锁有会被重新获得。当然，这并不是说，当任务持有BLK时，睡眠是安全的，仅仅是可以这样做，因为睡眠不会造成任务死锁。，很特别睡眠自动丢锁
- BKL是一种递归锁，一个进程可以多次请求一个锁，并不会像自旋锁那样产生死锁现象。
- BKL只可以用在进程上下文中，和自旋锁不同，你不能在中断上下文中申请BLK。
- 新的用户不允许使用BLK，随着内核版本的不断前进，越来越少的驱动和子系统再依赖于BLK了

这些特性有助于2.0版本的内核向2.2版本过渡。在SMP支持被引入到2.0版本时，内核中一个时刻上只能有一个任务运行。2.2版本的目标是允许多处理器在内核中并发执行程序。引入BKL是为了使细粒度加锁机制的过渡更容易些，虽然当时BKL对内核过渡很有帮助，但目前他已经称为内核可扩展性的障碍了。

在内核中不鼓励使用BKL，事实上，新代码中不再使用BKL，但是这种锁仍然在部分内核代码中得到沿用，所以我们仍然需要理解BKL以及他的接口。除了前面提到的以外，BKL的使用方式和自旋锁类似。函数lock_kernel()请求锁，unlock_kernel()释放锁。一个执行线程可以递归的请求锁，但是，释放锁时也必须调用同样次数的unlock_kernel()操作，在最后一个解锁操作完成后，锁才会被释放。函数kernel_locked()检测锁当前是否被持有，如果被持有，返回一个非0值，否则返回0.这些接口被声明在文件<linux/smp_lock.h>中，简单的用法如下：

```c
lock_kernel();
//临界区，可以安全的睡眠，锁会被释放，当任务被重新调度的时候锁会被获取，这意味着不会处于死锁状态，但是你需要锁保护这里的数据，你还是不需要睡眠
unlock_kernel();
```

BKL在被持有时同样会禁止内核抢占。在单一处理器内核中，BKL并不执行实际的加锁操作。

| 函数            | 描述                             |
| --------------- | -------------------------------- |
| lock_kernel()   | 获得BKL                          |
| unlock_kernel() | 释放BKL                          |
| kernel_locked() | 如果锁被持有返回非0值，否则返回0 |

对于BKL最主要的问题是确定BKL锁保护的到底是什么多数情况下，BKL更像是保护大妈，而不是保护数据，比如保护结构foo。这个问题给利用自旋锁取代BKL造成了很大困难，因为难以判断BKL到底锁的是什么，更难的是，发现所有使用BKL的用户之间关系。

### 10.9顺序锁

顺序锁，通常简称seq锁，是在2.6版本内核中才引入的一种新型锁。这几种锁提供了一种很简单的机制，用于读写共享数据。实现这种锁主要依靠一个序列计数器。当有疑义的数据被写入时，会得到一个锁，并且序列值会增加。在读取数据之前和之后，序列号都被读取。

如果读取的序列号值相同，说明在读操作进行的过程中没有被写操作打断过。

如果读取的值是偶数，那么就表明写操作没有发生（要明白因为锁的初值是0，所以写锁会使值成基数，释放的时候变成偶数）。

```c
//定义一个seq锁
seqlock_t mr_seq_lock = DEFINE_SEQLOCK(mr_seq_lock);
//写锁的方法如下：
write_seqlock(&mr_seq_lock);
write_sequnlock(&mr_seq_lock);

//上面看起来在写的时候和写自旋锁类似，但是读锁就不一样了
unsigned long seq;
do{
    seq = read_seqbegin(&mr_seq_lock);//这里的seq就是序列号
    //读这里的数据
}while(read_seqretry(&mr_seq_lock,seq));//我理解是这里将序列号进行对比，读取前后是否被写操作打断，如果并没有写操作加入的话就停止循环
```

在多个读者和少数写者共享一把锁的时候，seq锁有助于提供一种非常轻量级和具有可扩展性的外观。但是seq锁对写者更有利。只要没有其他写者，写锁总是能被成功的获取。读者不会影响写锁，这点和读-写自旋锁以及信号量一样。另外，挂起的写者会不断的使得读操作循环，直到不再有任何写者持有锁为止。

seq锁在你遇到如下需求时将是最理想的选择：

- 数据存在很多读者
- 数据写者很少
- 虽然写者很少，但希望写优先于读，而且不允许读者让写者饥饿，读写自旋锁是读优先于写。
- 数据很简单，某些场合是不可以使用 原子类型的。

使用seq锁中最有说服力的是jiffies，（这是系统的节拍数），该变量存储了Linux机器启动到当前的时间，jiffies是一个使用64位的变量，记录了自系统启动依赖的时钟节拍累加数。对于那些能自动读取全部64位jiffies_64位变量的机器来说，需要用get_jiffies_64方法完成，而该方法的实现就是用了seq锁：

```c
u64 get_jiffies_64(void)
{
    unsigned long seq;
    u64 ret;
    do{
        seq = read_seqbegin(&xtime_lock);
        ret = jiffies_64;
    }while(read_seqretry(&xtime_lock,seq));
    return ret;//直到被写操作打断才退出循环
}
//定时器中断会更新jiffies的值，此刻，也需要使用seq锁变量
write_seqlock(&xtime_lock);
jiffies_64 += 1;
write_sequnlock(&xtime_lock);
```

若想进一步了解jiffies和内核时间管理，请看内核源码树中的kernel/timer.c和kernel/time/tick-common.c文件。

这里讲的这个顺序锁太简单了，就是提了下限制和代码，连API都不讲。

### 10.10禁止抢占

由于内核是抢占性的，内核中的进程在任何时刻都可能停下来以便另一个具有更高优先权的进程运行。（由于调度器的缘故，会自动让优先级低的进程给优先级高的进程让位置，这就是内核抢占）。这意味着一个任务 与被抢占的任务可能会在同一个临界区内运行。为了避免这种情况，内核抢占代码使用自旋锁作为非抢占区域的标记，如果一个自旋锁被持有，内核便不能进行抢占了（这是因为安全抢占问题preempt-safe）。因为内核抢占好SMP面对的是相同的并发问题，并且内核已经是SMP安全的（SMP-safe），所以，这种简单的变化使得内核也是抢占安全的（preempt-safe）

或许这就是我们希望的，实际中，某些情况并不需要自旋锁，但是仍然需要关闭内核抢占。最频繁出现的情况就是每个处理器上的数据，如果数据对每个处理器是唯一的，那么这个数据不需要使用锁保护，因为数据只能被一个处理器访问。如果自旋锁没有被持有，内核又是抢占式的，那么一个新调度的任务就可能访问同一个变量。

这样即使是一个单处理器计算机，变量foo也会被多个进程以伪并发的方式访问。通常，这个变量会请求得到一个自旋锁。

为了解决这个问题，可以通过preempt_diisable()禁止内核抢占这是一个可以嵌套调用的函数，可以调用任意次。每次调用都必须有一个相应的preempt_enable()调用。当最后一次preempt_enable()被调用后，内核抢占才重新启用。

```c
preempt_disable();//这个函数其实就是对进程描述符结构体中的preempt成员进行操作的，所以内核抢占就是调度器根据优先级实时的对进程进行切换
//抢占被禁止
preempt_enable();
```

抢占计数存在着被持有锁的数量和preempt_disable()的调用次数，如果计数是0，那么内核可以进行抢占（这里就说明了，这个数是由持有锁数量和API决定的）；如果为1或更大的值，那么，内核就不会进行抢占。这个计数非常有用，他是一种对原子操作和睡眠很有效的调试方法。

为了用更简洁的方法解决每个处理器上的数据访问问题，可以通过get_cpu()获得处理器编号（假定是用这种编号对每个处理器的数据进行索引的）。这个函数在返回当前处理器号前首先会关闭内核抢占。

```c
int cpu;
//禁止内核抢占，并将CPU设置为当前处理器
cpu = get_cpu();
//对每个处理器的数据进行操作
//在给予内核抢占性
put_cpu();
```

### 10.11顺序和屏障

当处理多处理器之间或硬件设备之间的同步问题时，有时需要在你的程序代码中以指定的顺序发出读内存和写内存的指令。在和硬件交互时，时常需要确保一个给定的读操作发生在其他读或写之前。另外，在多处理器上，可能需要按写数据的顺序读数据。

但是编译器和处理器为了提高效率，可能对读和写重新排序，这样无疑使问题复杂化。型号，所有可能重新排序和写的处理器提供了机器指令来确保顺序要求，同样也可以指示编译器不要 对给定点周围的指令序列进行重新排序。这些确保顺序的指令称作屏障（barriers）。

基本上，在某些处理器上存在以下代码：

```c
a=1;
b=2;
```

有可能会在a中存放新值之前就在b中存放新值。

编译器和处理器看不出a和b之间的关系。编译器会在编译时按这种顺序编译，这种顺序会是静态的，编译的目标代码就只把a放在b前面。但是，处理器会重新动态排序，因为处理器在执行指令期间，会在取指令和分派时，把表面上看似无关的指令按自认为最好的顺序排列。大多数情况下，这样的排序是最佳的，因为a和b之间没有明显的关系。

尽管前面的例子可能被重新排序，但是处理器和编译器绝不会对下面的代码重新排序：

```c
a = 1;
b = a;
```

此处a和b均为全局变量，因为a和b之间有明确的数据依赖关系。

偶然情况下，有必要让写操作被其他代码识别，也让所期待的指定顺序之外的代码识别，这种情况常常发生在硬件设备上，但是在多处理器机器上很常见。

rmb()方法提供了一个读内存屏障，他确保跨越rmb的载入动作不会发生重排序。也就是说在rmb之前的载入操作不会被重新排在该调用之后，同理，在rmb之后的载入操作不会被重新排在该调用之前。

wmb的方法与rmb类似，不过是针对存储而非载入。

mb方法既提供了读屏障也提供了写屏障，载入和存储动作都不会跨越屏障重新排序。

read_barrier_depends()是rmb的变种，他提供了一个读屏障，但是仅仅是针对后续读操作所依靠的那些载入。因为屏障后的读操作依赖于屏障前的读操作，所以，该屏障确保屏障前的读操作在屏障后的读操作之前完成。基本上说，该函数设置一个读屏障，比如rmb操作，但是只针对特定的读 ，也就是那些相互依赖的读操作。在有些体系结构上，read_barrier_depends()比rmb()执行的快，因为它仅仅是个空操作，实际上并不需要。

虽然Intel x86处理器不会对写进行重新排序，也就是说，他不进行打乱顺序的存储，但是其他处理器会这么做。

看看使用了mb()和rmb()的一个例子，其中a的初始值是1，b的初始值是2。

<img src="C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1671102698228.png" alt="1671102698228" style="zoom: 50%;" />

如果不使用内存屏障，在某些处理器上，c可能接收了b的新值，而d接收了a原来的值。比如c可能等于4，但是d却等于1，使用mb()能确保a和b按照预定的顺序写入，而rmb能确保c和d按照预定的顺序读取。这里可以看出给a进行赋值指的就是载入了，而将ab中的值赋值给cd就是读取。

这种重排序的发生是因为现代处理器为了优化其传送管道pipeline（之前从来没提到过这个所谓的传送管道，进程间通信有管道通信但是没学过），打乱了分派和提交指令的顺序。如果上例中读入a/b时的顺序被打乱的话，又会发生什么情况，rmb()或wmb()函数相当于指令，他们告诉处理器在继续执行前提交所有尚未处理的载入或存储指令。

看一个类似的例子，但是其中一个线程用read_barrier_depends()代替了rmb，正好这个函数还不太懂。

<img src="C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1671103579398.png" alt="1671103579398" style="zoom:50%;" />

这里p等于a的地址，pp等于p，b等于pp的内容也就是a。虽然使用rmb同样有效，但是因为读是数据相关的，所以我们使用read_barrier_depends()可能更快。注意，不管在哪种情况下，左边的线程都需要mb()操作来确保预定的载入和存储顺序。（说了等于没锁）

宏smp_rmb()、smp_wmb()、smp_mb()、smp_read_barrier_depends()提供了一个有用的优化。在SMP内核中他们被定义成常用的内存屏障，而在单处理器内核中，他们被定义成编译器的屏障。对于SMP系统，在有顺序限定要求时，可以使用SMP的变种。

barrier()方法可以防止编译器跨屏障对载入或存储操作进行优化。**编译器不会重新组织存储或载入操作**，而防止改变C代码的效果和现有数据的依赖关系。但是，他不知道在当前上下文之外会发生什么事。。例如编译器不可能知道有中断发生，这个中断有可能在读取正在被写入的数据，这时就要求存储操作发生在读取操作前。前面讨论的内存屏障可以完成编译器屏障的功能，但是编译器屏障要比内存屏障轻量的多。实际上编译器屏障几乎是空闲的，因为他只**防止编译器可能重排指令**。

大体分为三类，分别是单处理器、SMP、编译器三种屏障。

注意，对于不同体系结构，屏障的实际效果差别很大。例如，如果一个体系结构不执行打乱存储，那么wmb什么都不做。

### 10.12小结

本章应用了第九章的概念和原理，方便理解Linux内核用于同步和并发的具体方法，一开始先描述了最简单的确保同步的方法，原子操作，然后考察了自旋锁，这是内核中最普通的锁，他提供了轻量级单独持有者的锁，即争用时忙。我们接着还讨论了信号量这是一种睡眠锁，但是苦于太复杂了，应用场景多是关系复杂的场景，没有相应的内核配置项来跟踪调试，所以提出了更通用的衍生锁-mutex，至于专用的加锁原语像完成变量completion、seq锁，只是少少提及，取消了BLK大内核锁，考察了禁止抢占的原因，理解了屏障。

以第九章好第十章的同步方法为基础，就可以编写避免竞争条件、确保正确同步，而且能在多处理器上安全运行的内核代码。

## 第十一章定时器和时间管理

时间管理在内核中占有非常重要的地位。相对于事件驱动而言，内核中有大量的函数都是基于时间驱动的。其中 有些函数是周期执行的，像对调度程序中的运行队列进行平衡调整或对屏幕进行刷新这样的函数，都需要定期执行，比如说，每秒执行100次；而另外一些函数，比如需要推后执行的磁盘IO操作等，则需要等待一个相对时间后才运行，比如说，内核会在500ms后再执行某个任务。除了上述两种函数需要内核提供时间外，内核还必须管理系统的运行时间以及当前日期和时间。

请注意相对时间和绝对时间之间的差别。如果某个时间在5s后被调度执行。那么系统所要的不是绝对时间，而是相对时间；相反，如果要求管理当前日期和当前时间，则内核不但要计算流逝的时间而且还要计算绝对时间。所以这两种时间概念对内核时间管理来说都至关重要。

另外，还请注意周期性产生的时间与内核调度程序推迟到某个确定点执行的事件之间的差别。周期性产生的事件，比如每10ms一次都是由系统定时器驱动的。系统定时器是一种可编程硬件芯片，他能以固定频率产生中断。该中断就是所谓的定时器中断，它所对应的中断处理程序负责更新系统时间，也负责执行需要周期性运行的任务。系统定时器和时钟中断处理程序是Linux系统内核管理机制的核心，本章将着重讨论他们（凡是同步信号，都是由系统时钟发出的）。

本章关注的另外一个焦点是动态定时器，一种用来推迟执行程序的工具。比如说，如果软驱马达在一定时间内都未活动，那么软盘驱动程序会使用动态定时器关闭软驱马达。内核可以动态创建或撤销动态定时器。本章将介绍动态定时器在内核中的实现，同时给出在内核代码中可供使用的定时器接口。

### 11.1内核中的时间概念

时间概念对计算机来说有些模糊，事实上内核必须在硬件的帮助下才能计算和管理时间。硬件为内核提供了一个系统定时器用以计算流逝的时间，该时钟在内核中可看成是一个电子时间资源，比如数字时钟或处理器频率等。系统定时器以某种频率自行触发（经常被称为击中或者射中）时钟中断。该频率可以通过编程预定，称为节拍率（tick rate）。当时钟中断发生时，内核就通过一种特殊的中断处理程序对其进行处理。

因为预编的节拍率对内核来说是可知的，所以内核知道连续两次时钟中断的间隔时间。

时间驱动事件也属于事件驱动的一张，时间的流逝本身就是一种事件。然而由于时间驱动你给的频率非常高，且对内核至关重要，因此，本章中仅仅分析时间驱动事件。

一个间隔时间称为节拍tick，他等于节拍率分之一秒，内核就是靠这种已知的时钟中断间隔来计算墙上时间和系统运行时间的。墙上时间也就是实际时间对用户空间的应用程序来说是最重要的。内核通过控制时钟中断维护实际时间，另外内核也为用户空间提供了一组系统调用以获取实际日期和实际时间。系统运行时间也就是系统启动 开始所经历的时间对用户空间的内核都很有用，因为许多程序都必须清楚流逝的时间。通过两次读取运行时间在计算差，就可以得到相对的流逝时间了。

- 时钟中断对于管理操作系统尤为重要 ，大量内核函数的生命周期都离不开流逝的时间的控制。下面给出一些利用时间中断周期执行的工作：
- 更新系统运行时间
- 更新实际时间
- 在SMP系统上，均衡调度程序中的各处理器上的运行队列。如果运行队列负载不均衡的话，尽量使他们均衡。
- 检查当前进程是否用尽了自己的时间片。如果用尽，就重新进行调度。
- 运行超时的动态定时器。
- 更新资源消耗和处理器时间的统计值

这其中这其中有些工作在每次的时钟中断处理程序中 都要被处理，也就是说这些工作随时钟的频率反复运行。另一些也是周期性地执行，但需要每n次时钟中断运行一次，也就是说，这些函数在累计了一定数量的时钟节拍数之后才会被执行。在定时器中断处理程序中，将详细讨论时钟中断处理程序。

### 11.2节拍率：HZ

系统定时器频率（节拍率）是通过静态预处理定义的，也就是HZ，在系统 启动时按照HZ值对硬件进行设置。体系结构不同，HZ的值也不同，实际上，对于某些体系结构来说，甚至是机器不同，他的值不一样。

内核在<asm/param.h>文件中定义了这个值。节拍率有一个HZ频率，一个周期为1/HZ秒。例如，x86体系结构中，系统定时器频率默认值为100。因此，x86上时钟中断的频率就为100HZ，也就是说在在i386处理上的每秒时钟中断100次。但是其他的体系结构的节拍率为250和1000，分别对应4ms和1ms。

编写内核代码时，不要认为HZ值是一个固定不变的，这不是一个常见的错误，因为大多数体系结构的节拍率都是可调的，但是再过去，只有Alpha一种机型的节拍率不等于100，所以很多本该使用HZ的地方，都错误的在代码中直接硬编码成100这个值了，硬编码就是直接用const类型了吧。稍后，会给出内核代码中使用HZ的例子。

正如我们所看到的的，时钟中断能处理许多内核任务，所以他对内核来说极为重要。事实上，内核中的全部时间概念都源于周期运行的系统时钟。所以选择一个合适的频率，必需取得各方面的折中。

众多体系结构中，只有Alpha和ia64是1024HZ的，其他都是100HZ

#### 11.2.1理想的HZ值

自Linux问世以来，i386体系结构中时钟中断频率就设定为100HZ，但是在2.5开发版内核中，中断频率被提高到1000HZ，当然，是否应该提高频率是饱受争议的，由于内核中众多子系统都必须依赖时钟中断工作，所以改变中断频率必然会对整个系统造成很大的冲击。但是，任何事情总是有两面性的，我们接下来就来分析系统定时器使用高频率与使用低频率各有那些优劣。

提高节拍率意味着时钟中断产生的更频繁，所以中断处理程序也会更频繁地执行。如此以来会给整个系统带来如下好处：

- 更高的时钟中断解析度可提高时间驱动事件的解析度
- 提高了时间驱动事件的准确性

提高节拍率等同于提高中断解析度。比如HZ=100的时钟的执行粒度为10ms，即系统中的周期事件最快为每10ms运行一次，而不可能有更高的精度，但当HZ=1000时，解析度就为1ms，精细了10倍，虽然内核可以提供频率为1ms的时钟，但是并没有证据显示对系统中的所有程序而言，频率高就合适。

另外，提高解析度的同时也提高了准确度，假设内核在某个随机时刻触发定时器，而他可能在任何时间超时，但由于只有在时钟中断到来时才可能执行它，所以平均误差为半个时钟中断周期。

#### 11.2.2高HZ的优势

更高的时钟中断频度和更高的准确度又会带来如下优点：

- 内核定时器能够以更高的频度和更高的准确性
- 依赖定时器执行的系统调用，比如poll好select能够以更高的精度运行
- 对诸如资源消耗和系统运行时间等的测量会有更精细的解析度。
- 提高进程抢占的准确度

对poll和select超时精度的提高会给系统性能代码极大的好处。提高精度可以大幅度提高系统性能。频繁使用上述两种系统调用的应用程序，往往在等待时钟中断上浪费大量的时间，而事实上，定时值可能早就超时了。

更高的准确率也使进程抢占更准确，同时还会加快调度响应时间。第四章中提到过，时钟中断处理程序负责减少当前进程的时间片计数。当时间片计数跌到0时，而又设置了need_resched标志的话，内核会立刻重新运行调度程序。假定有一个正在运行的进程，他的时间片只剩下2ms了，此时调度程序又要求抢占该进程，然后去运行另一个新进程；然而，该抢占行为不会在下一个时钟中断到来前发生，也就是说，在这2ms内不可能进行抢占。实际上，对于频率为100HZ的时钟来说，最坏要在10ms后，问题在于由于耽误了抢占，所以类似于填充音频缓冲区这样有严格时间要求的任务来说，结果是无法接收的。如果将节拍率提高到1000HZ，在最坏情况下延误时间是1ms，平均情况只有0.5ms左右。

#### 11.2.3高HZ的劣势

现在该谈谈另一面了，提高节拍率会产生副作用，节拍率越高，系统负担越重 ，中断处理程序占用的处理器的时间越多。这样不但减少了处理器其他工作的时间，而且还会更频繁地打乱处理器告诉缓存并增加耗电。负载造成的影响值得进一步探讨。将时钟频率提高10倍，会使时钟中断的负载增加10倍。最后的结论是：至少在现代计算机系统上，时钟频率为1000HZ不会导致难以接受的负担。

##### 无节拍的OS?

Linux内核支持无节拍操作这样的选项，当编译内核时设置了CONFIG_HZ配置选项，系统就根据这个选项动态 调度时钟中断。并不是每隔固定的时间间隔触发时钟中断，而是按需动态调度和重新设置。如果下一个时钟频率设置为3ms，就每3ms触发一次时钟中断，之后，如果50ms内都无事可做，内核以50ms重新调度时钟中断。

减少开销实质性收益是省电，特别是在系统空闲时。对于无节拍的系统而言，空闲档期不会被不必要的时钟中断所打断，于是减少了系统的能耗。且不论空闲期是200ms还是200秒，随着时间的推移，省电是实实在在的。

### 11.3jiffies

全局变量jiffies用来记录自系统启动依赖产生的节拍的总数。启动时，内核将该变量初始化为0，此后，每次时钟中断处理程序就会增加该变量的值。因为一秒内时钟中断的次数等于HZ，所以jiffies一秒内增加的值也就是HZ。系统运行时间以秒为单位计算，就等于jiffies/HZ。实际出现的情况可能复杂些：内核给jiffies赋值一个特殊的初值，引起这个变量不断的溢出，由此捕捉到bug。当找到实际的jiffies值后，就首先把这个偏差减去。

##### Jiffy的语源

jiffies定义于文件<linux/jiffies.h>中：

```c
extern unsigned long volatile jiffies;
```

在13.4节将会看到他的实际定义，看起来有点特殊，现在先看一些用到jiffies的内核代码。下面表达式将以秒为单位的时间转化为jiffies：

```c
(seconds *HZ)
```

相反，下面表达式将jiffies转换为以秒为单位的时间：

```c
(jiffies/HZ)
```

比较而言，内核中将秒转换为jiffies用的多一些，比如代码经常需要设置一些将来的时间：

把时钟转化为秒经常会用在内核和用户空间进行交互的时候，而内核本身很少用到绝对时间。注意jiffies类型为无符号长整形(unsigned long)，用其他任何类型存放它都不正确。

#### 11.3.1jiffies的内部表示

jiffies变量总是无条件长整形，因此在32位体系结构上是32位的，在64位体系结构上是64位的。32位的jiffies变量，在时钟频率为100HZ的情况下，497天后就会溢出。如果频率为1000HZ，49.7天后就会溢出。而如果使用64位的jiffies变量，看不到溢出了。

考虑到与现有内核代码的兼容性，内核开发者希望jiffies依然为unsigned long，有一些巧妙的思想和神奇的链接程序扭转了这一局面。

```c
//都在<linux/jiffies.h>
extern unsigned long volatile jiffies;
extern u64 jiffies_64;
```

ld(1)脚本用于连接主内核映像，然后用jiffies_64变量的初值覆盖jiffies变量：

```c
jiffies = jiffies_64;
```

因此，jiffies取整个64位变量的低32位。代码可以完全像以前一样继续访问jiffies。因为大多数代码只不过使用jiffies存放流逝的时间，因此，也就只关心32位。不过，时间管理代码使用整个64位，以此来避免整个64位的溢出。下图呈现了jiffies和jiffies_64的划分。

<img src="C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1671112687782.png" alt="1671112687782" style="zoom: 33%;" />

访问jiffies的代码仅会读取jiffies_64的低32位。通过get_jiffies_64()函数，就可以读取整个64位数值。但是这种需求很少，多数代码仍然只要能通过jiffies变量读取低32位就够了。

在64位体系结构上，jiffies_64和jiffies指的是同一个变量，代码既可以直接读取jiffies也可以调用get_jiffies_64()函数，他们的作用相同。

#### 11.3.2jiffies的回绕

变量溢出后值会回绕到0。

```c
unsigned long timeout = jiffies + HZ/2;//当前节拍数+0.5秒的节拍
//执行一些任务
//然后查看是否花的时间过长
if(timeout>jiffies){
    //没有超时
}else{
    //超时，发生错误
}
```

上面这一小段代码是希望设置一个准确的超时时间，本例中从现在开始计时，时间为半秒。然后再去处理一些工作，比如探测硬件然后等待他的响应。如果处理这些工作的时间超过了设定的超时时间，代码就要做相应的出错处理。

内核提供了四个宏来帮助比较节拍计数，他们能正确地处理节拍计数回绕情况。这些宏定义在文件<linux/jiffies.h>中。其中unkown参数通常是jiffies，known参数是需要对比的值。

因为32位体系结构不能原子地一次访问64位变量中的两个32位数值，所以在读取jiffies时，特殊的函数利用xtime_lock锁对jiffies变量进行锁定 。

所以前面的例子可以改造成时钟-回绕-安全的版本（timer-wraparoundsafe）形式如下：

```c
unsigned long timeout = jiffies + HZ/2;
if(time_before(jiffies,timeout)){
}else{
    //超时了，发生错误
}
```

#### 11.3.3用户空间和HZ

在2.6版以前的内核中，如果改变内核中HZ的值，会给用户空间中某些程序造成异常结果，这是因为内核是以节拍数/秒的形式给用户空间导出的这个值的，在这个接口稳定了很长一段时间后，应用程序变逐渐依赖于这个特定的HZ值了，所以如果在内核中更改了HZ的定义值，就打破了用户空间的常量关系，用户空间不知道这个新的HZ的值。

要想避免错误发生，内核必须更改所有导出的jiffies值。因而内核定义了USER_HZ来代表用户空间看到的HZ值。在x86体系结构上，由于HZ值 原来一直是100，，所以USER_HZ值就定义成100。内核可以使用函数jiffies_to_clock_t()定义在kernel/time.c中将一个由HZ表示的节拍计数转化为由USER_HZ表示的节拍计数。所采用的表达式取决于USER_HZ好HZ是否互为整数倍，而且USER_HZ是否小于等于HZ，如果这两个条件都满足，对大多数系统来说通常也能够满足，则表达式相当简单：

```c
return x /(HZ/USER_HZ)
```

如果不是整数倍关系，那么该宏就得用到更为复杂的算法了。

最后还要说明，内核使用函数jiffies_64_to_clock_t()将64位的jiffies值的单位从HZ转换位USER_HZ。后者是内核定义的HZ，前者是用户空间默认的HZ

在需要把以节拍数/秒为单位的值导出到用户空间时，需要使用上面这几个函数，比如：

```c
unsigned long start;
unsigned long total_time;
start = jiffies;
//执行一些任务
total_time = jiffies -start;
printk("That took %lu ticks\n",jiffies_to_clock_t(total_time));
```

用户空间期望HZ = USER_HZ，但是如果他们不相等，则由宏完成转换，这样的结果自然是皆大欢喜。

### 11.4硬时钟和定时器

体系结构提供了两种设备进行计时，一种是我们前面讨论过的系统定时器；另一种是实时时钟RTC，虽然在不同机器上这两种时钟的实现并不相同，但是他们有着相同的作用和设计思路。

#### 11.4.1实时时钟

实时时钟是用来持久存放系统时间的设备，即使系统关闭后，他也可以靠主板上的微型电池提供的电力保持系统的计时。在PC体系结构中，RTC和CMOS集成在一起，而且RTC的运行和BIOS的保存设置都是通过同一个电池供电的。

当系统启动后，内核通过读取RTC来初始化墙上时间，该时间存放在xtime变量中。虽然内核通常不会在系统启动后再读取xtime变量，但是有些体系结构会周期性地将当前时间值存回RTC中。尽管如此，实时时钟最主要的作用仍是在启动时初始化xtime变量。

#### 11.4.2系统定时器

系统定时器是内核定时机制中最为重要的角色。尽管不同体系结构中的定时器实现不尽相同，但是系统定时器的根本思想没有区别，提供一种周期性触发中断机制。有些体系结构是通过对电子晶振进行分频来实现系统定时器，还有些体系结构则提供了一个衰减测量器，衰减测量器设置一个初始值，该值以固定频率递减，当减到零的时候，触发一个中断。无论哪种情况，其效果都一样。

在x86体系结构中，主要采用可编程中断时钟PIT，在pc机器中很常见，而且从DOS时代，就开始以它作为时钟中断源了，内核在启动时对PIT进行编程初始化，使其能够以HZ/秒的频率产生时钟中断。虽然PIT设备很简单，功能也有限，但他却足以满足我们的需求。x86体系结构中的其他调度时钟资源还包括本地APIC时钟和时间戳计数TSC。

### 11.5时钟中断处理程序

现在已经理解了HZ/jiffies等概念以及系统定时器的功能。下面将分析时钟中断处理程序是如何实现的。时钟中断处理程序可以划分为两部分；体系结构相关部分和体系结构无关部分。

**与体系结构相关的例程作为系统定时器的中断处理程序而注册到内核中**，以便在产生时钟中断时，它能够相应地运行。虽然处理程序的具体 工作依赖于特定的体系结构，但是绝大多数处理程序最低限度也都要执行如下工作：

- 获得xtime_lock锁，以便对访问jiffies_64和墙上时间xtime进行保护
- 需要时应答或者重新设置系统时钟。
- 周期性地使用墙上时间更新实时时钟
- 调用体系结构无关的时钟例程：tick_periodic()

**中断服务程序主要通过调用与体系结构无关的例程**，tick_periodic()执行下面更多的工作：

- 给jiffies_64变量增加1这个操作即使是在32位体系结构上也是安全的，因为前面已经获得了xtime_lock锁。
- 更新资源消耗的统计值，比如当前进程所消耗的系统时间和用户时间。
- 执行已经到期的动态定时器，这是用来执行延后工作的
- 执行第四章曾讨论的sheduler_tick()函数。
- 更新墙上时间，该时间存放在xtime变量中。
- 计算平均负载值

因为上述工作分别都由单独的函数负责完成，所以tick_periodic()的代码很简单。

```c
static void tick_periodic(int cpu)
{
    if(tick_do_timer_cpu == cpu){
        write_seqlock(&xtime_lock);//这是对写顺序锁进行上锁
        tick_next_period = ktime_add(tick_next_period,tick_period);
        do_timer(1);//临界区中执行该函数
        write_sequnlock(&xtime_lock);
    }
    update_process_times(user_moe(get_irq_rege()));
    profile_tick(CPU_PROFILING);
}
```

很多重要的操作都在do_timer()和uddate_process_times()函数中进行。前者承担着对jiffies_64的实际增加操作：

```c
void do_timer(unsigned long ticks)
{
    jiffies_64 += ticks;
    update_wall_time();//根据流逝的时间更新墙上时钟
    calc_global_load();//更新系统的平均负载统计值
}
```

当do_timer()最终返回时，调用update_process_times()更新所耗费的各种节拍数。注意，通过user_tick区别是花费在用户空间还是内核空间

```c
void update_process_times(int user_tick)
{
struct task_struct *p = current;//获取当前进程的描述符
int cpu = smp_processor_id();
account_process_tick(p,user_tick);
run_local_timers();
rce_check_callbacks(cpu,user_tick);
printk_tick();
scheduler_tick();
run_posix_cpu_timers(p)l
}
```

回想一下tick_periodic()，user_tick的值是通过查看系统寄存器来设置的：

```c
update_process_times(user_mode(get_irq_regs()));
```

account_process_tick()函数对进程的时间进行实质性更新：

```c
void account_process_tick(struct task_struct *p,int user_tick)
{
cputime_t one_jiffy_scaled = cputime_to_scaled(cputime_one_jiffy);
struct rq *rq = this_rq();
if(user_tick)
	account_user_time(p,cputime_one_jiffy,one_jiffy_scaled);
	else if((p != rq->idle) || (irq_count() != HARDIRQ_OFFSET))
		account_system_time(p,HARDIRQ_OFFSET,cputime_one_jiffy,
				one_jiffy_scaled);
				else
				account_idle_time(cputime_one_jiffy);
}
```

这样做意味着内核对进程进行时间计数时，是根据中断发生时处理器所处的模式进行分类统计的，他把上一个节拍全部算给了进程。但是事实上进程在上一个节拍期间可能多次进入和退出内核模式，而且在上一个节拍期间，该进程也不一定是唯一一个运行进程。很不幸，这种粒度 进程统计方式是传统的Unix所具有的，现在还没有更加精密的统计算法支持，内核现在只能做到这种程度，这也是内核应该采用更高频率的另一个原因。

接下来的run_lock_timers()函数标记了一个软中断去处理所有到期的定时器。

最后，scheduler_tick()函数负责减少当前运行进程的时间片计数值并且在需要时设置need_resched标志。在SMP机器中，该函数还要负责平衡每个处理器上的运行队列。

tick_periodic()函数执行完毕后返回与体系结构相关的中断处理程序，继续执行后面的工作，释放xtime_lock锁，然后退出。

以上的全部工作每1/HZ秒都要发生一次，也就是说在x86机器上时钟中断处理程序每秒执行100次。

### 11.6实际时间

当前实际时间（墙上时间）定义在文件kernel/time/timekeeping.c中：

```c
struct timespec xtime;
```

timespec数据结构定义在文件<linux/time.h>中，形式如下：

```c
struct timespec{
_kernel_time_t tv_sec;
long tv_nsec;
};
```

xtime.tv_sec以秒为单位，存放着自1970年1月1日UTC依赖经过的时间。多数Unix系统的墙上时间都是基于该纪元而言的。这一天被称为纪元。xtime.v_nsec记录自上一秒开始经过的ns数。

读写xtime变量需要使用xtime_lock锁，该锁不是普通自旋锁而是一个seqlock锁，也就是顺序锁。之前确实讨论过，我也知道jiffies就是使用的顺序锁，但是确实没太明白。

更新xtime首先要申请一个seqlock锁：

```c
write_seqlock(&xtime_lock);
//更新xtime
write_sequnlock(&xtime_lock);
```

读取xtime时也要使用read_seqbegin()和read_seqretry()函数：

```c
do{
	unsigned long lost;
	seq = read_seqbegin(&xtime_lock);
	usec = timer->get_offset();
	lost = jiffies - wall_jiffies;
	if(lost)
    	usec += lost * (1000000/HZ);
 	sec = xtime.tv_sec;
    usec +=(xtime.tv_nsec/1000);
}while(read_seqretry(&xtime_lock,seq));//这里可以看到是获取此时的序列号与最初的序列号进行对比，如果没有写操作的话，序列号是不发生变化的
```

该循环不断循环，直到读者确认读取数据时没有写操作介入，如果发现循环期间有时钟中断处理程序更新xtime，那么read_seqretry()函数就返回无效序列号，继续循环等待

这个操作之前也出现过一次是在读取jiffies节拍数的时候。

从用户空间取得墙上时间的主要接口gettimeofday()，在内核中对应系统调用为sys_gettimeofday()，定义于kernel/time.c:

```c
asmlinkage long sys_gettimeofday(struct timeval *tv,struct timezone *tz)
{
	if(likely(tv)){
        struct timeval ktv;
        do_gettimeofday(&ktv);
        if(copy_to_user(tv,&ktv,sizeof(ktv)))
            return -EFAULT;
    }
    if(unlikely(tz)){
        if(copy_to_user(tz,&sys_tz,sizeof(sys_tz)))
            return -EFAULT;
    }
    return 0;
}
```

如果用户提供的tv参数非空，那么与体系结构相关的do_gettimeofday()函数将被调用。该函数执行的就是上面提到的循环读取xtime的操作。如果tz参数为空，该函数将把系统时区（存放在sys_tz中）返回用户。原来timezone表示的是时区啊，如果在给用户空间拷贝墙上时间或者时区时发生错误，该函数返回-EFAULT;如果成功，则返回0。

虽然内核也实现了time()系统调用，但是gettimeofday()几乎完全取代了它。另外C库函数也提供了一些墙上时间相关的库调用，比如ftime()和ctime()。

另外，系统调用settimeofday()来设置当前时间，他需要具有CAP_SYS_TIME权能。（从来没见过CAP_SYS_TIME这个东西）

除了更新xtime时间以外，内核不会像用户空间程序那样频繁使用xtime。但也有需要注意的特殊情况，那就是在文件系统的实现代码中存放访问时间戳（创建、存取、修改）需要使用xtime，IIO_buffer就是需要存放访问的时间戳吧，，

### 11.7定时器

定时器（有时也称为动态定时器或内核定时器，好家伙，原来动态定时器就是我们自己设置的定时器啊）是管理内核流逝的时间的基础。内核经常需要推后执行某些代码比如之前的章节，下半部机制就是为了将工作放到以后执行。但不幸的是，“之后”这个概念很模糊，下半部的本意并非是放到以后的某个时间去执行 任务，而仅仅是不再当前时间执行就可以了，为此需要一种工具，能够使工作在指定时间点上执行，不长不短，正好在希望的时间点上。内核定时器正是解决这个问题的理想工具。

定时器的使用很简单。你只需要执行一些初始化工作，设置一个超时时间，指定超时发生以后执行的函数，然后激活定时器就可以了。指定的函数将在定时器到期时自动执行。注意定时器并不周期运行，他在超时后就自行撤销了！就是这个东西，之前裸机使用定时器的时候并没有着重的提出这个特点，但是Linux下确实申请之后只能执行一次。这也正是这种定时器被称为动态定时器的一个愿您；动态定时器不断地创建和撤销，而且他的运行次数不受限制，定时器在内核中应用得非常普遍。

#### 11.7.1使用定时器

定时器由结构timer_list表示，定义在文件<linux/timer.h>中

```c
struct timer_list{
	struct list_head entry;//上来就是一个链表，这是包含链表的父结构
    unsigned long expires;//以jiffies为单位的定时值
    void (*function)(unsigned long);//定时器处理函数
    unsigned long data;//传给处理函数的长整形参数
    struct tvec_t_base_s *base;//定时器内部值，用户不要使用
}
```

幸运的是，使用定时器并不需要深入了解该数据结构。事实上，过深地陷入该结构，反而会使你的代码不能保证对可能发生的变化提供支持。内核提供了一组与定时器相关的接口用来简化管理定时器的操作。所有这些接口都声明在文件<linux/timer.h>中，大多数接口在文件kernel/timer.c中获得实现的。

实际上称为动态定时器另一个原因是因为2.3版本内核前也存在静态定时器。这种定时器在编译时创建，而不是实时创建。由于静态定时器存在缺陷，已经被淘汰了。

创建定时器时需要先定义它：

```c
struct timer_list my_timer;
```

接着需要通过一个辅助函数来初始化定时器内部数据结构的内部值，初始化必须在使用其他定时器管理函数对定时器进行操作前完成。

```c
init_timer(&my_timer);
```

然后就是自行填充结构体内部成员

```c
my_timer.expires = jiffies + delay;//设置定时器的超时的时候的节拍数量，注意这是绝对节拍数，是当前节拍数加上需要延时的节拍数
my_timer.data = 0;//给定时器处理函数传入0值
my_timer.function = my_function;//定时器超时的时候调用的函数
```

my_timer.expires表示超时时间，是以节拍数为单位的绝对计数值。如果当前jiffies计数大于或等于my_timer.expires，那么my_timer.function指向的处理函数就会开始执行，另外该函数还要使用长整形参数my_timer.data。所以正如我们从timer_list结构看到的形式，处理函数必须符合下面的函数原型：

```c
void my_timer_function(unsigned long data);
```

data参数作用就是可以让我们利用同一个处理函数注册多个定时器，只需要通过该函数就能区分对待他们。如果不需要这个参数就传入0或者任何值给处理函数即可。

最后，你必须激活定时器：

```c
add_timer(&my_timer);
```

大功告成，定时器可以工作了，但请注意定时值的重要性。当前节拍计数等于或大于指定的超时节拍时，内核就开始执行定时器处理函数。虽然内核可以保证不会在超时时间到期前运行定时器处理函数，但是有可能延误定时器的执行。一般来说，定时器都在超时后马上就会执行，但是也有可能推迟到下一次时钟节拍才能运行，毕竟是同步中断，可能到时了但是系统频率过低导致需要等到下一次时钟节拍才可以，所以不能用定时器来实现任何硬实时任务。

有时可能需要更改已经激活的的定时器超时时间，所以内核通过函数mod_timer()来实现该功能，该函数可以改变指定的定时器超时时间：

```c
mod_timer(&my_timer,jiffies + new_delay);//新的定时值，这个之前使用过，就是在驱动教程中的内核定时器的时候，因为定时器一次性的需要重新激活
```

mod_timer()函数也可操作那些已经初始化，但还没被激活的定时器，如果定时器未被激活，mod_timer()会激活它。如果调用时定时器未被激活，该函数返回0，否则返回1。但不论哪种情况，一旦 从mod_timer()函数返回，定时器都将被激活而且设置了新的定时值。

如果需要在定时器超时前停止定时器，可以使用del_timer()函数：

```c
del_timer(&my_timer);
```

被激活或未被激活的定时器都可以使用该函数，如果定时器还未被激活，该函数返回0；否则返回1.注意，不需要为已经超时的定时器调用该函数，因为他们会自动删除。

当激活定时器时，必须注意一个潜在的竞争条件。当del_timer()返回后，可以保证的只是：定时器不会再被激活（也就是，将来不会执行），但是在多处理器机器上定时器中断可能已经在其他处理器上运行了，所以删除定时器时需要等待可能在其他处理器上运行的定时器处理程序都退出，这时就要使用del_timer_sync()函数执行删除工作：

```c
del_timer_sync(&my_timer);//这就是等其他处理器上运行的中断结束后，再删除定时器
```

和del_timer()函数不同，del_timer_sync()函数不能在中断上下文中使用。

#### 11.7.2定时器竞争条件

因为定时器与当前执行代码是异步的，因此就有可能存在潜在的竞争条件。所以，首先，决不能直接操作定时器结构体来改变定时器的超时时间从而代替mod_timer()函数。这样的代码在多处理器上是不安全的。

其次，一般情况下应该使用del_timer_sync()函数取代了del_timer()函数，因为无法确定在删除定时器时，他是否正在其他处理器上运行。为了防止这种情况的发生，应该调用del_timer_sync()函数。

最后，因为内核异步执行中断处理程序，所以应该重点保护定时器中断处理程序中的共享数据。定时器数据的保护问题 曾在第8章和第9章讨论过。就是锁机制保护吧

#### 11.7.3实现定时器

内核在时钟中断发生后执行定时器，定时器作为软中断在下半部上下文中执行。具体来说，时钟中断处理程序会执行update_process_times()函数，该函数随即调用run_local_timers()函数：

```c
void run_local_timers(void)
{
	hrtimer_run_queues();
    raise_softirq(TIMER_SOFTIRQ);//执行定时器软中断
    softlockup_tick();
}
```

run_timer_softirq()函数处理软中断TIMER_SOFTIRQ，从而在当前处理器上运行所有的超时定时器。乱套了，这个run_timer_softirq函数是哪里冒出来的，怎么突然就提到了。

虽然所有定时器都以链表形式存放在一起，但是让内核经常为了寻找超时定时器而遍历整个链表是不明智的。同样，将链表以超时时间进行排序也是很不明智的做法，因为这样一来在链表中插入和删除定时器都会很费时。

为了提高搜索速率，内核将定时器按他们的超时时间划分为五组。当定时器超时时间接近时，定时器将随组一起下移。采用分组定时器的方法可以在执行软中断的多数情况下，确保内核尽可能减少搜索超时定时器所带来的的负担。因此定时器管理代码是非常高效的。

### 11.8延迟执行

内核代码尤其是驱动程序除了使用定时器或下半部机制以外，还需要 其他方法来推迟执行任务。这种推迟通常发生在等待硬件完成某些工作时，而且等待的时间往往非常短，比如，重新设置网卡的以太模式需要花费2ms，所以在设定网卡速度后，驱动程序必须至少等待2ms才能继续运行。

内核提供了许多延迟方法处理各种延迟要求。不同的方法有不同的处理特点，有些是在延迟任务时挂起处理器，防止处理器执行任何实际工作；另一些不会挂起处理器，所以也不能确保被延迟的代码能够在指定的延迟时间运行。

#### 11.8.1忙等待

最简单的延迟方法是忙等待或者说忙循环。但要注意该方法仅仅在想要延迟的时间是节拍的整数倍，或者精确率要求不高才可以使用。

忙循环实现起来很简单，在循环中不断旋转直到希望的时钟节拍数耗尽。

```c
unsigned long timeout = jiffies +10;//10个节拍
while(time_before(jiffies,timeout));//直到jiffies大于等于timeout的时候
```

循环不断执行，直到jiffies大于delay为止，总共的循环时间为10个节拍。在HZ值等于1000的x86体系结构上，耗时为10ms。就是把10换成n * HZ

程序要循环等待2 * HZ个时钟节拍，也就是说无论时钟节拍率如何，都要等待2s

对于系统的其他部分，忙循环方法算不上一个好办法。因为当代码等待时，系统是自旋。不会去处理其他任何任务！事实上，几乎不会用到这种低效率 的办法。这里介绍他仅仅因为它是最简单最直接的延迟方法。当然也可能在某些蹩脚的代码中发现他们的身影。

更好的方法应该是在代码等待时，允许内核重新调度执行其他任务：

```c
unsigned long delay = jiffies + 5 * HZ;
while(time_before(jiffies,delay))
cond_resched();
```

cond_resched()函数将调度一个新程序投入运行，但他只有在设置完need_resched标志后才能生效 。或者说只有当系统中存在更重要的任务时才能运行其他的任务。注意，因为该方法需要调用调度程序，所以他不能在中断上下文中使用，只能在进程上下文使用。事实上，所有延迟方法在进程上下文中使用的更好。

另外，延迟执行不管在哪种情况下，都不应该在持有锁时或禁止中断时发生。

事实上，没有方法能确保实际的延迟刚好等于指定的延迟时间，都是多于指定时间。

C语言的推崇者可能会问：什么能保证前面的循环已经执行了。C编译器通常只把变量装载一次。一般情况下不能保证循环中的jiffies变量在每次循环中被读取时都重新被载入。但是我们要求jiffies在每次循环时都必须重新装载，因为在后台jiffies值会随时钟中断的发生而不断增加。为了解决这个问题，<linux/jiffies.h>中jiffies变量被标记关键字volatile。（终于讲到这个了，之前看单片机的面试题，关键字有extern/static/vmline/volatile等等）。

**关键字volatile指示编译器在每次访问变量时都重新从主内存中 获取**，而不是通过寄存器中的变量别名来访问，从而确保前面的循环能按预期的方式执行。循环中的jiffies肯定是再别的地方extern过来的外部数据，不过这里的通过寄存器中的变量别名来访问是什么意思，本来数据就是变化的啊。

#### 11.8.2短延迟

有时内核代码（通常也是驱动程序）不但需要很短暂的延迟（比时钟节拍还短），而且还要求延迟的时间很精确。这种情况多发生在和硬件同步时，也就是说需要短暂等待某个动作的完成而这个等待时间要比系统频率还要小的（小于1ms），所以不可能使用像前面例子中那种基于jiffies的延迟方法。对于频率为100HZ的时钟中断，节拍间隔超过10ms。必须寻找其他方法满足更短、更精确的延迟要求。

幸运的是，内核提供了三个可以处理ms/ns和ms级别的延迟函数，他们定义在文件<linux/delay.h>和<asm/delay.h>中，可以看到他们并不使用jiffies：

```c
void udelay(unsigned long usecs);
void ndelay(unsigned long nsecs);
void mdelay(unsigned long mescs);
```

前一个函数利用忙循环将任务延迟指定的ms数后运行，后者延迟指定的ms数。这个太常用了。

udelay()函数依靠执行数次循环达到延迟效果，而mdelay()函数又是通过udelay()函数实现的。

##### 我的BogoMIPS比你的大

BogoMIPS值总是让人觉得糊涂，也让人觉得有意思。其实，计算BogoMIPS并不是为了表现你的机器性能，BogoMIPS值记录处理器在给定时间内忙循环执行的次数。其实，BogoMIPS记录处理器在空闲时速度有多快。该值存放在变量loops_per_jiffy中，可以从文件/proc/cpuinfo中读到它。延迟循环函数使用loops_per_jiffy值来计算为提供精确延迟而需要进行多少次循环。

内核在启动是利用calibrate_delay()计算loops_per_jiffy值，该函数在文件init/main.c中。udelay()函数应当只在小延迟中调用，因为在快速机器上的大延迟可能导致溢出。通常，超过1ms的范围不要使用udelay()进行延迟，对于较长的延迟，mdelay()工作良好。像其他忙等而延迟执行的方案，除非绝对必要，这两个函数都不应该被使用。

记住，持锁忙等或者禁止中断是一种粗鲁的做法，因为系统响应时间和性能都大受影响。不过，如果你需要精确的延迟，这些调用（就是上面三种延迟方法）是最好的办法。这些忙等函数主要用在延迟小的地方，通常在us范围内。

#### 11.8.3schedule_timeout()

更理想的延迟执行方法是使用 schedule_timeout()函数，该方法会让需要延迟执行的任务睡眠到指定的延迟时间耗尽后再重新运行。但该方法也不能保证睡眠时间正好等于。当指定的时间到期后，内核唤醒被延迟的任务并将其重新放回运行队列，用法如下：

```c
set_current_state(TASK_INTERRUPTIBLE);//将任务设置为可中断睡眠状态
schedule_timeout(s * HZ);//s秒之后唤醒
```

唯一的参数就是延迟的相对时间，单位是jiffies，上例中将相应的任务推入可中断睡眠队列，睡眠s秒。因为任务处于可中断状态，所以如果任务收到信号将被唤醒。如果睡眠任务不想接收信号，可以将任务状态设置为TASK_UNINTERRUPTIBLE，然后睡眠。注意，在调用sechedule_timeout()函数前必须首先将任务设置成上面两种状态之一，否则任务不会睡眠。

注意，由于schedule_timeout()函数需要调用调度程序，所以调用他的代码必须保证能够睡眠。也就是说，调用代码必须处于进程上下文中，并且不能持锁。

##### 1.schedule_timeout()的实现

schedule_timeout()函数的用法相当简单、直接。其实，他是内核定时器的一个简单应用。请看下面的代码：

```c
signed long schedule_timeout(signed long timeout)
{
    timer_t timer;
    unsigned long expire;
    switch(timeout)//进入switch都是特殊情况
    {
        case MAX_SCHEDULE_TIMEOUT:
            schedule();
            goto out;
        default:
            if(timeout<0)
            {
                printk(KERN_ERR "schedule_timeout:wrong timeout" "value %1x from %p\n",timeout,__builtin_return_address(0));
                current->state = TASK_RUNNING;
                goto out;
            }
    }//schedule好schedule_timeout特点就是会创建一个定时器，这样延迟一段时间后就可以唤醒了。
    expire = timeout + jiffies;
    init_timer(&timer);//初始化定时器结构体
    timer.expires  = expire;//设置延迟时间
    timer.data = (unsigned long )current;//传入处理程序的是进程描述符
    timer.function = process_timeout;
    add_timer(&timer);
    schedule();//本质也是schedule()函数
    timeout = expire - jiffies;
    out:
    return timeout < 0 ? 0 :timeout;
}
```

该函数用原始的名字timer创建了一个定时器timer；然后设置他的超时时间timeout；设置超时执行函数process_timeout()；接着激活定时器而且调用schedule()。因为任务被标识为TASK_INTERRUPTIBLE或TASK_UNINTERRUPTIBLE，所以调度程序不会再选择该任务投入运行，而会选择其他新任务运行。

当定时器超时时，process_timeout()函数会被调用：

```c
void process_timeout(unsigned long data)
{
wake_up_process((task_t *)data);
}
```

该函数将任务设置为TASK_RUNNING状态，然后将其放入运行队列。

当任务重新被调度时，将返回代码进入睡眠前的位置继续执行（正好在调用schedule()后）。如果任务提前被唤醒，那么定时器被撤销，process_timeout()函数返回剩余的时间。

在switch()括号中的代码是为处理特殊情况而写的，正常情况不会用到他们。MAX_SCHEDULE_TIMEOUT是用来检查任务是否无限期地睡眠，如果那样的话，函数不会为其设置定时器的，此时调度程序会立刻被调用。

##### 2.设置超时时间，在等待队列上睡眠

进程上下文中国的代码为了等待特定事件发生，可以将自己放入等待队列，然后调用调度程序去执行新任务。一旦事件发生后，内核调用wake_up()函数唤醒在睡眠队列上的任务，使其重新投入运行。

有时，等待队列上的某个任务可能既在等待一个特定事件到来，又在等待一个特定时间到来，这种情况下，代码可以简单地用schedule_timeout()函数代替schedule()函数，这样会顺便创建一个定时器，这样当 希望的指定时间到期，任务都会被唤醒。当然，嗲am需要检查被唤醒原因，然后执行相应的操作。

### 11.9小结

本章中，考察了时间的概念，并知道了墙上时钟与计算机的正常运行时间如何管理。对比了相对时间和绝对时间以及绝对事件和周期事件。我们还涵盖了诸如时钟中断、时钟节拍、HZ以及jiffies等概念。（时钟中断是每个节拍时中断，节拍是频1/率，每个一定时间间隔发生一次节拍，HZ就是系统频率，jiffies是当前总节拍）

考察了定时器的实现，如何把这些用到自己的内核代码中，本章最后，浏览了开发者用于延迟的其他方法。

写的大多数内核diamante都需要对时间以及走过的时间有一些理解。最大的可能是只要编写驱动程序，就需要处理内核定时器。

但是啊，因为本章是在我大病初愈时学的，加上我目前心态上有点着急回家，学的并不好。

## 第十二章内存管理

----

现在算是进入正题了，这才是真正需要复习的地方，ARM架构主要针对的就是CPU、指令集架构、存储器管理。

-----

在内核里分配内存可不像在其他地方分配内存那么容易。造成这种局面的因素很多，从根本上将，是因为内核本身不能像用户空间那样 奢侈的使用内存。内核与用户空间不同，不支持简单便捷的内存分配方式。比如，内核一般不能睡眠。（这里其实不太懂，内核为什么和睡眠有关，进程需要睡眠，内核理解为最大的进程吗）。此外，处理内存分配错误对内核来说也绝非易事。政工师由于这些限制，再加上内存分配机制不能太复杂，所以在内核中获取内存要比在用户空间复杂得多。不过，从程序开发者角度来看，至少和用户空间中的内存分配不太一样而已。

本章讨论的是在内核之中获取内存的方法。在深入研究实际的分配接口之前，需要理解内核是如何管理内存的。

### 12.1页

内核把物理页作为内存管理的基本单位，尽管处理器的最小可寻址单位通常是字（我记得好像是double类型，甚至是字节char）但是，内存管理单元（MMU，管理内存并把虚拟地址转换为物理地址的硬件）通常以页为单位进行处理。正因如此，MMU以页page大小为单位来管理系统中的页表（这也是页表名的由来）。从虚拟内存的角度来看，页就是最小单位。换入换出的都是以页为单位，不会以字节为单位的。（页对应页表项，一般页是4KB，主页表中的一个页表项是1MB）

---

引入分段式内存管理的原因：

各个内存段也是由分段式导致的，让内存段具有逻辑意义，段式管理内存，页式管理物理内存。

方便编程、信息保护、动态增长、动态链接（运行时动态加载需要的目标程序段）、而编译期间和内存无关。

----

在第19章中我们将会看到，体系结构不同，支持的页大小也不尽相同，还有些体系结构甚至支持几种不同的页大小。大多数32位体系结构支持4KB的页，而64位体系结构一般会支持8KB的页。这就意味着，在支持4KB页大小并有1GB物理内存的机器上，物理内存会被划分为1024 * 1024/4=262144个页。

内核用struct page结构表示系统中的每个物理页，该结构位于<linux/mm_types.h>中，简化了定义，去除了两个容易让我们混淆的联合结构体：（之前学过分为区域结构体RCB和页表结构体PTCB，区域结构体描述的是主页表和二级页表中的区域转换数据，就是段项和页表项的值，而页表结构体表示的是主页表以及每个二级页表，这两种结构体用于初始化页表，是内核启动时使用的，所以本书中没提到也正常）（这里表示的是每个物理页，也就是页帧的内容）

----

这里提到了页帧的三种用途：用于存放进程页表、作为私有数据（这指的是作为进程的数据存储吧）、作为页缓存使用。

而物理页帧结构体会记录对应的虚拟地址。区域分为固定区域和映射，内核空间和物理外设区是虚拟地址==物理地址，任务区域是需要虚实映射。这里提到的高端内存的非永久映射关系怎么理解。

```c
struct page {
	unsigned long flags;//存放页面状态，主要是脏位、被锁定在内存中（这句怎么理解，映射的时候要求的，不允许页存在于磁盘中，如果存在于磁盘要马上转入内存中）
    atomic_t _count;//计数页被引用次数，如果没被引用就分配出去
    atomic_t _mapcount;
    unsigned long private;//页作为私有数据时，与此有关
    struct address_space *mapping;//当页被缓存的时候，指向该类型对象
    pgoff_t index;
    struct list_head lru;//链表，将cgroup组内的内存以page为单位连接起来。
    void *virtual;//页的虚拟内存中的地址，不光是页表项对应物理地址，相互指向的
    PG_swapbacked;//此页可写入swap分区，表示此页是非文件页
    PG_swapcache;//页已经加入到swap cache中
};
```

看一下其中比较重要的域flag，这是用来存放页的状态。这些状态包括页是不是脏的，是不是被锁定在内存中。flag的每一位单独表示一种状态，所以他至少可以同时表示出32种不同的状态。这些标志定义在<linux/page-flags.h>中。

_count存放页的引用计数，也就是这一页被引用了多少次，当计数值变为-1的时候，就说明当前内核并没有引用这一页，于是，在新分配中就可以使用它。内核代码不应当直接检查该域，而是调用page_count()函数进行检查，该函数唯一的参数就是page结构。这个应该是和页面换入换出算法有关。当页空闲时，尽管该结构内部的 _count值是负的，但是对page _count()函数而言，返回0表示页空闲，返回一个正整数表示页在使用。一个页可以由页缓存使用（这时，mapping指向和这个页关联的address_space对象），或者作为私有数据（由private指向），或者作为进程页表中的映射。

virtual域是页的虚拟地址。通常情况下，他就是页在虚拟内存中的地址。有些内存（所谓的高端内存）并不永久地映射到内核地址空间上。在这种情况下，这个域的值为NULL，需要的时候，必须动态地映射这些页。

必须要理解的一点是page结构与物理页相关，而并非与虚拟页相关。而并非与虚拟页相关。因此，该结构对页的描述只是短暂的。即使页中所包含的数据继续存在，由于交换等原因，他们也可能并不再和同一个page结构相关联。内核仅仅用这个数据结构来描述当前时刻在相关物理页中存放的东西。这种数据结构的目的在于描述物理内存本身，而不是描述包含其中的数据。（这句话不理解，内存本身就是物理地址了，不就是描述数据吗）

内核用这一结构来管理系统中所有的页，因为内核需要知道一个页是否空闲。如果页已经被分配，内核还需要知道谁拥有这个页。拥有者可能是用户空间进程、动态分配的内核数据、静态内核代码或页高速缓存等。

系统中的每个物理页都要分配一个这样的结构体，开发者常常对此感到惊讶，计算下一共需要花费的内存也就是20MB而已。因此管理系统中这么多的物理页面，这个代价并不算太高。

### 12.2区

由于硬件的限制，内核并不能对所有的页一视同仁。有些页位于内存中特定的物理地址上，所以不能将其用于一些特定的任务。由于存在这种限制，所以内核把页划分为不同的区（zone）。内核使用区对具有相似特性的页进行分组。Linux必须处理如下两种由于硬件存在缺陷而引起的内存寻址问题：（这里的区是物理地址划分的，而不是region区域的意思。）

- 一些硬件只能用某些特定的内存地址来执行DMA（直接内存访问）（cache一般是逻辑cache，使用的是虚拟地址查找所以这里没有，其实我对DMA的具体执行流程不熟悉）
- 一些体结构的内存的物理寻址范围比虚拟寻址范围大得多。这样，就有一些**内存不能永久地映射到内核空间**上。神奇，虚拟寻址多大呢（物理地址大于虚拟地址所以会出现流动的虚实映射关系）

因为存在这些制约条件，Linux主要使用了四种区：

- ZONE_DMA，这个区包含的页能用来执行DMA操作
- ZONE_DMA32，和ZOME_DMA类似，该区包含的页面可用来执行DMA操作；而和ZONE_DMA不同之处在于，这些页面只能被32位设备访问。在某些体系结构中，该区将比ZONE_DMA更大。
- ZONE_NORMAL，这个区包含的都是能正常映射的页
- ZONE_HIGHEM，这个区包含高端内存，其中的页并不能永久地映射到内核地址空间。这些区，在<linux/mmzone.h>中定义。

---

内核空间的1GB中包含了这四种区域。

----

区的实际使用和分布是与体系结构相关的。例如，某些体系结构在内存的任何地址上执行DMA都没问题，在这些体系结构中，ZONE_DMA为空，ZONE_NORMAL就可以直接用于分配。与之相反，在x86体系结构上，ISA设备就不能在整个32位的地址空间中执行DMA，因为ISA设备只能访问物理内存的前16MB。因此，ZONE_DMA在x86上包含的页都在0-16MB的内存范围中。

ZONE_HIGHMEM的工作方式也差不多。能否直接映射取决于体系结构。在32位x86系统上，ZONE_HIGHMEM为高于896MB的所有物理内存。在其他体系结构上，由于所有内存都被直接映射，所以ZONE_HIGHMEM为空。ZONE_HIGHMEM所在的内存就是所谓的高端内存（high memory）。系统的其余内存就是所谓的低端内存（low memory）。

前两个区各取所需之后，剩余的就由ZONE_NORMAL区独享了。在x86上，ZONE_NORMAL是从16MB到896MB的所有物理内存。在其他的体系结构上，ZONE_NORMAL是所有的可用物理内存。下表是每个区以及在x86-32上所占页的列表。

| 区           | 描述           | 物理内存 |
| ------------ | -------------- | -------- |
| ZONE_DMA     | DMA使用的页    | <16MB    |
| ZONE_NORMAL  | 正常可寻址的页 | 16~896MB |
| ZONE_HIGHMEM | 动态映射的页   | >896MB   |

Linux把系统的页划分为区，形成不同的内存池，这样就可以根据用途进行分配了，例如ZONE_DMA内存池让内核有能力为DMA分配所需的内存。如果需要这样的内存，那么，内核就可以从ZONE_DMA中按照请求的数目取出页。注意，区的划分没有任何物理意义，这只不过是内核为了管理页而采取的一种逻辑上的分组。

某些分配可能需要从特定的区中获取页，而另外一些分配则可以从多个区中获取页。比如，尽管用于DMA的内存必须必须从ZONE_DMA中进行分配，但是一般用途的内存也可以从这里分配，不过不可能同时从两个区分配，因为分配是不能跨区界限的。当然，内核更希望一般用途的内存从常规区分配。

不是所有的体系结构都定义了全部区，有些64位的体系结构，如Intel的x86-64体系结构可以映射和处理64位的内存空间，所以x86-64没有ZONE_HIGHMEM区，所有的物理内存都处于ZONE_DMA和ZONE_NORMAL区。

每个区都用struct zone表示，在<linux/mmzone.h>中定义：

```c
struct zone{
	unsigned long watermark[NR_WMARK];//数组持有该区的最小值、最低值和最高水位值。
    unsigned long lowmem_reserve[MAX_NR_ZONES];
    struct per_cpu_pageset pageset[NR_CPUS];
    spinlock_t lock;//自旋锁用于防止该结构被并发访问。
    struct free_area free_area[MAX_ORDER];
    spinlock_t lru_lock;
    struct zone_lru{//使用链表维护整个zone，页帧结构体也有对应的链表，回收内存的时候就会遍历zonelist
        struct list_head list;
        unsigned long nr_saved_scan;
    }lru[NR_LRU_LISTS];
    struct zone_reclaim_stat reclaim_stat;
    unsigned long pages_scanned;
    unsigned long flags;
    atomic_long_t vm_stat[NR_VM_ZONE_STAT_ITEMS];
    int prev_priority;
    unsigned int inactive_ratio;
    wait_queue_head_t *wait_table;
    unsigned long wait_table_hash_nr_entries;
    unsigned long wait_table_bits;
    struct pglist_data *zone_pgdat;
    unsigned long zone_start_pfn;
    unsigned long spanned_pages;
    unsigned long present_pages;
    const char *name;
}
```

这个结构体很大，但是，系统中只有三个区，因此，也只有三个这样的结构体。看一下其中重要的域。

lock域是一个自旋锁，它防止该结构被并发访问。注意，这个域只保护结构，而不保护驻留在这个区中的所有页。没有特定的锁来保护单个页，但是，部分内核可以锁住在页中驻留的数据。

watermark数组持有该区的最小值、最低和最高水位值。内核使用水位为每个内存区设置合适的内存消耗基准。该水位随空闲内存的多少而变化。（IIO缓冲区中有这个值，表示的是存储多少字节允许被读取）这个对应min/low/high阀值吧。

name域是一个以NULL结束的字符串表示这个区的名字。内核启动期间初始化这个值，其代码位于mm/page_alloc.c中。三个区的名字分别为DMA、Normal和HighMem

### 12.3获得页

我们已经对内核如何管理 内存（页、区）有所了解了，现在看下内核实现的接口，通过这些接口在内核中分配和释放内存的。

内核提供了一种请求内存的底层机制，并提供了对它进行访问的几个接口。所有这些接口都以页为单位分配内存，定义于<linux/gfp.h>中。最核心的函数是：

```c
struct page * alloc_pages(gfp_t gfp_mask ,unsigned int order)
```

该函数分配2的order次幂个连续的物理页，并返回一个指针，该指针指向第一个页的page结构体；如果出错，返回NULL，在以后在研究gft_t类型和gft_mask参数。你可以用下面这个函数把给定的页转换成他的逻辑地址：

```c
void *page_address(struct page *page)
```

该函数返回一个指针，指向给定物理页当前所在的逻辑地址，之前page里面存放着虚拟地址，应该就是调用该成员virtual

如果无须用到struct page，可以调用：

```c
unsigned long __get_free_pages(gfp_t gfp_mask,unsigned int order)
```

这个函数与alloc_pages()作用相同，只不过他直接返回所请求的第一个页的逻辑地址。因为页是连续的，所以其他也会紧随其后。

如果只需要一页，就可以用下面两个封装好的函数，他能让你少敲几下键盘：

```c
struct page*alloc_page(gfp_t gfp_mask)
    unsigned long __get_free_page(gfp_t gfp_mask)
```

只不过传递给order的值是0,2^0=1

#### 12.3.1获得填充为0的页

如果需要让返回的页的内容全为0，请用下面这个函数：

```c
unsigned long get_zeroed_page(unsigned int gfp_mask)
```

这个函数和__get_free_pages函数工作方式一样，只不过把分配好的页都填充成了0，字节中的每一位都要取消设置。如果分配的页是给用户空间的，这个函数就非常有用了，虽说分配好的页中应该包含的都是随机产生的垃圾信息，但其实这些信息可能随机地包含某些敏感数据。用户空间的页在返回之前，所有数据必须填充为0，或做其他清理工作，在保障系统安全这一点上，决不妥协。

#### 12.3.2释放页

当你不再需要页时，可以用下面的函数释放他们：

```c
void __free_pages(struct page * page,unsigned int order)
void free_pages(unsigned long addr,unsigned int order)
void free_page(unsigned long addr)
```

释放页时要谨慎，只能释放属于你的页。传递了错误的struct page或地址，用了错误的order值，这些都可能导致系统崩溃。请记住，内核是完全信任自己的，这点和用户空间不同，如果有什么非法操作，内核会停止运行。

接下来看一个例子，其中想得到8个页：

```c
unsigned long page:
page = __get_free_pages(GFP_KERNEL,3);//申请8页
if(!page){
//没有足够的内存
return -ENOMEM;
}
//page现在指向8个连续页中第一个页的地址
在此，我们使用完这8个页之后释放他们：
    free_pages(page,3);
//释放页
```

GFP_KERNEL参数是gfp_mask标志的一个例子。

调用__get_free_pages()之后要注意进行错误检查，内核分配可能失败，因此代码必须检查返回值做相应的处理。这意味着在此之前，所做的所有工作可能前功尽弃。当需要以页为单位的一族连续物理页时，尤其是在只需要一两页时，这些低级页函数很有用。对于常用的以字节为单位的分配来说，内核提供的函数是kmalloc()。终于讲到这个了。

### 12.4kmalloc()

kmalloc()函数与用户空间的malloc()一族函数非常类似，只不过它多了一个flags参数。kmalloc()函数是一个简单的接口，用它可以获得以字节为单位的一块内核内存。如果需要整个页，那么，前面讨论的页分配接口可能是更好的选择。但是对于大多数内核分配来说，kmalloc()接口用的更多。

kmalloc()在<linux/slab.h>中声明：

```c
void *kmalloc(size_t size,gfp_t flags)
```

这个函数返回一个指向内存块的指针，其内存块至少要有size大小。所分配的内存区在物理上是连续的，在出错时，他返回NULL，除非没有足够的内存可用，否则内核总能分配成功。在对kmalloc()调用之后，你必须检查返回值，如果是NULL就要处理错误。

```c
struct dog *p;
p = kmalloc(sizeof(struct dog),GFP_KERNEL);
if(!p)
    //处理错误
```

如果kmalloc()调用成功，那么ptr现在指向一个内存块，内存块大小至少为所请求的大小。GFP_KERNEL标志表示在试图获取内存并返回给kmalloc()的调用者的过程中，**内存内存分配器将要采取的行为**。这里算是介绍了下标志位的作用。

#### 12.4.1gfp_mask标志

我们已经看过了几个例子，发现不管是在低级页分配函数中，还是在kmalloc()中，都用到了分配器标志。现在深入讨论下这些标志。

这些标志可分为三类：行为修饰符、区修饰符以及类型。行为修饰符表示内核应当如何分配所需的内存。在某些特定情况下，只能使用某些特定的方法分配内存。例如，中断处理程序就要求内核在分配内存的过程中不能睡眠（因为中断处理程序不能被重新调度）。区修饰符表示表示从哪分配内存。前面已经知道了，区修饰符指明到底从这些区中的哪一区中进行分配。

类似标志组合了行为修饰符和区修饰符，将各种可能用到的组合归纳为不同类型，简化修饰符的使用；这样只需要指定一个类型标志就可以了。GFP_KERNEL就是一个类型标志，内核中进程上下文相关的代码可以使用它。

##### 1.行为修饰符

所有这些标志，包括行为描述符都是在<linux/gfp.h>中声明的。不过在slab.h中包含有这个头文件。事实上，一般只使用类型修饰符即可。

可以同时指定这些分配标志，例如：

```c
ptr = kmalloc(size,__GFP_WAIT | __GFP_IO | __GFP_FS);
```

说明页分配器（最终调用alloc_pages()）在分配时可以等待、执行IO，还可以执行文件系统操作。这就让内核有很大的自由度，以便他尽可能找到空闲的内存来满足分配请求。

##### 2.区修饰符

区修饰符表示内存区应当从何处分配，通常，分配可以从任何区开始。内核优先从ZONE_NORMAL开始，这样可以确保其他区在需要时有足够的空闲页可供使用。

实际上只有两个区修饰符，因为除了默认的ZONE_NORMAL之外只有两个区。

指定标志中的一个就可以改变内核试图进行分配的区。_GFP_DMA标志强制内核从ZOME_DMA分配。

不能给_ get_free_pages()或kalloc()指定ZONE_HIGHMEM，因为这两个函数返回的都是逻辑地址，而不是page结构，这两个函数分配的内存当前有可能还没有映射到内核的虚拟地址空间，因此，也可能根本没有逻辑地址。只有alloc_pages()才能分配高端内存。实际上，你的分配在大多数情况下都不必指定修饰符，ZONE_NORMAL就足够了.

##### 3.类型标志

类型标志指定所需的行为和区描述符以完成特殊类型的处理。正因为这一点，内核代码趋向于使用正确的类型标志，而不是一味地指定它可能需要用到的多个描述符。这么做既简单又不容易出错误。下面仅介绍一种标志

| 标志       | 描述                                                         |
| ---------- | ------------------------------------------------------------ |
| GFP_KERNEL | 这是一种常规分配发放时，可能会阻塞。这个标志在睡眠安全时用在进程上下文代码中，为了获得调用者所需的内存，内核会尽力而为。这个标志应当是首选标志。(__ GFP_WAIT \|  __ GFP_IO \| __ GFP_FS) |

让我们看一下最常用的标志以及什么时候、为什么需要使用他们。内核中最常用的标志是GFP_KERNEL。这种分配可能会引起睡眠，他使用的是普通优先级。因为调用可能阻塞，因此这个标志只用在可以重新安全调度的进程上下文中（也就是说锁没有被持有的情况下）。因为这个标志对内核如何获取请求的内存没有任何约束，所以内存分配成功的可能性很高。

另一个截然相反的标志是GFP_ATOMIC，因为这个标志表示不能睡眠的内存分配，因此想要满足调用者获取内存的请求将会收到很严格的限制。即使没有足够的连续内存块可供使用，内核也很可能不会释放出可用内存来，因为内核不能让调用者睡眠。相反，GFP_KERNEL分配可以让调用者睡眠、交换、刷新一些页到硬盘等。因为GFP_ATOMIC不能执行以上任何操作，因此与GFP_KERNEL相比，它分配成功的机会较小。即便如此，在当前代码不能睡眠时，只能选择GFP_ATOMIC(中断处理程序、软中断和tasklet)。

在以上两种标志中间的是GFP_NOIO和GFP_NOFS。这两个标志进行的分配可能会引起阻塞，但他们会避免执行某些其他操作。GFP_NOIO分配绝不会启动任何磁盘IO来帮助满足请求，而GFP_NOFS可能和启动磁盘IO，但是他不会启动文件系统IO。为什么需要这些标志，他们分别用在某些低级块IO或者文件系统的代码中。设想，如果文件系统代码中需要分配内存，但是没有使用GFP_NOFS，这种分配可能会引起更多的文件系统操作，而这些操作又会导致另外的分配，从而引起更多的文件系统操作。这样的代码在调用分配器的时候，必须确保分配器不会再执行到代码本身，否则，分配就可能产生死锁。也别紧张，内核使用这两个标志的地方是很少了。

GFP_DMA标志表示分配器必须满足从ZONE_DMA进行分配的请求，这个标志用在需要DMA的内存的设备驱动程序中。一般你会把这个标志与GFP_ATOMIC和GFP_KERNEL结合起来使用。

在编写的绝大多数代码中，用到的要么是GFP_KERNEL，要么是GFP_ATOMIC。

#### 12.4.2kfree()

kmalloc()的另一端就是kfree()，kfree()声明于<linux/slab.h>中：

```c
void kfree(const void *ptr);
```

kfree()函数释放由kmalloc()分配出来的内存块，如果想要释放的内存不是由kmalloc()分配的，或者想要释放的内存早就被释放了，调用这个函数会导致很严重的后果，和用户空间类似，分配和回收要注意配对使用，以避免内存泄漏和其他bug。注意调用kfree(NULL)是安全的。

让我们看一个在中断处理程序中分配内存的例子，在这个例子中，中断处理程序想分配一个缓冲区来保存输入数据。BUF_SIZE预定义为以字节为单位的缓冲区长度，他应该是大于两个字节的。

```c
char *buf;
buf = kmalloc(BUF_SIZE,GFP_ATOMIC);
if(!buf)
    //内存分配出错！
    kfree(buf);
```

### 12.5vmalloc()

---

内核中分为kmalloc、kzalloc、kcalloc三种。第一个仅仅申请一片空间，第二个申请后置零，第三个对数组进行分配空间。

---

vmalloc()函数的工作方式类似于kmalloc()，只不过前者分配的内存虚拟地址是连续的，而物理地址则无须连续。这也是用户空间分配函数的工作方式：由malloc()返回的页在进程的虚拟地址空间内是连续的，但是，这并不保证他们在物理RAM中也是连续的。kmalloc()函数确保页在物理地址上是连续的（虚拟地址自然也是连续的）。vmalloc通过分配非连续的物理内存块，再修正页表，把内存映射到逻辑地址空间的连续区域中。

大多数情况下，只有硬件设备需要得到物理地址连续的内存，在很多体系结构上，硬件设备存在于内存管理单元以外，他根本不理解什么是虚拟地址。因此硬件设备用到的任何内存区都必须是物理上连续的块，而不仅仅是虚拟地址连续上的块。而仅供软件使用的内存块比如进程相关的缓冲区，就可以使用只有虚拟地址连续的内存块。但在编程中，根本察觉不到这种差异。对内核来说，所有内存看起来都是逻辑上连续的。

尽管在某些情况下才需要物理上连续的内存块，但是，很多内核代码都用kmalloc()来获得内存，而不是vmalloc()。这主要是出于性能的考虑。vmalloc()函数为了把物理上不连续的页转换为虚拟地址空间上连续的页，必须专门建立页表项。糟糕的是，通过vmalloc()获得的页必须一个一个地进行映射，这就导致比直接内存映射大得多的TLB抖动，（TLB存在于MMU内部，是由一组重定位寄存器组成的）（快表抖动之前学过，我记得是频繁的换入换出导致的）。因为这些原因，vmalloc()仅在不得已时才会使用，典型的就是为了获得大块内存时，时，例如当模块被动态的插入到内核中时，就把模块装载到由vmalloc()分配的内存上。

vmalloc()函数声明在<linux/vmalloc.h>中，定义在<mm/vmalloc.c>中。用法和用户空间的malloc()相同：

```c
void * vmalloc(unsigned long size)
```

该函数返回一个指针，指向逻辑上连续的一块内存区，其大小至少为size。在发生错误时，函数返回NULL。函数可能睡眠，因此，不能从中断上下文中进行调用，也不能从其他不允许阻塞的情况下进行调用。

要释放通过vmalloc()所获得的的内存，使用下面的函数:

```c
void vfree(const void * addr)
```

这个函数会释放从addr开始的内存块，其中addr是以前由vmalloc()分配的内存块的地址，也不能再中断上下文中调用。他没有返回值。

TLB(translation lookaside buffer)是一种硬缓冲区，很多体系结构用它来缓存虚拟地址到物理地址的映射关系，它极大地提高了系统的性能，因为大多数内存都要进行虚拟寻址。

### 12.6slab层

分配和释放数据结构是所有内核中最普遍的操作之一。为了便于数据的频繁分配和回收，编程人员常常会用到空闲链表。空闲链表包含可供使用的、已经分配好的数据结构块。当代码需要一个新的数据结构实例时，就可以从空闲链表中抓取一个，而不需要分配内存，再把数据放进去。以后，当不再需要这个数据结构 的实例时，就把他放回空闲链表，而不是释放它。从这个意义上说，空闲链表相当于对象高速缓存，快速存储频繁使用的对象类型。（task_struct结构体就是因为slab分配器分配的空闲链表，所以不再内核栈中）

----

对于大内存的物理地址分配采用伙伴算法，对于小于一页的内存频繁的分配释放采用slab分配方法，这两个属于并行的关系，slab内存分配器是对伙伴分配算法的补充。确实，分配一个结构体还用不到2的幂个页表的伙伴算法。然后才是slob和slub对slab的优化。

---

在内核中，空闲链表面临的主要问题之一是不能全局控制，当可用内存变得紧缺时，内核无法通知每个空闲链表，让其收缩缓存的大小以便释放出一些内存来。实际上内核根本就不知道存在任何空闲链表。为了弥补这一缺陷，也为了使代码更加稳固，Linux内核提供了slab层（也就是所谓的slab分配器）。slab分配器扮演了通用数据结构缓存层的角色。

slab分配器试图在几个基本原则之间寻求一种平衡：

- 频繁使用的数据结构也会频繁分配和释放，因此应当缓存他们
- 频繁分配和回收必然会导致内存碎片（难以找到大块连续的可用内存）。为了避免这种现象，空闲链表的缓存会连续地存放。因为已释放的数据结构又会放回空闲链表，因此不会导致碎片。实际上不存在释放链表了，我申请一些连续存放的空闲链表，你要用申请拿给你用，不用了就还给我把链表清空。
- 回收的对象可以立即投入下一次分配，因此，对于频繁的分配和释放，空闲链表能提高其性能。
- 如果分配器知道对象大小、页大小和总的高速缓存的大小这样的概念，他会做出更明智的决策。
- 如果让部分缓存专属于单个处理器，那么分配和释放可以在不加SMP锁的情况下进行
- 如果分配器是与NUMA相关的，他就可以从相同的内存节点为请求者进行分配。这里的NUMA是啥
- 对存放的对象进行着色（color），以防止多个对象映射到相同的高速缓存行（cache line），Linux的slab层在设计与实现上充分考虑了以上原子。这里的高速缓存行经常提到，着色难道是红黑树吗？

#### 12.6.1slab层的设计

slab层把不同的对象划分为所谓高速缓存组，其中每个高速缓存组都存放不同类型的对象。每种对象类型对应一个高速缓存。例如，一个高速缓存用于存放进程描述符（task_struct结构的一个空闲链表），而另一个高速缓存存放索引节点对象（struct inode）。有趣的是，kmalloc()接口建立在slab层之上，使用了一组通用高速缓存。

然后，这些高速缓存又被划分为slab。slab由一个或多个物理上连续的页组成。一般情况下，slab也就仅仅由一页组成。每个高速缓存可以由多个slab组成。每个slab都包含一些对象成员，这里的对象指的是被缓存的数据结构。每个slab处于三种状态之一：满、部分满或者空。一个满的slab没有空闲的对象，slab中的所有对象都已被分配。一个空的slab没有分配出任何对象，当内核的某一部分需要一个新的对象时，先从部分满的slab中进行分配。如果没有部分满的slab，就从空的slab中进行分配。如果没有空的slab，就要创建一个slab了，显然，满的slab无法满足请求。

作为一个例子，让我们考察以下inode结构，该结构是磁盘索引节点在内存中的体现。这些数据结构会频繁地创建和释放，因此，用slab分配器来管理他们就很有必要。因为struct inode就由inode_cachep高速缓存（这是一种标准的命名规范）进行分配。这种高速缓存由一个或多个slab组成，由多个slab组成的可能性大一些，因为这样的对象数量很大。每个slab包含尽可能多的struct inode对象。当内核请求分配一个新的inode结构时，内核就从部分满的slab或空的slab（如果没有部分满的slab）返回一个指向已分配但未使用的结构的指针。当内核用完inode对象后，slab分配器就把该对象标记为空闲。

----

这个图就说明具体分布了。

<img src="C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1671280906018.png" alt="1671280906018" style="zoom:33%;" />

每个高速缓存都是用kmem_cache结构来表示。这个结构包含三个链表：slabs_full/slabs_partial和slabs_empty，均存放在kmem_list3结构内，该结构在mm/slab.c中定义。这些链表包含高速缓存中的所有slab，slab描述符struct slab用来描述每个slab：

```c
struct slab{
	struct list_head list;//满、部分满或空链表
    unsigned long colouroff;//slab着色的偏移量
    void *s_mem;//在slab中的第一个对象，（slab中存在多个对象，所以描述slab的结构体成员就是多个对象）
    unsigned int inuse;//slab中已分配的对象数
    kmem_bufctl_t free;//第一个空闲对象
};
```

slab描述符要么在slab之外另行分配，要么就放在slab自身开始的地方。如果slab很小或者slab内部有足够的空间容纳slab描述符，那么描述符就存放在slab里面。

slab分配器可以创建新的slab，这是通过__get_free_pages()低级内核页分配器进行的：

```c
static void *kmem_getpages(struct kmem_cache *cachep,gfp_t flags,int nodeid)
{
    struct page *page;
    void *addr;
    int i;
    flags |=cachep->gfpflags;
    if(likely(nodeid == -1)){
        addr = (void *)__get_free_pages(flags,cachep->gfporder);
        if(!addr)
            return NULL;
        addr = page_address(page);//将page结构体变成对应的页地址
    }else{
        page = alloc_pages_node(nodeid,flags,cachep->gfporder);
        if(!page)
            return NULL;
        addr = page_address(page);
    }
    i = (1<<cachep->gfporder);
    if(cachep->flags & SLAB_RECLAIM_ACCOUNT)
        atomic_add(i,&slab_reclaim_pages);
    add_page_state(nr_slab,i);
    while(i--){
        SetPageSlab(page);
        page++;
    }
    return addr;
}
```

该函数使用__ get_free_pages()来为高速缓存分配足够多的内存，该函数的第一个参数就指向需要很多页的特定高速缓存。第二个参数是要传给  __ get_free_pages()的标志，注意这个标志是如何与另一个值进行二进制或运算的，这相当于把高速缓存需要的缺省标志加到flags参数上。分配的页大小为2的幂次方，存放在cachep->gfporder中。由于与分配器NUMA相关的代码的关系前面这个函数比想象的要复杂一些。当nodeid是一个非负数时，分配器就试图对从相同的内存节点给发出的请求进行分配。这在NUMA系统上提供了较好的性能，但是访问节点之外的内存会导致性能的损失。

为了便于理解，可以忽略与NUMA相关的代码。写一个简单的kmem_getpages()函数：

```c
static inline void *kmem_getpages(struct kmem_cache *cachep,gfp_t flags)//之前看到的slab结构体就是描述slab分配器的，而slab分配一个个对象，高速缓存结构体就是kmem_cache，此函数就是给高速缓存分配内存页的。
{
    void addr;
    flags |= cachep->gfpflags;
    addr = (void *)__get_free_pages(flags,cachep->gfporder);
    return addr;
}
```

这个版本就简单好懂了， 就是申请页。

接着，调用kmeme_freepages()释放内存，而对给定的高速缓存页，kmem_freepages()最终调用的是free_pages()。当然，slab层的关键就是避免频繁分配和释放页。由此可知，slab层只有当给定的高速缓存部分中既没有满也没有空的slab时才会调用页分配函数。而只有在下列情况下才会调用释放函数：当可用内存变得紧缺时，系统试图释放出更多内存以供使用；或者当高速缓存显式地被撤销时。

slab层的管理是**在每个高速缓存的基础上**，通过提供给整个内核一个简单的接口来完成的。通过接口就可以创建和撤销新的高速缓存，并**在高速缓存内分配和释放对象**。高速缓存及其内slab的复杂管理完全通过slab层的内部机制来处理。当创建了一个高速缓存后，slab层所起的作用就是一个专用的分配器，可以为具体的对象类型进行分配。

#### 12.6.2slab分配器的接口

就我目前感觉的slab就是一个分配器，专门用来分配空闲链表的。

一个新的高速缓存通过以下函数创建：

```c
struct kmem_cache *kmem_cache_create(const char *name,size_t size,size_t align,unsigned long flags,void(*ctor)(void*));//这个函数作用是创建一个新的高速缓存，之前是对高速缓存分配内存页。
```

第一个参数是字符串，存放着高速缓存的名字；第二个参数是高速缓存中每个元素的大小；第三个参数是slab内第一个对象的偏移，确保在业内进行特定的对齐。通常情况下，0可以满足要求也就是标准对齐。flags参数是可选的设置项，用来控制高速缓存的行为。可以是0，表示没有特殊的行为。最后一个参数ctor是高速缓存的构造函数。只有在新的页追加到高速缓存时，构造函数才被调用。实际上，Linux内核的高速缓存不使用构造函数。可以将ctor参数赋值为NULL。

这个函数不能在中断上下文中调用，因为他可能会睡眠。

要撤销一个高速缓存，则调用：

```c
int kmem_cache_destroy(struct kmem_cache *cachep)
```

顾名思义，这样就可以撤销给定的高速缓存。这个函数通常在模块的注销代码中被调用，同样不能在中断上下文中调用这个函数。调用该函数之前必须确保存在以下两个条件：

- 高速缓存中的所有slab都必须为空，其实，不管哪个slab中 ，只要有一个对象被分配出去了并正在使用的话，怎么可能撤销这个高速缓存呢？
- 调用kmem_cache_destroy()过程中不再访问这个高速缓存。使用者必须确保这种同步。

该函数在成功时返回0，否则返回非0值。

##### 1.从缓存中分配

创建高速缓存之后，就可以通过下列函数获取对象：

```c
void *kmem_cache_alloc(struct kmem_cache *cachep,gfp_t flags)//此函数是创建高速缓存后，从给定的高速缓存中返回一个指向对象的指针，void *返回类型就是对象指针，如果高速缓存的所有slab中都没有空闲的对象，那么slab层通过kmem_getpages()获取新的页，函数API上只看到了高速缓存和对象之间的操作，slab结构体没看到被使用。
```

该函数从给定的高速缓存cachep中返回一个指向对象的指针。如果高速缓存的所有slab中都没有空闲的对象，那么slab层必须通过kmeme_getpaegs()获取新的页，flags的值传递给__ get_free_pages()。也就是gfp_t类型的标志。为什么不使用kmem_cache_create函数，那个函数是用用来创建高速缓存结构体的，并不是用来申请内存的。现在要做的是扩展slab层的空间。

最后释放一个对象，并把它返回给原先的slab，可以使用下面这个函数：

```c
void kmem_cache_free(struct kmem_cache *cachep,void *objp)
```

这样就能把cachep中的对象objp标记为空闲。

##### 2.slab分配器的使用实例

考察一个鲜活的实例，这个例子用得上task_struct结构。取自kernel/fork.c

首先内核用一个全局变量存放指向task_struct高速缓存的指针：

---

我发现之前根本就没学明白slab分配器的具体使用流程。

```c
struct kmem_cache *task_struct_cachep;
```

在内核初始化期间，在定义于kernel/fork.c的**fork_init()中会创建高速缓存**：

```c
task_struct_cachep = kmem_cache_create("task_struct",sizeof(struct task_struct),ARCH_MIN_TASKALIGH,SLAB_PANIC | SLAB_NOTRACK,NULL);//这里创建一个高速缓存，名称为进程描述符，接下来分配一个进程描述符内存从高速缓存中，这就得到一个进程描述符了。
```

这样就创建了一个名为task_struct的高速缓存，每一个高速缓存其实就是一个链表。这个链表存放的类型就是struct task_struct的对象。要注意！！！这里的链表是用来存储的，至于task_struct内的链表那个CFS调度器的工作列表，用来检索接下来调度哪个进程的。存放还是存放在本章学到的这个高速缓存中。

该对象被创建后存放在slab中偏移量为ARCH_MIN_TASKALIGN个字节的地方，预定义值与体系结构相关。通常将它定义为L1_CACHE_BYTES，也就是L1高速缓存的字节大小。这个L1又是什么?

没有构造函数或析构函数，注意不用检查返回值是否为失败标记NULL，因为SLAB_PANIC标志已经被设置了。如果分配失败，slab分配器就调用panic()函数。如果没有提供SLAB_PANIC标志，就必须自己检查返回值。SLAB_PANIC标志用在这是因为这是操作系统 必不可少的高速缓存。

每当进程调用fork()时，一定会创建一个新的描述符。这是在dup_task_struct()中完成的，而该函数会被do_fork()调用：

```c
struct task_struct *tsk;
tsk=kmem_cache_alloc(task_struct_cachep,GFP_KERNEL);//每创建一个进程，就会申请一个高速缓存中的节点给tsk
if(!tsk)
	return NULL;
```

进程执行完后，如果没有子进程在等待的话，他的进程描述符就会被释放，并返回给task_struct_cachep slab高速缓存。这是在free_task_struct()中执行的：

```c
kmem_cache_free(task_struct_cachep,tsk);//给进程收尸的时候就会将进程描述符释放，因为进程描述符分配由slab分配器实现，释放同样如此。，但是这里只是释放进程描述符，而不是销毁进程描述符高速缓存。因为高速缓存时刻会被使用。
```

由于进程描述符是内核的核心组成部分，时刻都要用到，因此task_struct_cachep高速缓存绝不会被撤销掉。即使真的撤销，也要通过下面的函数阻止其撤销：

```c
int err;
err = kmem_cache_destroy(task_struct_cachep);
if(err)
    //这里销毁一个高速缓存。
```

slab层负责内存紧缺情况下所有底层的对齐、着色、分配、释放和回收等。如果要频繁创建很多相同类型的对象，那么，就应该考虑使用slab高速缓存，而不是自己实现空闲链表。

---

总的来说，slab结构体没看到在哪里被创建初始化，主要是创建高速缓存、从高速缓存中分配一个对象内存、如果无法分配就给高速缓存增加页、释放一个对象、销毁一个高速缓存。

所谓的高速缓存是因为使用了一组预分配的内存块来存储对象数据，当需要分配内存给一个对象时，kmem_cache快速返回一个空闲内存块，当该对象不再需要时，内存块被释放从而实现提高内存管理的效率。

----

### 12.7在栈上的静态分配

在用户空间，我们以前所讨论到的那些分配的例子，有不少都可以在栈上发生。因为我们毕竟可以事先知道所分配空间的大小。用户空间能够奢侈地负担起非常大的栈，而且栈空间还可以动态增长，相反，内核却不能这么奢侈，内核栈小且固定，当给每个进程分配一个固定大小的小栈后，不但可以减少内存的消耗，而且内核也无须负担太重的栈管理任务。

每个进程的内核栈大小既依赖体系结构，也与编译时的选项有关。历史上，每个进程都有两页的内核栈。因为32位和64位体系结构的页面大小分别是4KB和8KB，所以通常他们的内核栈大小分别是8KB和16KB，对于之前提到的用户空间的栈是动态增长的不理解。

#### 12.7.1单页内核栈

但是，在2.6系列内核的早期，引入了一个选项设置单页内核栈。当激活选项之后，每个进程的内核栈只有一页那么大，根据体系结构的不同，或为4KB，或为8KB，这么做出于两个原因：首先，可以让每个进程减少内存消耗。其次，随着机器运行时间的增加，寻找两个未分配的、连续的页变得越来越困难。物理内存渐渐变为碎片，因此，给一个新进程分配虚拟内存VM的压力也在增大。

还有一个更复杂的原因：目前掌握了关于内核栈的全部知识。现在，每个进程的整个调用链必须放在自己的内核栈中。不过，中断处理程序也曾经使用他们所中断的进程的内核栈，这样，中断处理程序也要放在内核栈中。之前我记得修改了，中断处理程序单独有自己的中断栈。

内核开发者们实现了一个新功能：中断栈。中断栈为每个进程提供一个用于中断处理程序的栈，这样中断处理程序不用再和被中断的进程共享一个内核栈了，他们可以使用自己的栈。对每个进程来说仅仅耗费了一页而已。

总的来说，内核栈可以是1页，也可以是2页，这取决于编译时配置选项。栈大小因此在4~16KB的范围内。

----

中断栈和内核栈的关系

中断栈是每个进程都具备的，因为以前中断是通过占用被打断的进程的内核栈，现在为每个进程配备一个中断栈，这样不需要占用内核栈了。

-----

#### 12.7.2在栈上光明正大地工作

在任意一个函数中，你都必须尽量节省栈资源。这并不难，也没有什么窍门，只需要在具体的函数中让所有局部变量所占空间之和不要超过几百字节。在栈上进行大量的静态分配是很危险的。要不然，在内核和用户空间中进行的栈分配就没有什么差别了。栈溢出时悄无声息，但势必会引起严重的问题。因为内核没有在管理内核栈上做足工作。

因此，当栈溢出的时候，多出的数据就会直接溢出来，覆盖掉紧邻堆栈末端的东西。其实就是栈头，而进程描述符是放在最前面的，其中的thread_info是放在进程描述符最前面的这是用来查找进程描述符的地址的。所以首先影响的就是thread_info结构。在堆栈之外，任何内核数据都可能存在潜在的危险。当栈溢出时，最好的情况死锁机器宕机，最坏的情况是悄无声息破坏数据。

因此，进行动态分配是一种明智的选择，本章有关大块内存的分配就是采用这种方式。

---

之前分析过内核栈是否存在栈溢出的可能，内核栈就是svc模式下的进程栈，而内核对内核栈溢出没有做足够多的工作。会覆盖掉thread_info结构。

----

### 12.8高端内存的映射

根据定义，在高端内存中的页不能永久地映射到内核地址空间上。因此，通过alloc_pages()函数以 __ GFP_HIGHMEM标志获得的页**不可能有逻辑地址**。（不太理解所谓的非永久性映射是啥意思，怎么就不能获得逻辑地址了，意思是使用完后需要解除映射关系。）

在x86体系结构上，高于896MB的所有物理内存的范围大都是高端内存，他并不会永久地或自动的映射到内核地址空间，尽管x86处理器能够寻址物理RAM的范围达到4GB（启用PAE可以寻址到64GB）。一旦这些页被分配，就必须映射到内核的逻辑地址空间上。在x86上，高端内存中的页被映射到3GB~4GB。

#### 12.8.1永久映射

要映射一个给定的page结构到内核地址空间，可以使用定义在文件<linux/highmem.h>中的这个函数：

```c
void *kmap(struct page *page)
```

这个函数在高端内存或低端内存上都能用，如果page结构对应的是低端内存中的一页，函数只会单纯地返回该页的虚拟地址。如果页位于高端内存，则会建立一个永久映射，再返回地址。这个函数可以睡眠，所以kmap()只能用在进程上下文中。（这里意思是没有虚实映射关系，所以要先创建页表项，然后再返回逻辑地址）

----

永久映射是因为高端内存并不会自动直接映射，需要使用kmap函数才能分配高端内存，因为内核空间是有限的。

----

因为允许永久映射的数量是有限的，（确实，如果没有这些限制，就不必搞得这么复杂了，把所有内存都映射为永久内存就行了），当不再需要高端内存时，，应该解除映射，这可以通过下面函数完成：（**因为高端内存无法永久映射，所以不需要高端内存后，需要解除映射**。）

```c
void kunmap(struct page *page)
```

当必须创建一个映射而当前上下文又不能睡眠时，内核提供了临时映射（所谓的原子映射）。**有一组保留的映射**，他们可以存放新创建的临时映射。内核可以**原子地**把高端内存中的一个页映射到某个保留的映射中。因此，临时映射可以用在不能睡眠的地方，比如中断处理程序中，因为获取映射绝不会阻塞。

---

所谓的临时映射，指的就是建立映射时不会睡眠，原理就是不需要自己创建页表项，通过原子的占用保留的页表项，不会因为创建新的内存做不到而被迫陷入睡眠，也就不会被睡眠了。并且原子性导致其不会被其他进程打断。

----

PAR是physical address extension的缩写，这是x86处理器的特点，这个特点使得x86处理器尽管只有32位的虚拟地址空间，但从物理上能寻址到32位（64GB）的内存空间。

通过下列函数建立一个临时映射：

```c
void *kmap_atomic(struct page *page,enum km_type type)
```

参数type是下列枚举类型之一，这些枚举类型描述了临时映射的目的，他们定义在<asm/kmap_types.h>中。

这个函数不会阻塞，因此可以用在中断上下文和其他不能重新调度的地方。他也禁止内核抢占，这是有必要的，因为映射对每个处理器都是唯一的。

通过下列函数取消映射：

```c
void kunmap_atomic(void *kvaddr,enum km_type type)
```

这个函数也不会阻塞。在很多体系结构中，除非激活了内核抢占，否则kmap_atomic()根本就无事可做，因为只有在下一个临界映射到来前上一个临时映射才有效。因此，内核完全可以忘掉kmap_atomic()直接覆盖即可，不需要取消映射。

### 12.9每个CPU的分配

支持SMP的现代操作系统使用每个CPU上的数据，对于给定的处理器其数据是唯一的，一般来说，每个CPU的数据存放在一个**数组**中。**数组中的每一项对应着系统上一个存在的处理器**。按当前处理器号确定这个数组的当前元素，这就是2.4内核处理每个CPU数据的方式。这种方式还可以，2.6内核的很多代码依旧使用他们，可以声明数据如下：

```c
unsigned long my_percpu[NR_CPUS];
```

然后，按如下方式访问它：

```c
int cpu;
cpu = get_cpu();//获得当前处理器，并禁止内核抢占
my_percpu[cpu]++;
printk("my_percpu on cpu=%d is %1u\n",cpu,my_percpu[cpu]);
put_cpu();
```

注意，上面的代码中并没有出现锁，这是因为所操作的数据对当前处理器来说是唯一的。除了当前处理器之外，没有其他处理器可接触到这个数据，不存在并发访问问题，所以当前处理器可以在不用锁的情况下安全访问它。

现在，内核抢占成了唯一需要关注的问题了，内核抢占会引起下面提出的两个问题：

- 如果代码被其他处理器抢占并重新调度，那么这时CPU变量就会无效，因为它指向的是错误的处理器（通常，代码获得当前处理器后就不可以睡眠）。
- 如果另一个任务抢占了代码，那么有可能在同一个处理器上发生并发访问my_percpu的情况，显然这属于一个竞争条件，只能由一个进程访问存放CPU数据的数组。

这是从CPU数组的角度考虑的内核抢占，之前是从锁机制考虑的，如果获得锁的普通进程阻塞了，而被中断处理程序抢占内核，因为中断处理程序是必须执行完才会离开中断上下文去执行普通进程，所以有可能导致中断处理程序获取自旋锁不得而一直自旋，还拥有着无限的时间片。

虽然如此，但是不必惊慌，因为在获取当前处理器号的时候，就已经禁止了内核抢占，相应的在调用put_cpu()就会重新激活当前处理器号。

### 12.10新的每个CPU接口

2.6内核为了方便创建和操作每个CPU数据，引进了新的操作接口，成为percpu。该接口归纳了前面所述的操作行为，简化了创建和操作每个CPU的数据。

前面讨论的创建和访问每个CPU的方法依然有效，不过大型对称多处理器计算机要求对每个CPU数据操作更简单，功能更强大，正是这种背景下，新接口应运而生。

头文件<linux/percpu.h>声明了所有的接口操作例程，可以在文件mm/slab.c和<asm/percpu.h>中找到他们的定义。

#### 12.10.1编译时的每个CPU数据

在编译时定义每个CPU变量易如反掌：

```c
DEFINE_PER_CPU(type,name);
```

这个语句 为系统中的每一个处理器都创建了一个类型为type，名字为name的变量实例，如果你需要在别处声明变量，以防范编译时警告，那么下面的宏将是你的好帮手：

```c
DECLARE_PER_CPU(type,name);
```

可以利用get_cpu_var()和put_cpu_var()例程操作变量。调用get_cpu_var()返回当前处理器上的指定变量，同时他将禁止抢占；另一方面put_cpu_var()将相应的重新激活抢占。

```c
get_cpu_var(name)++;
put_cpu_var(name);
//也可以获得别的处理器上的每个CPU数据：
per_cpu(name,cpu)++;
```

使用此方法要小心，因为不会禁止内核抢占，也不会提供任何形式的锁保护。如果一些处理器可以接触到其他处理器的数据，那么你就必须要给数据上锁。

另外还有一个需要提醒的问题：这些编译时每个CPU数据的例子，并不能在模块中使用，因为连接程序实际上将他们创建在一个唯一的可执行段中（.data.percpu）。如果你需要从模块中访问每个CPU数据，或者如果你需要动态创建这些数据，那还是有希望的。

#### 12.10.2运行时的每个CPU数据

内核实现每个CPU数据的动态分配方法类似于kmalloc()，该例程为系统上的每个处理器创建所需内存的实例，其原型在文件<linux/percpu.h>中：

```c
void *alloc_percpu(type);
void *__alloc_percpu(size_t size,size_t align);
void free_percpu(const void *);
```

宏alloc_percpu()给**系统中的每个处理器分配一个指定类型对象的实例**。其实是宏 _ alloc_percpu()的一个封装，这个原始宏接收的参数有两个：一个是要分配的实际字节数，一个是分配时要按多少字节对齐。而封装后的alloc_percpu()按照单字节对齐，按照给定类型的自然边界对齐。这种对齐方式最为常用。

比如：

```c
struct rabid_cheetah = alloc_percpu(struct rabid_cheetah);
```

等价于

```c
struct rabid_cheetah = __alloc_percpu(sizeof(struct rabid_cheetah),__alignof__(struct rabid_cheetah));
```

其中 __ alignof __是gcc编译器的一个功能，他会返回指定类型或lvalue所需对齐字节数。语义和sizeof一样。比如，下列程序在x86体系中返回4

如果指定一个lvalue，那么将返回lvalue的最大对齐字节数。这个lvalue是啥啊

相应的调用free_percpu()将释放所有处理器上指定的每个CPU数据。

无论是alloc_percpu()或是 __ alloc_percpu()都会返回一个指针，他用来间接引用动态创建的每个CPU数据，**内核提供了两个宏来利用指针获取每个CPU数据**：

就是之前讲的get_cpu_var(ptr);//返回一个void类型的指针，该指针指向处理器的ptr的拷贝

这个宏返回了一个指向当前处理器数据的特殊实例他同时会禁止内核抢占；而在put_cpu_var()中重新激活内核抢占。

看一个使用这些函数的完整例子。当然这个例子有点无聊，因为通常会一次分配够内存，就可以在各种地方使用它，或者再释放一次。不过，这个例子可清楚地说明如何使用这些函数。

```c
void *percpu_ptr;
unsigned long *foo;
percpu_ptr = alloc_percpu(unsigned long );//创建每个CPU数据，获得指向CPU数据的指针
if(!ptr)
    //内存分配错误
foo = get_cpu_var(percpu_ptr);//通过得到的指针找到具体的数据
//操作foo
put_cpu_var(percpu_ptr);
```

### 12.11使用每个CPU数据的原因

使用每个CPU数据具有不少好处。首先是**减少了数据锁定**。（数据锁定又是个啥）因为按照每个处理器访问每个CPU数据的逻辑，你可以不再需要任何锁 。这里的数据锁定就是锁保护。记住，只有这个处理器能访问这个数据的规则纯粹是一个编程约定。你需要确保本地处理器只会访问他自己的唯一数据。系统本身并不存在任何措施禁止开发者欺骗内核。

第二个好处是使用每个CPU数据可以大大**减少缓存失效**。失效发生在处理器试图使他们的缓存保持同步时。如果一个处理器操作某个数据，而该数据又存放在其他处理器缓存中，那么存放该数据的那个处理器必须清理或刷新自己的缓存。（这里有印象，缓存需要与主存保持同步）。持续不断的缓存失效称为缓存抖动，这样对系统性能影响颇大。使用每个CPU数据将使得缓存影响降至最低，因为理想情况下只会访问自己的数据。percpu接口缓存对齐（cache-align）所有数据，以便确保在访问一个处理器的数据时，不会将另一个处理器的数据代入同一个缓存线上。

综上所述，使用每个CPU数据会省去许多数据上锁，他唯一的安全要求就是要禁止内核抢占。而这点代价相比上锁要小得多，而且接口会自动帮我们完成这个步骤。每个CPU数据在中断上下文或进程上下文中使用都很安全。但要注意，不能在访问每个CPU数据过程中睡眠。

目前并不要求必须使用每个CPU的新接口。只要你禁止了内核抢占，用手动方法也就是原来讨论的数组就很好，但是新接口在将来更容易使用，而且功能也会得到长足的优化。如果确实决定在你的内核中使用每个CPU数据，请考虑新接口。

### 12.12分配函数的选择

在这么多分配函数和方法中，有时并不能搞清楚到底该选择那种方法分配，但这确实很重要。如果你需要连续的物理页，就可以使用某个低级页分配器或者kmalloc()。这是内核中内存分配的常用方法，也是大多数情况下你自己应该使用的内存分配方式。回忆一下，传递给这些函数 的两个最常见的标志是GFP_ATOMIC和GFP_KERNEL。前者表示进行不睡眠的高优先级分配，这是中断处理程序和其他不能睡眠的代码段的需要。对于可以睡眠的代码使用后者。

如果想从高端内存进行分配，就使用alloc_pages()。alloc_pages()函数返回一个指向struct page结构的指针，而不是一个指向某个逻辑地址的指针。因为高端内存**很可能并没有被映射**，因此，访问它唯一方式就是通过相应的struct page结构。为了获得真正的指针，应调用kmap()，把高端内存映射到内核的逻辑地址空间。（高端内存每次使用完需要销毁映射，所以可能只有物理地址。）

如果不需要物理上连续的页，仅仅需要虚拟地址上的连续的页，那么就使用vmalloc()（不过要记住vmalloc()相对kmalloc()来说，有一定的性能损失，这是因为每个页需要一一对应到页表项，因为物理地址不是连续的，所以需要一一对应）。vmalloc()函数分配的内存虚地址是连续的，但本身并不保证物理地址上的连续，和用户空间的分配非常类似，他也是把物理内存块映射到连续的逻辑地址空间上。

如果要创建和撤销很多大的数据结构，那么需要考虑建立slab高速缓存。slab层会给每个处理器**维持一个对象高速缓存**(**空闲链表**)，这种高速缓存会极大地提高对象分配和回收的性能。slab层不是频繁地分配和释放内存，而是为你把事先分配好的对象存放到高速缓存中。

当你需要一块新的内存来存放数据结构时，slab层一般无须另外去分配内存，而只需要从高速缓存中得到一个对象就可以了。

-----

交换空间是当全部的RAM被占用，并且还需要更多内存时，用磁盘空间代替RAM内存。使用交换分区和交换文件实现。

内存分配和内存回收的算法本书没有提到。

空闲链表分配策略：首次适应分配、循环首次适应分配和最佳适应分配。

- 首次适应分配将所发现的第一个满足分配的内存单元中进行分配
- 循环首次适应当查找完链表末尾时，会重新回到链表头继续查找。每次从上次成功过分配的位置开始查找。
- 最佳适应分配在空闲链表中找到满足分配且空间最小的单元进行分配。

介绍下最坏适应分配算法

要求将所有的空闲分区按其容量从大到小的顺序形成一空闲分区链，查找时只要看第一个分区能否满足作业要求。

slob分配器是simple link list of block针对较小的内存系统，采用最先适配算法。



页框：Page Frame系统内存管理的最小单位，页框对应于page结构体，也称为页帧，就是页面的控制信息。

DIMM：双列直插内存模块，和SDRAM的区别是物理结构方面的分类。提供64位的数据通道，是SDRAM集合形式的最终体现，至少包含一个P-Bank芯片集合，每个模组最多可以包含两个P-Bank内存芯片集合。

UDIMM是没有寄存器的DIMM，不会缓冲DDR/DDR2和DDR3 SDRAM信号。

RDIMM是寄存双列直插内存模块，缓冲SDRAM时钟、命令和地址信号。

FB-DIMM是全面缓冲的DIMM。

MMDC是最常见的内存控制器。对于DDR3，最大支持8bit的突发访问。

DRAM核心结构由多个内存单元组成，这些内存单元分成由行和列组成的二维阵列，所以访问需要分成两步，先寻找某个行的地址，在寻找特定列的地址，并且读取具有破坏性，所以读写操作后，要把行数据写回同一行中。完成后才能访问新的行。

物理bank位宽也就是P-Bank实际上是系统位宽，SDRAM芯片位宽 为存储芯片的实际支持位宽数。

---

### 12.13小结

本章中，我们学习了Linux内核如何管理内存，首先看到了内存空间的各种不同的描述单位，包括字节、页面和区。接着讨论了各种内存分配机制，其中包括页分配器和slab分配器（分配页和空闲链表分配器）。在内核中分配内存并非总是轻而易举的，因为必须小心的确保分配过程遵从内核特定的状态约束。比如分配过程中不得阻塞，或者访问文件系统等约束，为此讨论了gfp标识以及每种标识的针对场景。分配内存相对复杂是内核开发和用户程序开发的最大区别之一，本章使用大量篇幅描述内存分配的各种 不同接口，通过这些不同调用接口，能感觉到内核中分配内存为什么更复杂的原因。在本章基础上，在13章讨论虚拟文件系统VFS。负责管理文件系统且**为用户空间程序提供一致性接口的内核子系统**。（这里指的就是系统调用接口了）终于学到虚拟文件系统了。

## 第十三章虚拟文件系统

虚拟文件系统有时也称为虚拟文件交换，VFS作为内核子系统，为用户空间程序**提供了文件和文件系统相关的接口**。系统中所有文件系统不但依赖VFS共存，而且也依靠VFS系统协同工作。通过虚拟文件系统，程序可以利用标准的Uinx系统调用对不同的文件系统，甚至不同介质上的文件系统进行读取操作。

---

当时看八股的时候也提到了VFS层在流程上的体现，这里讲的太粗浅了。

<img src="C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1671343149468.png" alt="1671343149468" style="zoom: 25%;" />

### 13.1通用文件系统接口

VFS使得用户可以直接使用open()、read()/write()这样的系统调用而无须考虑具体文件系统和实际物理介质。但是，使得这些通用的系统调用可以跨越各种文件系统和不同介质执行，绝非是微不足道的成绩。更了不起的是，系统调用可以在这些不同的文件系统和介质之间执行，我们可以使用标准的系统调用从一个文件系统拷贝或移动到另一个文件系统。老师的操作系统DOS是物理完成上述工作的，任何对非本地文件系统的访问都必须依靠特殊工具才能完成。正是由于现代操作系统引入抽象层，比如Linux，通过虚拟接口访问文件系统，才使得这种协作性和泛型存取称为可能。

新的文件系统和新类型的存储介质都能找到进入Linux之路，程序无须重写，甚至无须重新编译。在本章中，我们将讨论VFS，**他把各种不同的文件系统抽象后采用统一的书写方式进行操作。**这句话应该就是在第14章中，将 讨论块IO层，它支持各种各样的存储设备，从CD到蓝光光盘，从硬件设备再到压缩闪存。VFS与块IO相结合，提供抽象、接口以及交融，使得用户空间的程序调用统一的系统调用访问各种文件，不管文件系统是什么，也不管文件系统位于何种介质，采用的命名策略是统一的。（其实我不知道文件系统是个啥，但知道文件系统与虚拟文件系统不一样，现在看来虚拟文件系统是把文件系统抽象了一层）。

### 13.2文件系统抽象层

之所以可以使用这种通用接口对所有类型的文件系统进行操作，是因为内核在他的底层文件系统接口上建立了一个抽象层。该抽象层使Linux能够支持各种文件系统，即便是他们在功能和行为上存在很大差别，为了支持多文件系统，VFS提供了一个通用文件系统模型，该模型囊括了任何文件系统的常用功能集合行为。当然，该模型偏重于Unix风格的文件系统。但即使这样，Linux仍然可以支持很多种差异很大的文件系统，从DOS系统的FAT到Windows系统的NTFS，再到各种Unix风格文件系统和Linux特有的文件系统。

VFS抽象层之所以能衔接各种各样的文件系统，是因为它定义了所有文件系统都支持的基本的，概念上的接口和数据结构。同时实际文件系统也将自身的诸如如何打开文件，目录是什么等概念在形式上与VFS的定义保持一致。因为实际文件系统的代码在统一的接口和数据结构下隐藏了具体的实现细节，所以在VFS层和内核的其他部分看来，所有文件系统都是相同的，他们都支持像文件和目录这样的概念，同时也支持像创建文件和删除文件这样的操作。

内核通过抽象层能够方便、简单地支持各种类型的文件系统。实际文件系统通过编程提供VFS所期望的抽象接口和数据结构，这样，内核就可以毫不费力地和任何文件系统协同工作，并且这样提供给用户空间的接口，也可以和任何文件系统无缝地连接在一起，完成实际工作。

其实在内核中，除了文件系统本身外，其他部分并不需要了解我呢间系统的内部细节。比如一个简单的用户空间程序执行如下操作：

```c
ret = write(fd,buf,len);
```

该系统调用将buf指针指向的长度为len字节的数据写入文件描述符fd对应的文件的当前位置。这个系统调用首先被一个通用系统调用sys_write()处理，要注意此时已经从用户层陷入到内核态了，并且根据传入的系统调用号找到了对应的内核中的系统调用sys_write函数。次函数要找到fd所在的文件系统实际给出的是哪个写操作，然后再执行该操作。实际文件系统的写方法是文件系统实现的一部分，数据最终通过该操作写入介质。下图描述了从用户空间的write()调用到数据被写入磁盘介质的整个流程。一方面，系统调用是通用VFS接口，提供给用户空间的前端；另一方面，系统调用是具体文件系统的后端，处理实现细节。接下来的小结中具体看到VFS抽象模块以及它提供的接口。

![1671345043345](C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1671345043345.png)

原来虚拟文件系统提供的内核中的系统调用。

### 13.3Unix文件系统

Unix使用了四种和文件系统相关的传统抽象概念：文件、目录项、索引节点和安装点（mount point）。

从本质上讲文件系统是特殊的数据分层存储结构，它包含文件、目录和相关的控制信息。文件系统的通用操作包含创建、删除和安装等。在Unix中文件系统被安装在一个特定的安装点上，该安装点在全局层次结构中被称为命名空间（之前进程也有各自的命名空间，学到现在我还是不理解命名空间是什么意思），所有的已安装文件系统都作为根文件系统树的枝叶出现在系统中。（这句话把各个文件系统与根文件系统之间的关系解释了）。与这种单一、统一的树形成鲜明对比的就是DOS和Windows的表现，他们将文件的命名空间分类为驱动字母，比如C:（这不就是C盘么，所谓的命名空间其实就是安装点，安装在C盘中）这种将命名空间划分为设备和分区的做法，相当于把硬件系统泄漏给文件系统抽象层。对用户而言，如此的描述有点随意，甚至产生混淆，这是Linux统一命名空间所不屑一顾的，所以Linux中就不划分安装点了

文件其实可以做一个有序字节串，字节串中第一个字节是文件的头，最后一个字节是文件的尾。每一个文件为了便于系统和用户识别，都被分配了一个便于理解的名字。典型的文件系统有读、写、创建和删除等。Unix文件的概念与面向记录的文件系统形成鲜明的对比。面向记录的文件系统提供更丰富、更结构化的表示，而简单的面向字节流抽象的Unix文件则以简单性和相当灵活性作为代价。

**文件通过目录组织起来**，文件目录好比一个文件夹，用来容纳相关文件。因为目录也可以包含其他目录，即子目录，所以目录可以层层嵌套，形成文件路径。路径中的每一部分都被称为目录条目。“/home/wolfman/butter”是文件系统路径的一个例子，其中根目录/，目录home、wolfman和文件butter都是目录条目，他们统称为目录项。在Unix中，目录属于普通文件，他列出包含在其中的所有文件。由于VFS把目录当做文件对待，所以可以对目录执行和文件相同的操作。

Unix系统将**文件的相关信息和文件本身两个概念加以区分**，比如访问控制权限、大小、拥有者、创建时间等信息。文件相关信息有时也被称为文件的元数据，也就是文件的相关数据，被存储在一个单独的数据结构中，该结构被称为索引节点inode，它其实是index node 的缩写，不过近来inode使用的更普遍一些。这个就是文件节点啊，应用篇中讲过。

所有这些信息都和**文件系统的控制信息密切相关**，文件系统的控制信息存储在超级块中，超级块是一种包含文件系统信息的数据结构。有时把这些收集起来的信息称为文件系统数据元，它集单独文件信息和文件系统的信息于一身。（不知道文件系统信息是啥，负责提供文件的管理的具体实现）

----

对文件的操作，内核会通过fd文件描述符找到相关的文件对象，再通过文件对象找到相应的驱动程序。因为内核不知道文件是存放在哪种文件系统中，所以需要根据文件对象决定具体实现。

-----

一直以来，Unix文件系统在他们物理磁盘布局中也是按照上述概念实现的。比如说在磁盘上，文件信息按照索引节点形式存储在单独的块中：控制信息被集中存储在磁盘的超级块中，等等。Unix中文件的概念从物理上被映射到存储介质。Linux的VFS的射击目标就是要保证能与支持和实现了这些概念的文件系统协同工作。像非Unix风格的文件系统，虽然也可以在Linux上工作，但是他们必须经过封装，提供一个符合这些概念的界面。比如，即使一个文件系统不支持索引节点，他也必须在内存中装配索引节点结构体，就像它本身包含索引节点一样。再比如，如果一个文件系统将目录看成一种特殊对象

进来，Linux已经将这种层次化概念引入了单个进程中，每个进程都指定一个唯一的命名空间，因为每个进程都会继承父进程的命名空间。所以所有进程往往都只有一个全局命名空间。命名空间理解成安装点，其实就是进程所在的地址。（每个进程都拥有自己的根文件系统，只能访问该进程内的文件）

那么要想使用VFS，就必须将目录重新表示为文件形式。通常，这种转换需要在使用现场（on the fly）引入一些特殊处理，使得非Unix文件系统能够兼容Unix文件系统的使用规则并满足VFS的需求。这种文件系统当然仍能工作，但是带来的开销太大了。

### 13.4VFS对象及其数据结构

VFS其实采用的是面向对象的设计思路，使用一组数据结构来代表通用文件对象。这些数据结构类似于对象。因为内核纯粹使用C代码实现，没有直接利用面向对象的语言，所以内核中的数据结构都使用C语言的结构体实现，而这些结构体包含数据的同时也包含操作这些数据的函数指针，其中的操作函数由具体文件系统实现。

VFS中有四个主要的对象类型，他们分别是：

- 超级块对象，他代表一个具体的已安装文件系统。
- 索引节点对象，他代表一个具体文件。
- 目录项对象，他代表一个目录项，是路径的一个组成部分。
- 文件对象，他代表由进程打开的文件。

注意，因为VFS将目录作为一个文件来处理，所以不存在目录对象。回忆本章前面锁提到的目录项代表的是路径中的一个组成部分，他可能包括一个普通文件，换计划瘦，目录项不同于目录，而目录是另一种形式的文件。

每个主要对象都包含一个操作对象，这些操作对象描述了内核针对主要对象可以使用的方法，其实就是操作方法

- super_operations对象，其中包括内核针对**特定文件系统**所能调用的方法，比如write_inode()和sync_fs()等方法。这是写文件节点和同步文件系统函数
- inode_operations对象，其中包括内核针对**特定文件**所能调用的方法，比如create()和link()等方法，这是创建和链接函数（我记得是未打开的文件只有索引节点）
- dentry_operations对象，其中包括内核针对**特定目录**所能调用的方法，比如d_compare()和d_delete()等方法。这是对比和删除函数
- file_operations对象，其中包括进程针对**已打开文件**所能调用的方法，比如read()和write()等方法。

看到最后一个file_operations一下子就熟悉了，这是文件操作函数结构体，需要我们自己实现的，用户层执行对应的函数，最终调用我们自己创建的函数执行。当时就有一种自己写一个系统调用的感觉，后来知道系统调用是由VFS提供，我这里算不算是替换了VFS提供的结构体呢。

操作对象作为一个结构体指针实现，此结构体中包含指向操作其父对象的函数指针。对于其中许多方法来说，可以继承使用VFS提供的通用函数，如果通用函数提供的基本功能无法满足需要，那么就必须使用实际文件系统的独有方法填充这些函数指针，使其指向文件系统实例。也就是我们自己编写的file_operations么

再次提醒，这里说的对象就是结构体。

内核中确实存在很多利用面向对象思想编程的例子VFS就是一个利用C代码有效和简洁地实现OOP的例子。

VFS使用了大量结构体对象，他所包括的对象远远多于上面提到的这几种主要对象。比如每个注册的文件系统都由file_system_type结构体来表示，描述了文件系统及其性能；另外，每个安装点也都用vfsmount结构体表示，它包含的是安装点的相关信息，如位置和安装标志等。

在本章的最后还要介绍两个与进程相关的结构体，他们描述了文件系统以及和进程相关的文件，分别是fs_struct结构体和file结构体。

接下来讨论这些对象及其他们在VFS层的实现中扮演的角色。

### 13.5超级块对象

各种文件系统都必须实现超级块对象，该对象用于存储特定文件系统的信息，通常对应于存放在磁盘特定扇区中的文件系统超级块或文件系统控制块（所以称为超级块对象）。对于并非基于磁盘的文件系统（比如基于内存的文件系统，比如sysfs）,他们会在使用现场创建超级块并将其保存在内存中。

超级块对象由super_block结构体表示，定义在文件<linux/fs.h>中，下面给出他的结构和各个域的描述：（就是文件系统的相关信息，太多了）

不展开介绍了。

创建、管理和撤销超级块对象的代码位于文件fs/super.c中。超级块对象通过**alloc_super**()函数创建并初始化。在文件系统安装时，文件系统会调用该函数以便从磁盘读取文件系统超级块，并且将其**信息填充到内存**中的超级块对象里。

### 13.6超级块操作

超级块对象中最重要的一个域是s_op，它指向超级块的操作函数表。超级块操作函数表由super_operations结构体表示，定义在文件<linux/fs.h>中，其形式如下：

里面全是函数指针，同样不展开说了。

该结构体中的每一项都是一个指向超级块操作函数的指针，超级块操作函数执行**文件系统**和**索引节点**的底层操作。（怎么会涉及到索引节点）

当文件系统需要对其超级块执行操作时，首先要在超级块对象中寻找需要的操作方法。比如如果一个文件系统要写自己的超级块，需要调用：

```c
sb->s_op->write_super(sb);//其中sb是指向文件系统超级块的指针，沿着该指针进入超级块操作函数吧s_op，并从表中取得希望得到的write_super()函数，该函数执行写入超级块的实际操作。注意，尽管write_super()方法来自超级块，但是在调用时，还是要把超级块作为参数传递给他，这是因为C语言中缺少面向对象的支持。也就是说方法的作用域需要通过传参的方式指定
```

由于在C语言中无法直接得到操作函数的父对象，所以**必须将父对象以参数形式传给操作函数**。下面给出super_operation中，超级块操作函数的用法。

```c
struct inode *alloc_inode(struct super_block *sb)//在给定的超级快下创建和初始化一个新的索引结点对象。看起来超级块是包含索引结点的
    void destroy_inode(struct inode *inode)//用于释放给定的索引节点
    void dirty_inode(struct inode *inode)//VFS在索引节点被修改时，会调用此函数。日志文件系统比如ext34执行该函数进行日志更新
    void write_inode(struct inode * inode,int wait)//用于将给定的索引节点写入磁盘wait参数指明写操作是否需要同步。这里的同步指的是执行完返回，还是没执行完就返回。
    void drop_inode(struct inode *inode)//在最后一个指向索引节点的引用被释放后，也就是说索引节点不再被使用了之后，VFS会调用该函数。VFS只需要简单地删除这个索引节点后，普通Unix文件系统就不会定义这个函数了。最后一句话啥意思
    void delete_inode(struct inode * inode)//用于从磁盘上删除给定的索引节点
    void put_super(Struct super_block *sb)//在卸载文件系统时由VFS调用，用来释放超级块。调用者必须一直保持s_lock锁。这个锁是超级块对象的一个成员。超级块信息量。
    int sync_fs(struct super_block * sb,int wait)//使文件系统的数据元与磁盘上的文件系统同步。wait参数指定操作是否同步。同样是写入磁盘中的
    void write_super_lockfs(struct super_block *sb)//首先禁止对文件系统做改变，再使用给定的超级块更新磁盘上的超级块目前LVM(逻辑卷标管理)会调用该函数，这也是一种同步吧。
    void unlockfs(struct super_block *sb)//对文件系统解除锁定，他是write_super_lockfs()的逆操作。
    int statfs(struct super_block * sb,struct statfs *statfs)//VFS通过该函数获取文件系统状态。指定文件系统相关的统计信息将放置在statfs中。
    int remount_fs(struct super_block *sb,int *flags,char*data)//当指定新的安装选项重新安装文件系统时，VFS会调用该函数。调用者必须一直保持s_lock锁。
    void clear_inode(struct inode *inode)//VFS调用该函数释放索引节点，并清空包含相关数据的所有页面。
    void umount_begin(struct super_block *sb)//VFS调用该函数中断安装操作。该函数被网络文件系统使用，比如NFS。
```

所有以上的函数都是VFS在进程上下文中调用。除了dirty_inode()，其他函数在必要时都可以阻塞。

这其中的一些函数是可选的。在超级块操作表中，文件系统可以将不需要的函数指针设置成NULL。如果VFS发现操作函数指针是NULL，那它要么就会调用通用函数执行相应操作，要么什么也不做，如何选择取决于具体操作。（这里可以看到vfs会选择使用哪种实现方式）

突然发现文件系统是内核的，而文件系统是根文件系统树上的树枝，可是为啥先有文件系统后有根文件系统呢。

### 13.7索引节点对象

索引节点对象包含了内核在操作系统文件或目录时需要的全部信息。对于Unix风格的文件系统来说，这些信息可以从磁盘索引节点直接入读。如果一个文件系统没有索引节点，那么不管这些相关信息在磁盘上是怎么存放的，文件系统都必须从中提取这些信息。其实这里的索引节点就是存放在磁盘中的inode表吧。应用篇学过。

没有索引节点的文件系统通常将文件的描述信息作为文件的一部分来存放。这些文件系统与Unix风格的文件系统不同没有将数据与控制信息分开存放。有些现代文件系统使用数据库来存储文件的数据。不管哪种情况、采用哪种方式，索引节点对象必须在内存中创建，以便于文件系统使用。

索引节点对象由inode结构体表示，它定义在文件<linux/fs.h>中，下面给出他的结构体和各项的描述。

一个索引节点代表文件系统中的一个文件（但是**索引节点**仅当文件**被访问**时，才在内存中**创建**），他也可以是设备或者管道这种的特殊文件。因此索引节点结构体中有一些和特殊文件相关的项，比如i_pipe项就指向一个代表有名管道的数据结构，i_bdev指向块设备结构体，i_cdev指向字符设备结构体。这三个指针被存放在一个公用体中因为一个给定的索引节点每次只能表示三者之一，这个i_cdev太眼熟了，就是驱动中经常申请的。不申请这个就会导致在sysfs中不创建对应的设备节点。

有时，某些文件系统可能并不能完整地包含索引节点结构体要求的所有信息。举个例子，有的文件系统可能并不记录文件的访问时间，这时，该文件系统就可以在实现中选择任意合适的办法来解决这个问题。他可以在i_atime中存储0，或者由文件系统的实现者来决定。

### 13.8索引节点操作

和超级块操作一样，索引节点对象中的inode_operations项也非常重要，因为它描述了VFS用以操作索引节点对象的所有办法。和超级快累死，对索引节点的操作调用方式如下：

```c
i->i_op->truncate(i);//i指向给定的索引结点，truncate()函数是由索引结点i所在的文件系统定义的。inode_operations结构体定义在文件<linux/fs.h>中
```

下面这些接口由各种函数组成，在给定的节点上，可能由VFS执行这些函数，也可能由具体的文件系统执行(这里已经不再是由抽象层了属于文件系统内部了，而超级块是VFS对文件系统的抽象)：

```c
int create(struct inode *dir,struct dentry *dentry,int mode)//VFS通过系统调用create()和open()来调用该函数，从而为dentry对象创建一个新的索引节点。这里是为目录项创建一个索引节点。在创建时使用mode指定的初始模式。
    struct dentry *lookup(struct inode *dir,struct dentry * dentry)//该函数在特定的目录中寻找索引节点，该索引节点要对应于dentry中给出的文件名。其实inode就是文件的控制信息，而dentry表示的是目录项
        int link(struct dentry *old_dentry,struct inode *dir,struct dentry *dentry)//该函数被系统调用link()调用，用来创建硬链接，硬链接名称由dentry参数指定，连接的对象是dir目录中old_dentry目录项所代表的文件。
        int unlink(struct inode * dir,struct dentry *dentry)//该函数被系统调用unlink()调用，从目录dir中删除由目录项dentry指定的索引节点对象。
        int symlink(struct inode *dir,struct dentry*dentry,const cha *symname)//该函数被系统调用symlink调用，创建符号连接，该符号链接由symname指定，连接对象是dir目录中的dentry目录项。软连接
        int mkdir(struct inode *dir,struct dentry *dentry,int mode)//该函数被系统调用mkdir调用，创建一个新目录。创建是使用mode指定的初始模式，这个太眼熟了，终端的命令和系统调用有关，而内核下的系统调用是文件系统提供的。
        int mknod(struct inode *dir,struct dentry *dentry,int mode,dev_t rdev)//该函数被系统调用mknod()调用，创建特殊文件。要创建的文件存放在dir目录中，其目录项为dentry，关联的设备为rdev，初始权限为mode
        int rmdir(struct inode *dir,struct dentry *dentry)//该函数被系统调用rmdir调用，删除dir目录中的dentry目录项代表的文件。
        int rename(struct inode *old_dir,struct dentry *old_dentry,struct inode *new_dir,struct dentry *new_dentry)//VFS调用该函数来移动文件，文件源路径在old_dir目录中，源文件由old_dentry目录项指定，目标路径在new_dir目录中，目标文件由new_dentry指定。
        int readlink(struct dentry *dentry,char *buffer,int buflen)//该函数被系统调用readlink()调用，拷贝数据到特定的缓冲buffer中。拷贝的数据来自dentry指定的符号链接，拷贝大小最大可达buflen字节。
        int follow_link(struct dentry *dentry,struct nameidata *nd)//该函数由VFS调用，从一个符号链接查找它指向的索引节点。由dentry指向的链接被解析，结果存放在由nd指向的nameidata结构体中。
        int put_link(struct dentry *dentry,struct nameidata *nd)//在follow_link()调用之后，该函数由VFS调用进行清除工作。
        void truncate(struct inode *inode)//该函数由VFS调用，修改文件的大小。在调用前，索引节点的i_size项必须设置为预期的大小。
        int permission(struct inode * inode ,int mask)//该函数用来检查给定的inode所代表的文件是否允许特定的访问模式。如果允许特定的访问模式，就返回0，否则返回负值的错误码。多数文件系统都会将该域设置为NULL，使用VFS提供的通用方法进行检查。这种检查操作仅仅比较索引节点对象中的访问模式位是否和给定的mask一直。比较复杂的系统就需要使用特殊的permission函数了。
        int setattr(struct dentry *dentry,struct iattr *attr)//该函数被notify_change()调用，在修改索引节点后，通知发生了改变事件。
        int getattr(struct vfsmount *mnt.struct dentry *dentry,struct kstat * stat)//在通知索引节点需要从磁盘中更新时，VFS会调用该函数。扩展属性允许key/value这样的一对值与文件相关联。
        int setxattr(struct dentry *dentry,const char *name,const void *value,size_t size,int flags)//该函数由VFS调用，给dentry指定的文件设置扩展属性。属性名为name，值为value。
        ssize_t getxattr(struct dentry *dentry,const char *name,void *value,size_t size)//该函数由VFS调用，向value中拷贝给定文件的扩展属性name对应的数值。
        ssize_t listxattr(struct dentry *dentry,char *list,size_t size)//该函数将特定文件的所有属性列表拷贝到一个缓冲列表中。
        int removexattr(struct dentry *dentry,const char *name)//该函数从给定文件中删除指定的属性。
```

上面的API我已经不想去写了，索引节点对象可以理解为描述文件的控制信息，而目录项指的就是目录了

### 13.9目录项对象

VFS把目录当做文件对待，所以在路径/bin/vi中，bin和vi都属于文件，bin是特殊的目录文件，而vi是一个普通文件，路径中的每个组成部分都由一个索引节点对象表示，虽然他们可以统一由索引节点表示，但是CFS经常需要执行目录相关的操作，比如路径名查找等。路径名查找需要解析路径中的每一个组成部分，不但要确保它有效，而且还需要再进一步寻找路径中的下一个部分。

为了方便查找操作，VFS引入了目录项的概念，**每个dentry代表路径中的一个特定部分**，对前一个例子来说，/、bin和vi都属于目录项对象，前两个是目录，最后一个是普通文件。必须明确一点：**在路径中包括普通文件在内，每一个部分都是目录项对象。**这句话把dentry讲透了。解析一个路径并编译其分量绝非简单的演练，他是耗时的、常规的字符串比较过程，执行耗时、代码繁琐。目录项对像的引入使得这个过程更简单了。

目录项也可包括安装点，目录项对象由dentry结构体表示，定义在文件<linux/dcache.h>中。下面给出该结构体和其中各项的描述：不展开了，与前面的两个对象不同，**目录项并没有对应的磁盘数据结构**，前两个是可以在执行前保存到磁盘中的，**VFS根据字符串形式的路径名现场创建它**。而且由于目录项对象并非真正保存在磁盘上，所以目录项结构体没有是否被修改的标志。

#### 13.9.1目录项状态

目录项对象有三种有效状态：被使用、未被使用和负状态。

一个被使用的目录项对应一个有效的索引节点（这句话把dentry和inode关系说透了，d_inode指向相应的索引节点）并且表明该对象存在一个或者多个使用者（即d_count为正值）。一个目录项处于被使用状态，因为这个他正被VFS使用并且指向有效的数据，因此不能丢失。

一个未被使用的目录项对应一个有效的索引节点（d_inode指向一个索引节点），但是应指明VFS当前并未使用它（d_count为0）。该目录项对象仍然指向一个有效对象，并且被保留在缓存中以便需要时再使用它。由于该目录项不会过早地被撤销，所以以后再需要它时，不必重新创建，与未缓存的目录项相比，这种会使得路径查找更迅速。如果要回收内存的话，可以撤销未使用的目录项。

一个负状态的目录项没有对应的有效索引节点（d_inode为NULL），因为索引节点已被删除了，或路径不再正确了，但是目录项仍然保留，以便快速解析以后的路径查询。比如，一个守护进程不断地试图打开并读取一个不存在的配置文件。open()系统调用不断地返回ENOENT，直到内核构建了这个路径、遍历磁盘上的目录结构体并检查这个文件的确不存在为止。即便这个失败的查找很浪费资源，但是将负状态缓存起来还是非常值得的。但是用的很少。

目录项对象释放后也可以保存到slab对象缓存中，此时，任何VFS或文件系统代码都没有指向该目录项对象的有效索引。为啥可以保存到slab中去，那个是空闲链表分配器。

#### 13.9.2目录项缓存

如果VFS层遍历路径名中所有的元素并将他们逐个地解析成目录项对象，还要到达最深层目录，将是一件非常费力的工作，会浪费大量的时间。所以内核将目录项对象缓存在目录项缓存中（dcache）.

目录项缓存包括三个主要部分：

- 被使用的目录项链表。概览表通过索引节点对象中的i_dentry项连接相关的索引节点，因为一个给定的索引节点可能有多个链接，所以就可能有多个目录项对象，因此用一个链表来连接他们。
- 最近被使用的双向链表。该链表含有未被使用的和负状态的目录项对象。由于该链总是在头部插入目录项，所以链头节点的数据总比链尾的数据要新。当内核必须通过删除节点项回收内存时，会从链尾删除节点项，因为尾部的节点最旧，所以他们在近期内再次被使用的可能性最小。
- 散列表和相应的散列函数**用来快速地将给定路径解析为相关目录项对象**。（散列表就是用来将给定路径解析为相关目录项对象存入dcache中的呗）

散列表由数组dentry_hashtable表示，其中每一个元素都是一个指向具有相同键值的目录项对象链表的指针。数组的大小取决于系统中物理内存的大小。

实际的散列表由d_lookup()函数计算，他是内核提供给文件系统的唯一的一个散列函数。查找散列表要通过d_lookup()函数，如果该函数在dcache中发现了与其相匹配的目录项对象，则匹配的对象被返回；否则，返回NULL指针。

举例说明，假设需要在自己目录中编译一个源文件，/home/dracula/src/the_sun_sucks.c，每一次对文件进行访问，VFS都必须沿着嵌套的目录依次解析全部路径：/、home、dracula、src和最终的the_sun_sucks.c。为了避免每次访问该路径名都进行这种耗时的操作，VFS会先在目录项缓存中搜索路径名，如果找到了，就无须话费那么大的力气了。相反，如果该目录项在目录项缓存中并不存在，VFS就必须自己通过遍历文件系统为每个路径分量解析路径，解析完毕后，再将目录项对象加入dcache中，以便以后可以快速查找到他。

而dcache在一定意义上也提供对索引节点的缓存，也就是icache。和目录项对象相关的索引节点对象不会被释放，因为目录项会让相关索引节点的使用计数为正，这样就可以确保索引节点留在内存中。只要目录项被缓存，其相应的索引节点也就被缓存了。所以像前面的例子，只要路径名在缓存中被找到了，相应的索引节点肯定也在内存中缓存着。之前学索引节点的时候有一个API用来释放索引节点的，就是依靠使用计数判断当前的索引节点是否被使用。

因为文件访问呈现时间和空间的局限性（这个指的就是在时间上最近被访问的文件大概率还要被访问，空间上被访问的文件周围其他文件大概率被访问)。所以对目录项和索引节点进行缓存非常有益。文件访问有时间上的局部性，是因为程序可能会一次又一次地访问相同文件。因此，当一个文件被访问的时候，所缓存的相关目录项和索引节点不久被命中的概率较高。文件访问具有空间的局部性是因为程序可能在同一个目录下访问多个文件，因此一个文件对应的目录项缓存后极有可能被命中，因为相关的文件可能在下次又被使用。

### 13.10目录项操作

dentry_operation结构体指明了VFS操作目录项的所有方法。

该结构定义在文件<linux/dcache.h>中。

下面给出函数的具体用法：

```c
int d_revalidate(struct dentry *dentry,struct nameidata *);//该函数判断目录对象是否有效。VFS准备从dcache中使用一个目录项时，会调用该函数，大部分文件系统将该方法置NULL，因为他们认为dcache中的目录项对象总是有效的。
int d_hash(struct dentry *dentry,struct qstr *name);//该函数为目录项生成散列值，当目录项需要加入到散列表中时，VFS调用该函数。
int d_compare(struct dentry *dentry,struct qstr *name1,struct qstr *name2);//VFS调用该函数来比较name1和name2这两个文件名。多数文件系统使用VFS默认的操作，仅仅作字符串比较。对有些文件系统，比如FAT，简单的字符串比较不能满足其需求。因为FAT文件系统不区分大小写，所以需要实现一种不区分大小写的字符串比较函数。注意使用该函数时需要加dcache_lock锁。
int d_delete(struct dentry *dentry);//当目录项对象的d_count计数值等于0时，VFS调用该函数。注意使用该函数需要加dcache_lock锁和目录项的d_lock。
void d_release(struct dentry * dentry);//当目录项对象将要被释放时，VFS调用该函数，默认情况下，什么都不做。释放和删除有什么区别？
void d_iput(struct dentry *dentry,struct inode *inode);//当一个目录项对象丢失了其相关的索引节点时（也就是说磁盘索引节点被删除了，因为当有目录项对象在dcache中的时候，相应的索引节点会保存在缓存中，所以只能是磁盘中的索引节点被删除），VFS调用该函数，默认情况下VFS会调用iput()函数释放索引节点。如果文件系统重载了该函数，那么除了执行此文件系统特殊的工作外，还必须调用iput()函数。
```

### 13.11文件对象

VFS的最后一个主要对象是文件对象。文件对象表示**进程已打开的文件**。如果我们站在用户角度来看到VFS，文件对象会首先进入我们的视野。**进程直接处理的是文件**，而不是超级块、索引节点或目录项。所以不必奇怪：文件对象包含我们非常熟悉的信息（访问模式、当前偏移），同样道理，文件操作和我们非常熟悉的系统调用read()和write()等也很类似。

文件对象是已打开的文件在内存中的表示。该对象由相应的open()系统调用创建。由close()系统调用撤销，所有这些文件相关的调用实际上都是文件操作表中定义的方法。因为多个进程可以同时打开和操作同一个文件，所以通过一个文件也可能存在多个对应的文件对象。之前的目录项对象其实指的是路径上的每个对象，通过散列表进行解析存入dcache中。

文件对象仅仅在进程观点上代表已打开文件，反过来指向目录项对象（同样反过来指向索引节点），其实只有目录项对象才表示已打开的实际文件。虽然一个文件对应的文件对象不是唯一的，但对应的索引节点和目录项对象无疑是唯一的。目录项对象是如何生成的，是当文件被打开后才会生成目录项对象并存入缓存中。之前提到的dcache和icache分别存入的目录项对象和索引节点对象。之前学过类似的概念存入的是数据缓存和代码缓存，难道是之前学错了吗。

文件对象由file结构体表示，定义在文件<linux/fs.h>中，下面给出该结构体和各项的描述。

总结下，超级块对象表示特定的文件系统，索引节点对象表示具体文件的控制信息文件，目录项对象表示文件在路径上的解析。最后一个文件对象表示已打开文件在内存上的表示。

类似于目录项对象，文件对象实际上没有对应的磁盘数据。所以在结构体中没有代表其对象是否为脏、是否需要写回磁盘的标志。文件对象通过f_dentry指针指向相关的目录项对象。目录项会指向相关的索引节点，索引节点会记录文件是否是脏的。文件在磁盘中肯定有对应的数据，但是文件对象是已打开文件在内存上的表现自然是没有了。

---

open函数触发软中断陷入内核执行sys_open函数，此时还没有创建file结构体，故访问inode节点，此节点内有设备号以及i_cdev结构体，根据设备号确定为字符设备，访问i_cdev结构体中的cdev结构体，这是驱动层定义的代表字符设备的结构体，cdev->file_operations。如果是write函数的话应该是直接走file->file_operations结构体了。如果没有定义file_operations结构体，直接走inode->file_operations这是默认的ops

---

### 13.12文件操作

和VFS的其他对象一样，文件操作表在文件对象中也非常重要。跟file结构体相关的操作与系统调用很类似，这些操作是标准Unix系统调用的基础。

文件对象的操作由file_operations结构体表示，定义在文件<linux/fs.h>中。终于是讲到这个file_operations了。具体的文件系统可以为每一种操作做专门的实现，或者如果存在通用操作，也可以使用通用操作。一般在基于Unix的文件系统上，这些通用操作效果都不错。并不要求实际文件系统实现文件操作函数表中的所有方法，确实我们自己编写的也只是实现其中的一两种而已。所以我们编写的这些函数同样是系统调用，只不过不使用通用的通用的系统调用而是专门设计的。

下面给出操作的用法说明：

```c
loff_t lleek(struct file *file,loff_t offset,int origin)//该函数用于更新偏移量指针，由系统调用lleek()调用它。
    ssize_t aio_read(struct kiocb *iocb,char *buf,size_t count,loff_t offset)//该函数从iocb描述的文件里，以同步方式读取count字节的数据到buf中。由系统调用aio_read()调用它。
    ssize_t read(struct file *file,char *buf,size_t count,loff_t *offset)//该函数从给定文件的offset偏移处读取count字节的数据到buf中，同时更新文件指针，指向当前偏移处。由系统调用read()调用它。
    ssize_t write(struct file *file,const char *buf,size_t count,loff_t *offset)//该函数从给定的buf中取出count字节的数据，写入到给定文件的offset偏移处，同时更新文件指针，由系统调用write()调用它。
    ssize_t aio_write(struct kiocb *iocb,const cahr *buf,size_t count,loff_t offset)//该函数以同步方式从给定的buf中取出count字节的数据，写入由iocb描述的文件中，由系统调用aio_write()调用它。这里的kiocb结构体类型是什么，同步是写入磁盘再返回吧。
    int readdir(struct file *file,void *dirent,filldir_t filldir)//该函数返回目录列表中的下一个目录。由系统调用readdir()调用它。
    unsigned int poll(struct file *file,struct poll_table_struct *poll_table)//该函数睡眠等待给定文件活动。由系统调用poll()调用它。这是监控函数，监控文件描述符中代表的文件资源
    int ioctl(struct inode *inode,struct file*file,unsigned int cmd,unsigned long arg)//该函数用来对设备发送命令参数对。当文件是一个被打开的设备节点时，可以通过他进行设置操作。由系统调用ioctl()调用它。调用者必须持有BKL。这里的BKL不知道是什么，此函数就是一个针对文件的工具箱。
    int unlocked_ioctl(struct file *file,unsigned int cmd,unsigned long arg)//实现与ioctl有类似的功能，只不过不需要调用者持有BKL。如果用户空间调用ioctl()系统调用，VFS便可以调用unlocked_ioctl(0)因此文件系统只需要实现这两个其中的一个，一般优先实现unlock_ioctl()。这里的BLK指的是大内核锁么
    int compat_ioctl(struct file *file,unsigned int cmd,unsigned long arg)//该函数是ioctl()函数的可移植变种，被32位应用程序用在64位系统上。这个函数被设计成即使在64位的体系结构上对32位也是安全的。可以进行必要的字大小转换。新的驱动程序应该设计自己的ioctl命令以便所有的驱动程序都是可移植的，从而使得compat_ioctl()和unlocked_ioctl()指向同一个函数。同样不必持有BLK
    int mmap(struct file *file,struct vm_area_struct *vma);//该函数将给定的文件映射到指定的地址空间上。由系统调用mmap()调用。这个mmap好像是共享内存所使用的的函数吧。有一个kmap是内核中使用的，用于将page物理地址永久的映射到逻辑地址上
int open(struct inode *inode,struct file *file)//该函数创建一个新的文件对象，并将它和相应的索引节点对象关联起来。由系统调用open()调用它。
    int flush(struct file *file)//当已打开文件的引用计数减少时，该函数被VFS调用。他的作用根据具体文件系统而定。糊涂了 ，这个函数干啥用的
    int release(struct inode *inode,struct file *file)//当文件的最后一个引用被注销到时候（比如，当最后一个共享文件描述符的进程调用了close()或者退出的时候也会自动关闭所打开的文件）该函数会被VFS调用。作用根据文件系统而定。
    int fsync(struct file *file,struct dentry *dentry,int datasync)//将给定文件的所有被缓存数据写回磁盘。由系统调用fsync()调用它。这个好像是刷新函数
    int aio_fsync(struct kiocb *iocb,int datasync)//将iocb描述的文件的所有被缓存数据写回到磁盘。由系统调用aio_fsync()调用它。
    int lock(struct file *file ,int cmd ,struct file_lock*lock)//用于给指定文件上锁，这里指的就是文件锁吧，
    ssize_t readv(struct  file *file ,const struct iovec *vector,unsigned long count,loff_t *offset)//该函数从给定文件中读取数据，并将其写入由vector描述的count个缓存中去。同时增加文件的偏移量。由系统调用readv()调用它。
    ssize_t writev(struct file *file,const struct iovec *vector,unsigned long count,loff_t *offset)//该函数将由vector描述的count个缓冲中的数据写入到file指定的文件中，同时减小文件的偏移量，由系统调用writev()调用它。
    ssize_t sendfile(struct file *file.loff_t *offset,size_t size,read_actor_t actor,void *target)//函数用于从一个 文件拷贝数据到另一个文件中，他执行的拷贝操作完全在内核中完成，避免了向用户空间进行不必要的拷贝。由系统调用sendfile()调用它。
    ssize_t sendpage(struct file *file,struct page *page,int offset,size_t size,loff_t *pos,int more)//该函数用来从一个文件向另一个文件发送数据。
    unsigned long get_unmapped_area(struct file *file ,unsigned long addr,unsigned long len,unsigned long offset,unsigned long flags)//该函数用于获取未使用的地址空间来映射给定的文件。
    int check_flags(int flags)//当给出SETFL命令时，这个函数用来检查传递给fcntl()系统调用的flags的有效性。和大多数VFS操作一样，文件系统不必实现check_flags()，目前，只有在NFS文件系统上实现了。这个函数能使文件系统限制无效的SETFL标志，不进行限制的话，普通的fcntl()函数能使标志生效。在NFS文件系统中，不允许把O_APPEND和O_DIRECT相结合。
    int flock(struct file *filp,int cmd,struct file_lock*fl)；//函数用来实现flock()系统调用，该调用提供忠告锁。
```

----

对于驱动模块中没有初始化的file_operation结构体的其余部分，默认为NULL，VFS会自动调用通用操作函数。但这里只是介绍了VFS与实际文件系统的操作函数之间的关系，并没有介绍系统调用到vfs的过程。

----

##### 如此之多的ioctls

不久之前，只有一个单独的ioctl方法。如今，有三个相关的方法。unlocked_ioctl()和ioctl相同，而前者在没有大内核锁BKL情况下被调用。因此函数的作者必须确保适当的同步。因为大内核锁是粗粒度、低效的锁，驱动程序应当实现unlocked_ioctl()而不是ioctl()。

compat_ioctl()也在无大内核锁的情况下被调用，但他的目的是**为64位系统提供32位ioctl的兼容方法**。至于你如何实现它取决于现有的ioctl命令，早期的驱动程序隐含有确定大小的类型，应该实现适用于32位应用的compat_ioctl()方法。这通常意味着把32位值转换为64位内核中合适的类型。新驱动程序重新设计ioctl命令，应该确保所有的参数和数据都有明确大小的数据类型，在32位系统上运行32位应用是安全的，在64位系统上运行64位应用更安全。这些驱动程序可以让compat_ioctl()函数指针和unlocked_ioctl()函数指针指向同一函数。

### 13.13和文件系统相关的数据结构

除了以上几种VFS基础对象外，内核还使用了另外一些标准数据结构来管理文件系统的其他相关数据。第一个对象是file_system_type，用来描述各种特定文件系统类型，比如ext3/4或UDF。第二个结构体是vfsmount，用来描述一个安装文件系统的实例。

因为Linux支持众多不同的文件系统，所以内核必须由一个特殊的结构来描述每种每种文件系统的功能和行为。我记得超级块不就是描述特定文件系统的吗file_system_type结构体被定义在<linux/fs.h>中，具体实现如下：

不展开介绍了。

get_sb()函数从磁盘上读取超级块，并且在文件系统被安装时，在内存中组装超级块对象。剩余的函数描述文件系统的属性。看起来这个结构体是用来创建超级块的结构体。

每种文件系统，不管有多少个实例安装在系统中，还是根本就没有安装到系统中，都只有一个file_system_types结构。

更有趣的事情是，当文件系统被实际安装时，将有一个vfsmount结构体在安装点被创建。该结构体用来代表文件系统的实例，换句话说代表一个安装点。之前说过安装点就是命名空间。

vfsmount结构被定义在<linux/mount.h>中。

不展开了。

理清文件系统和所有其他安装点之间的关系，是维护所有安装点链表中最复杂的工作。所以vfsmount结构体中维护的各种链表就是为了能够跟踪这些关联信息。

vfsmount结构还保存了在安装时指定的标志信息，该信息存储在mnt_flages域中。这里的安装指的就是由file_system_type结构体中的方法来安装文件系统从而组成超级块。

安装那些管理员不充分信任的移动设备时，这些标志很有用处。他们和其他一些很少用的标志一起定义在<linux/mount.h>中。

### 13.14和进程相关的数据结构

系统中每个进程都有自己的一组打开文件，像根文件系统、当前工作目录、安装点等。有三个数据结构将VFS层和系统的进程紧密联系在一起，分别是file_struct 、fs_struct 和namespace结构体。

file_struct结构体定义在文件<linux/fdtable.h>中。该结构体由**进程描述符中的files目录项指向**。所有与单个进程相关的信息都包含在其中，其结构和描述如下：

fd_array数组指针指**向已打开的文件对象**。因为NR_OPEN_DEFAULT等于BITS_PER_LONG，在64位机器体系结构中这个宏的值为64，所以该数组可以容纳64个文件对象。如果一个进程所打开的文件对象超过64个，内核将分配一个新数组，并且将fdt指针指向它，所以对适当数量的文件对象的访问会执行的很快，因为他是静态数组进行的操作；如果一个进程打开的文件数量过多，那么内核就需要建立新数组。如果系统中有大量的进程都要打开超过64个文件，为了优化性能，管理员可以适当增大NR_OPEN_DEFAULT的预定义值。

和进程相关的第二个结构体是fs_struct。该结构由进程描述符的fs域指向。它包含文件系统和进程相关的信息，定义在文件<linux/fs_struct.h>中，下面是他的具体结构体和各项描述：

**该结构包含了当前进程的当前工作目录pwd和根目录。**

第三个相关结构体 namespace结构体，他定义在文件<linux/mmt_namespace.h>中，由进程描述符中的mmt_namespace域指向。2.4版内核以后，单进程命名空间被加入到内核中，它使得每一个进程在系统中都看到唯一的安装文件系统。不仅是**唯一的根目录**，而且是**唯一的文件系统**层次结构。下面是其具体结构和描述。不展开了

list域是连接已安装文件系统的双向链表，它包含的元素组成了全体命名空间。上述这些数据结构都是通过进程描述符连接起来的。对多数进程来说，他们的描述符都指向**唯一的files_struct和fs_struct结构体**。但是，对于那些使用**克隆标志CLONE_FILES或CLONE_FS创建的进程**（），会共享这两个结构体。所以多个进程描述符可能指向同一个files_struct或fs_struct结构体。每个结构体都维护一个count域作为引用计数，它防止在进程正在使用该结构体时，被撤销。之前应用篇的时候只是用思维导图的方式介绍了下系统调用深层的东西。这回是落实到代码层了。

使用clone函数的时候可以克隆出另一个文件描述符他们都指向同一个文件描述符结构体，就是体现在这里的，已打开的文件数组相同，当前工作目录和根目录相同，（线程共享进程的fs和files，fork创建的进程只作为子进程、vfork共享进程地址空间）

namespace结构体的使用方法和前两种结构体完全不同，默认情况下，所有的进程共享同样的命名空间（他们都从相同的挂载表中看到同一个文件系统层次结构）。只有在进行clone()操作时使用CLONE_NEWS标志，才会给进程一个唯一的命名空间结构体的拷贝。因为大多数进程不提供这个标志，所有进程都继承其父进程的命名空间。因此在大多数系统上只有一个命名空间。之前接触过进程命名空间，防止子侄辈进程访问。因为进程的命名空间不同，现在看来就是安装点不同，但是这是谁的安装点呢。（大多数进程共享同一个命名空间）

不过一般是同一个进程中的不同线程共享这两个结构体，不同进程之间拥有各自的命名空间。

### 13.15小结

Linux支持了相当多种类的文件系统。从本地文件系统ext3/4到网络文件系统NFS，Linux在标准内核中以支持的文件系统超过60种。VFS层提供给这些不同文件系统一个统一的实现框架，而且也提供了能和标准系统调用交互工作的统一接口。由于VFS层的存在，使得在Linux上实现新文件系统的工作变得简单起来，他可以轻松使这些文件系统通过标准Unix系统调用而协同工作。

本章描述了VFS的目的，讨论了各种数据结构，包括最重要的索引节点、目标项以及超级块对象。第十四章将讨论数据如何从物理上存放在文件系统中。file_system_type中的方法实现文件系统从磁盘安装到内存中生成超级块。**超级块是对特定文件系统的描述**，**索引节点对象是对文件的控制信息的文件描述**，**目录项是对路径上 成员的解析**，**文件对象是已打开文件在内存上的描述**。

## 第十四章块I/O层

系统中能够随机访问固定大小数据片的硬件设备称为块设备，这些固定大小的数据片称为块。最常见的块设备是硬盘，除此之外，还有软盘驱动器、蓝光光驱和闪存等许多其他块设备。注意他们都是以安装文件系统的方式使用的，（偏偏我就不理解安装文件系统的方式是个啥方式），这也是块设备的一般访问方式。硬盘能做到随机访问么，闪存能做到呢，至于所谓的安装文件系统的方式指的是不是从硬盘中把文件系统加载到内存上从而生成超级块这个方式。

另一种基本的设备类型是字符设备。字符设备按照字符流的方式被有序访问，像串口和键盘就属于字符设备。如果一个硬盘设备是以字符流的方式被访问的话，那就应该将它归于字符设备；反过来，如果一个设备是随机访问的就属于块设备。

对于这两种类型的设备，他们的区别在于是否可以随机访问数据，换句话说，就是能否在访问设备时随意地从一个位置跳转到另一个位置。举个例子，键盘何种设备提供的就是一个数据流，当你输入wolf这个字符串时，键盘驱动程序会按照和输入完全相同的顺序返回这个由四个字符组成的数据流。如果让键盘驱动程序打乱顺序来读字符串，或读取其他字符，都是没有意义的。所以键盘就是一种典型的字符设备，他提供的就是用户从键盘输入的字符流。对键盘进行读操作会得到一个字符流，当媒人敲击键盘的时候，字符流就是空的。硬盘设备的情况就不太相同了。硬盘设备的驱动可能要求读取磁盘上任意块的内容，然后又转去读取别的块的内容，而被读取的块在磁盘上位置不一定要连续。所以说硬盘的数据可以被随机访问，而不是以流的方式被访问。这里的随机访问指的并不是随机访问的能力，而是随机访问是存在的这种情况。

内核管理块设备要比管理字符设备细致得多，需要考虑的问题和完成的工作相对于字符设备来说要复杂许多。这是因为字符设备仅仅需要控制一个位置，就是当前位置，而块设备访问的位置必须能够在介质的不同区间前后移动。所以事实上内核不必提供一个专门的子系统来管理字符设备，那驱动篇学的那么多子系统都是个啥

但是对快设备的管理却必须要有一个专门的提供服务的子系统。不仅仅是因为块设备的复杂性远远高于字符设备，更重要的原因是块设备对执行性能的要求很高；对硬盘每多一分利用都会给整个系统的性能带来提升，其效果要远远比键盘吞吐速度成倍的提高大得多。另外会看到块设备的复杂性会为这种优化留下很大的施展空间。这一章的主题就是讨论内核如何对块设备和块设备的请求进行管理。该部分在内核中称为块IO层。

### 14.1剖析一个块设备

块设备中最小的可寻址单元是扇区。扇区大小一般是2的整数倍，而最常见的是512字节。扇区的大小是设备的物理属性，扇区是所有块设备的基本单元，块设备无法对比它还小的单元进行寻址和操作，尽管许多块设备能够一次对多个扇区进行操作。虽然大多数块设备的扇区大小都是512字节，不过其他大小的扇区也很常见。比如，很多CD-ROM盘的扇区都是2KB大小。

因为各种软件的用途不同，所以他们都会用到自己的最小逻辑可寻址单元，块。块是文件系统的一种抽象。只能基于块来访问文件系统。虽然**物理磁盘寻址是按照扇区级进行的**，**但是内核执行的所有磁盘操作都是按照块进行的**。由于扇区是设备的最小可寻址单元，所以块不能比扇区小，只能数倍于扇区大小。如果比扇区小了就找不到了，物理上能找到的最小单位就是扇区了，而内核执行的操作是基于物理的，所以只能比扇区大。

一般块的大小是512字节，1KB或者4KB。

扇区和块还有一些不同的叫法，为了不引起混淆，我们在这里简要介绍一下他们的其他名称。扇区：设备的最小寻址单元，有时会称作应删去或设备块。同样的，块：文件系统的最小寻址单元，有时会成为文件块或I/O块。下图是扇区和缓冲区之间的关系图。

<img src="C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1671430928297.png" alt="1671430928297" style="zoom:25%;" />

### 14.2缓冲区和缓冲区头

当一个块被调入内存时（也就是读入后或者等待写出时），他要存储在一个缓冲区中。每个缓冲区与一个块对应，他相当于是磁盘块在内存中的表示。前面提过，块包含一个或多个扇区，但大小不能超过一个页面，所以一个页可以容纳一个或多个内存中的块。由于内核在处理数据时需要一些相关的控制信息（比如块属于哪一个块设备，块对应于那个缓冲区等），所以每一个缓冲区都有一个对应的描述符。该描述符用buffer_head结构体表示，称作缓冲区头，在文件<linux/buffer_head.h>中定义，它包含了内核操作缓冲区所需要的全部信息。

下面给出缓冲区头结构体和其中各个域的说明：

不展开了。

b_state域表示缓冲区的状态，可以是一种标志或者多种标志的组合。合法的标志存放在bh_state_bits枚举中，该枚举在<linux/buffer_head.h>中定义。

bh_state_bits列表还包含了一个特殊标志-BH_PrivateStart，该标志不是可用状态标志，使用它是为了指明可被其他代码使用的起始位。块IO层不会使用BH_PrivateStart或更高的位。那么某个驱动程序希望通过b_state域存储信息时就可以安全地使用这些位。驱动程序可以在这些位中定义自己的状态标志。

b_count域表示缓冲区的使用计数，可通过两个定义在文件<linux/buffer_head.h>中的内联函数对此域进行增减。其实也就是物理块的使用计数

```c
static inline void get_bh(struct buffer_head*bh)
{
    atomic_inc(&bh->b_count);
}
static inline void put_bh(struct buffer_head *bh)
{
    atomic_dec(&bh->b_count);
}
```

在操作缓冲区头之前，应该先试用get_bh()函数增加缓冲区头的引用计数，确保该缓冲区头不会再被分配出去；当完成对缓冲区头的操作后，还必须使用put_bh()函数减少引用计数。

与缓冲区对应的磁盘物理块由b_blocknr-th域索引，该值是b_bdev域指明的块设备中的逻辑块号。

与缓冲区对应的内存物理页由b_page域表示，另外，b_data域直接指向相应的块，（它位于b_page域所指明的页面中的某个位置上），块的大小由b_size域表示，所以块在内存中的起始地址是在b_data处，结束位置在b_data + b_size处。

**缓冲区头的目的在于描述磁盘块和物理内存缓冲区之间的映射关系**。这个结构体在内核中只扮演一个描述符的角色，说明从缓冲区到块的映射关系。

在2.6内核以前，缓冲区头的作用比现在还要重要。因为缓冲区头作为内核中的IO操作单元，不仅仅描述了从磁盘块到物理内存的映射，而且还是所有块IO操作的容器。可是，将缓冲区头作为IO操作单元带来了两个弊端。

首先，缓冲区头是一个很大且不易控制的数据结构体，而且缓冲区头对数据的操作既不方便也不清晰。对内核来说，它更倾向于操作页面结构，因为页面操作起来更为简便，同时效率还高。**使用一个巨大的缓冲区头表示每一个独立的缓冲区效率低下**。所以在2.6版本中，许多IO操作都是通过内核直接对页面或地址空间进行操作来完成的，不再使用缓冲区头了。具体情况请参考address_space结构体和pdflush等守护进程部分。

缓冲区头带来的第二个弊端是：**它仅能描述单个缓冲区**，当作为所有IO的容器使用时，缓冲区头会促使内核把对大块数据的IO操作分解为对多个buffer_head结构体进行操作。这么做必然会造成不必要的负担和空间浪费。所以2.5开发版内核的主要目标就是为块IO操作引入一种新型、灵活并且轻量级的容器，也就是bio结构体。

### 14.3bio结构体

目前内核中块IO操作的基本容器由bio结构体表示，他定义在文件<linux/bio.h>中。该结构体代表了正在现场的（活动的）以片段链表形式组织的块IO操作。一个片段是一小段连续的内存缓冲区。这样的话，就不需要保证单个缓冲区一定要连续。所以通过**用片段来描述缓冲区**，即使一个缓冲区分散在内核的多个位置上，bio结构体也能对内核保证IO操作的执行。像这样的向量IO就是所谓的聚散IO。

bio结构体定义于<linux/bio.h>中，下面给出bio结构体和各个域的描述。

使用bio的**目的主要是代表正在现场执行的IO操作**，所以该结构体中的主要域都是用来管理相关信息的，其中最重要的几个域是bi_io_vecs、bi_vcnt和bi_idx。

下图显示了bio结构体及其他结构体之间的关系。

<img src="C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1671448054033.png" alt="1671448054033" style="zoom:25%;" />

可以看出来，这里面的bio_vec表示链表，bi_io_idx表示当前链表节点的索引，bi_io_vcnt表示链表节点的总数，可以看出来，每一个节点代表着一个页面。所以bio结构体代表着分散在内核多处的缓冲区。

#### 14.3.1IO向量

bi_io_vec域指向一个bio_vec结构体数组，该结构体链表包含了一个特定IO操作所需要使用到的所有片段，这明显是一个链表怎么说是数组了呢，难道是静态链表吗，还有索引

每个bio_vec结构都是一个形式为<page,offset,len>的向量，他描述的是一个特定的片段：片段所在的物理页、块在物理页中的偏移位置、从给定偏移量开始的块长度。就是这么的来代表分散的片段的。整个bio_io_vec结构体数组表示了一个完整的缓冲区。bio_vec结构定义在<linux/bio.h>文件中：

```c
struct bio_vec{
	struct page *bv_page;
    unsigned int bv_len;
    unsigned int bv_offset;
};
```

在每个给定的块IO操作中，bi_vcnt域用来描述bi_io_vec所指向的bio_vec数组中的向量数目。当块IO操作执行完毕后，bi_idx域指向数组的当前索引。

总而言之，每一个块IO请求都通过一个bio结构体表示。每个请求包含一个或多个块，这些快存储在bio_vec结构体数组中。这些结构体描述了每个片段在物理页中的实际位置，并且像向量一样被组织起来。IO操作的第一个片段由bio_vec结构体所指向，其他的片段在其后依次放置，共有bi_vcnt个片段。当块IO层开始执行请求、需要使用各个片段时，bi_idx域会不断更新，从而总指向当前片段。

bi_idx域指向数组中的当前bio_vec片段，块IO层通过它可以跟踪块IO操作的完成进度。但该域更重要的作用在于分割bio结构体。像冗余廉价磁盘阵列（RAID，出于提高性能和可靠性的目的，将单个磁盘的卷扩展到多个磁盘上）这样的驱动器可以把单独的bio结构体分割到RAID阵列中 的各个硬盘上去。RAID设备驱动只需要拷贝这个bio结构体，再把bi_idx域设置为每个独立硬盘操作时需要的位置就可以了。

bicnt域记录bio结构体的使用计数，如果该域值减为0，就应该撤销该bio结构体，并释放它占用的内存。通过下面两个函数管理使用计数。

```c
void bio_get(struct bio *bio)
    void bio_put(struct bio *bio)
```

前者增加使用计数，后者减少使用计数。在操作正在活动的bio结构体时，一定要首先增加他的使用计数。以免在操作过程中该bio结构体被释放；相反在操作完毕后，要减少使用计数。之前缓冲区也是这一出。

最后要说明的是bi_private域，这是一个拥有者的私有域，只有创建了bio结构的拥有者可以读写该域。

#### 14.3.2新老方法对比

缓冲区头和新的bio结构体之间存在明显差别。bio结构体代表的是IO操作，它可以包括内存中的一个或多个页；而另一方面，buffer_head结构体代表的是一个缓冲区，它描述的仅仅是磁盘中的一个块。因为缓冲区头关联的是单独页中的单独磁盘块，所以他可能会引起不必要的分割，将请求按块为单位划分，只能靠以后才能再重新组合。由于bio结构体是轻量级的，描述的块可以不需要连续存储区，并且不需要分割IO操作。

利用bio结构体代替buffer_head结构体还有以下好处：

- bio结构体很容易处理高端内存，因为他处理的是物理页而不是直接指针，高端内存特点是不是永久的内存映射，所以使用指针的话，可能还没有那个东西呢。
- bio结构体既可以代表普通页IO，同时也可以代表直接IO（指那些不通过页高速缓存的IO操作，也就是不需要TLB）。
- bio结构体便于执行分散-集中块IO操作，操作中的数据可取自多个物理页面。
- bio结构体相比缓冲区头属于轻量级的结构体。因为只需要包括块IO操作所需的信息就行了，不用包含与缓冲区本身相关的不必要信息。

但是还是需要缓冲区头这个概念，毕竟他还负责描述磁盘块到页面的映射。bio结构体不包含任何和缓冲区相关的状态信息，仅仅是一个矢量数组，描述一个或多个单独块IO操作的数据片段和相关信息。在当前设置中，当bio结构体描述当前正在使用的IO操作时，buffer_head结构体仍然需要包含缓冲区信息。内核通过这两种结构分别保存各自的信息，可以保证每种结构所包含的信息量尽可能地少。

### 14.4请求队列

块设备将他们挂起的块IO请求保存在请求队列中，该队列由reques_queue结构体表示，定义在文件<linux/blkdev.h>中，包含一个双向请求链表以及相关控制信息。通过内核中像文件系统这样高层的代码将请求加入到队列中。请求队列只要不为空，队列对应的块设备驱动程序就会从队列头获取请求，然后将其送入对应的块设备上。请求队列表中的每一项都是一个单独的请求，由reques结构体表示。

队列中的请求由结构体request表示，它定义在文件<linux/blkdev.h>中。因为一个请求可能操作多个连续的磁盘块，所以每个请求可以由多个bio结构体组成。注意，虽然磁盘上的块必须连续，但是在内存中这些块不一定要连续，（内存块大于等于磁盘块，因为磁盘块是物理可检测到的最小单位了），每个bio结构体都可以描述多个片段（片段是内存中连续的小区域），而每个请求也可以包含多个bio结构体。

### 14.5IO调度程序

一个bio_vec数组中的元素代表一个页面，而物理块小于一个页面。

如果简单的以内核产生请求的次序直接将请求发向块设备的话，性能肯定让人难受。磁盘寻址是整个计算机中最慢的操作之一，每一次寻址（定位硬盘磁头到特定块上的某个位置）需要花费不少时间，所以尽可能缩短寻址时间无疑是提高系统性能的关键。

为了优化寻址操作，内核既不会简单地按请求接收次序，也不会立即将其提交给磁盘。相反，他会在提交前，先执行名为合并和排序的预操作，这种预操作可以极大地提高系统的整体性能。在内核中负责提交IO请求的子系统称为IO调度程序。

IO调度程序将**磁盘IO资源分配给系统中所有挂起的块IO请求**。具体地说，这种资源分配是通过请求队列中挂起的请求合并和排序来完成的。注意不要将IO调度程序和进程调度程序混淆。进程调度程序的作用是将处理器资源分配给系统中的运行进程。这两种子系统看起来非常相似，但并不相同。进程调度程序和IO调度程序都是将一个资源虚拟给多个对象。对进程调度程序而言，处理器被虚拟并被系统中的运行进程共享。这种虚拟提供给用户的就是多任务和分时操作系统，也就是时间片轮询，虚拟地认为宏观上多任务同时执行。相反，IO调度程序虚拟块设备给多个磁盘请求，以便降低磁盘寻址时间，确保磁盘性能的最优化。（这里的虚拟没听懂）。

#### 14.5.1IO调度程序的工作

IO调度程序的工作是管理块设备的请求队列，他决定队列中的请求排队顺序以及在什么时刻派发请求到块设备。这样做有利于减少磁盘寻址时间，从而提高全局吞吐量。注意，全局这个定语很重要，坦率的讲，一个IO调度器可能为了提高系统整体性能，而对某些请求不公。

IO调度程序通过两种方法减少磁盘寻址时间：合并与排序。（这里的磁盘寻址时间是物理上的，寻找道、磁盘会花费一定的时间）。合并指的是将两个或多个请求结合成一个新请求。考虑一下这种情况，文件系统提交请求到请求队列，从文件中读取一个数据区（当然，最终所有的操作都是针对扇区和块进行的），而不是文件，还假定请求的块都是来自文件块，如果这时队列中已经存在一个请求，他访问的磁盘扇区和当前请求访问的磁盘扇区相邻那么这两个氢气就可以合并为一个对单个和多个相邻磁盘扇区操作的新请求。通过合并请求，IO调度程序将多次请求的开销压缩成一个请求的开销。最重要的是，请求合并后只需要传递给磁盘一条寻址命令，就可以访问到请求合并前必须多次寻址才能访问完的磁盘区域了，因此合并请求显然能减少系统开销和磁盘寻址次数。

现在，假设在读请求被提交给请求队列的时候，队列中并不需要操作相邻扇区的其他请求。此时就无法将当前情感求与其他请求合并，当然，可以将其插入请求队列的尾部。但是如果有其他请求需要操作磁盘上类似的位置呢，如果存在一个请求，他要操作的磁盘扇区位置与当前请求比较接近，那么是不是该让这两个请求在请求队列上页相邻呢，事实上，IO调度程序的确是这么处理上述情况的，整个请求队列将按扇区增长方向有序排列。使所有请求按硬盘上扇区的排列顺序有序排列的目的不仅是为了缩短单独一次请求的寻址时间，更重要的优化在于，通过保持磁盘头以直线方向移动，缩短了所有请求的磁盘寻址时间。该排序算法类似于电梯调度，所以IO调度程序也称作电梯调度。这个在计组中讲过。

#### 14.5.2linus电梯

下面看看linux中实际使用的IO调度程序。看到的第一个IO调度程序称为linus电梯。在2.4版内核中，linus电梯是默认的IO调度程序。虽然后来在2.6版内核中他被另外两种调度程序取代了。

linus电梯能执行合并和排序预处理，当有新的请求加入队列时，他首先会检查其他每一个挂起的请求是否可以和新请求合并。linus电梯IO调度程序可以执行向前和向后合并，合并类型描述的是请求向前还是向后，如果新请求正好连在现存的请求前，就是向前合并。

鉴于文件的分布通常以扇区号的增长表现特点和IO操作执行方式具有典型性（一般都是从头读到尾，很少有从尾读到头的），所以向前合并相比向后合并要少得多，但是linus电梯还是会对两种 合并类型进行检查。如果合并尝试失败那么就需要寻址可能的插入点（新请求在队列中的位置必须符合请求以扇区方向有序排序的原则）。如果找到，新请求将被插入到该点；如果没有合适的位置，那么新请求就被加入到队列尾部。另外，如果发现队列中有驻留时间过长的请求，那么新请求也将被加入到队列尾部，即使插入后，也要排序。这样做是为了避免由于访问相近磁盘位置的请求过多。从而造成访问磁盘其他位置的请求很难得到执行机会的问题。不幸的是，这种年龄检测方法并不很有效，因为它并非是给等待了一段时间的请求提供实质性服务，仅仅是在经过了一段时间后停止插入-排序请求，这改善了等待时间但最终还是会导致请求饥饿现象的发生，所以这是一个2.4内核IO调度程序中必须要修改的缺陷。

总而言之，当一个请求加入到队列中时，有可能发生四种操作，依次是：

1. 如果队列中已存在一个对相邻磁盘扇区操作的请求，那么新请求将和这个已经存在的请求合并成一个请求。
2. 如果队列中存在一个驻留时间过长的请求，那么新请求将被插入到队列尾部，防止其他旧的请求饥饿发生。
3. 如果队列中以扇区方向为序存在合适的插入位置，那么新的请求将被插入到该位置，保证队列中的请求是以被访问磁盘物理位置为序进行排列
4. 如果队列中不存在合适的请求插入位置，就插入到队列尾部。

#### 14.5.3最终期限IO调度程序

最终期限deadlineIO调度程序是为了解决linus电梯所带来的饥饿问题而提出的。出于减少磁盘寻址时间的考虑，对某个磁盘区域上的繁重操作，无疑会使得磁盘其他位置上的操作请求得不到运行机会。实际上，一个对磁盘同一位置操作的请求流可以造成较远位置的其他位置永远得不到运行机会，这是一种很不公平的饥饿现象。

更糟糕的是，普通的请求饥饿还会带来名为写-饥饿-读这种特殊问题。写操作通常是在内核有空时才将请求提交给磁盘的，写操作完全和提交他的应用程序**异步执行**；读操作则恰恰相反，通常当应用程序提交 一个读请求时，应用程序会发生阻塞直到读请求被满足，也就是说，读操作是和提交它的应用程序同步执行的。虽然写反应时间不会个系统响应速度造成很大影响，但是应用程序却必须等待读请求完成后才能运行其他程序，所以**读操作响应时间对系统的性能非常重要**。

问题还可能更严重，这是因为读请求往往会相互依靠。不要读大量的文件，每次都是针对一块很小的缓冲数据区进行读操作，而应用程序只有将上一个数据区从磁盘中读取并返回之后，才能继续读取下一个数据区。糟糕的是，不管是读还是写，二者都需要读取项索引节点这样的元数据。从磁盘进一步读取这些块会使IO操作串行化。所以如果每一次请求都发生饥饿现象，那么对读取文件的应用程序来说，全部延迟加起来会造成过长的等待时间。

综上所述，读操作具有同步性，并且彼此之间往往互相依赖，所以读请求直接影响系统性能，因此2.6版本内核新引入了最后期限IO调度程序来减少请求饥饿现象，特别是读请求饥饿现象。

注意，减少请求饥饿必须以降低全局吞吐量为代价。linus电梯调度程序虽然也做了这样的折中，但显然不够，linus电梯可以提供更好的系统吞吐量，可是它总按照扇区顺序将请求插入到队列，从不检查驻留时间过长的请求，更不会将请求插入到队列尾部，所以他虽然能让寻址时间最短，但却会带来同样不可取的请求饥饿问题。

在最后期限IO调度程序中，每个请求都有一个超时时间 。默认情况下，读请求的超时时间是500ms，写请求的超时时间是5s，最后期限IO调度请求类似于linus电梯，也以磁盘物理位置为次序维护请求队列，这个队列称为排序队列。当一个新请求递交给排序队列时，最后期限IO调度程序在执行合并和插入请求时类似于linus电梯，但是最后期限IO调度程序同时也会以**请求类型为依据将他们插入到额外队列**中。

读请求按次序被插入到特定的读FIFO队列中，写请求被插入到特定的写FIFO队列中。虽然普通队列以磁盘扇区为序进行排序，但是这些队列是以FIFO形式组织的，结果新队列总是被加入到队列尾部。对于普通操作来说，最后期限IO调度程序将请求从排序队列的头部取下，再推入到派发队列中，派发队列将请求提交给磁盘驱动，从而保证了最小化的请求寻址。

如果在**写FIFO队列头，或是在读FIFO队列头的请求超时，那么最后期限IO调度程序便从FIFO队列中提取请求进行服务**。依靠这种方法，最后期限IO调度程序试图保证不会发生有请求在明显超期的情况下仍不能得到服务的现象。

<img src="C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1671454201180.png" alt="1671454201180" style="zoom:33%;" />

注意，最后期限IO调度算法并不能严格保证请求的相应时间，但是通常情况下，可以在请求超时或超时前提交和执行，以防止请求饥饿现象的发生。由于读请求给定的超时时间要比写请求短许多，所以最后期限IO调度器也确保了写请求不会因为堵塞读请求而使读请求发生饥饿。这种对读操作的照顾确保了读响应时间尽可能短。

最后期限IO调度程序的实现在文件block/deadline-iosched.c中。

#### 14.5.4预测IO调度程序

虽然最后期限IO调度程序为降低读操作响应时间做了许多工作，但是它同时也降低了系统吞吐量。假设一个系统处于很繁重的写操作期间，每次提交读请求，IO调度程序都会迅速处理读请求，然后返回再寻址进行写操作，并且对每个读请求都重复这个过程。这个做法对读请求来说是件好事，但是却顺海了系统吞吐量。**预测IO调度程序的目标就是在保持良好的读响应的同时也能提供良好的全局吞吐量。**

预测IO调度的基础仍然是最后期限IO调度程序，所以他们有很多相同之处。预测IO调度程序也实现了三个队列，并为每个请求设置了超时时间，这点和最后期限IO调度程序一样。预测IO调度程序最重要的改进是它增加了预测启发能力。

预测IO调度试图减少在进行IO操作期间，处理新到的读请求所带来的的寻址数量。和最后期限IO调度程序一样，读请求通常会在超时前得到处理，但是预测IO调度程序的不同之处在于，**请求提交后并不直接返回处理其他请求，而是会有意空闲片刻**（实际空闲时间可以设置，默认 为6ms）。这段时间对应用工程需来说是个提交其他读请求的好机会，任何对相邻磁盘位置操作的请求都会立刻得到处理。在等待时间结束后，预测IO调度程序重新返回原来的位置，继续执行以前剩下的请求。

要注意，如果等待可以减少读请求所带来的向后再向前的寻址操作，那么完全值得花一些时间来等待更多的请求。比如此时请求队列中有两个请求在磁盘上的位置距离很远，那么此时如果直接执行去了，而此时刚好来了一个与上一个请求相近的请求，此时就要进行两次寻址了，但如果直接执行这个相近的请求就不需要寻址了。

当然，如果没有IO请求在等待期到来，那么预测IO调度程序会给系统性能带来轻微的损失，浪费掉几毫秒。预测IO调度程序所能带来的优势取决于能否正确预测应用程序和文件系统的行为。（这里才是预测的含义）这种预测依靠一系列的自发和统计工作。预测IO调度程序跟踪并统计每个应用程序块IO操作的习惯行为，以便正确预测应用程序 的未来行为。如果预测准确率足够高，那么预测调度程序便可以大大减少服务读请求所需的寻址开销，而且同时仍能满足请求所需要的系统响应时间要求。这样的话既减少了读响应时间又减少寻址次数和时间，所以说它既缩短了系统响应时间，有提高来了系统吞吐量。

预测IO调度程序的实现在文件内核源代码树的block/as-iosched.c中，他是Linux内核中默认的IO调度程序，对大多数工作负荷来说都执行良好，对服务器也是理想的。不过，在某些非常见而又有严格工作负荷的服务器上，这个调度程序执行的效果 不好。

#### 14.5.5完全公正的排队IO调度程序

完全公正的排队IO调度程序CFQ是为专有工作负荷设计的，不过，在实际中，也为多种工作负荷提供了良好的性能。但是，它与前面介绍的IO调度程序有根本的不同。

CFQ IO调度程序把进入的IO请求放入特定的队列中，这种队列是根据引起IO请求的进程组织的。例如，来自foo进程的IO请求进入foo队列，而来自bar进程的IO请求进入bar队列。在每个队列中，刚进入的请求与相邻请求合并在一起，并 进行插入分类。队列由此按扇区方式分类，这与其他IO调度程序队列类似。**CFQ I/O调度程序的差异在于每一个提交IO的进程都有自己的队列。**

CFQ IO调度程序以时间片轮询调度队列，从每个队列中选取请求数然后进行下一轮调度，这就在进程级提供了公平，确保每个进程接收公平的磁盘带宽片段。预定的工作负荷是多媒体，在这种媒体中，这种公平的算法可以得到保证，比如，音频播放器总能及时从磁盘再填满他的音频缓冲区。不过实际上，CFQIO调度程序在很多场合都能很好地执行。因为每个进程有自己的调度队列，这才提供了进程级的公平。

完全公平 排队IO调度程序CFQ和完全公平调度算法CFS很像啊。位于block/cfq-iosched.c。

#### 14.5.6空操作的IO调度程序

第四种也是最后一种IO调度程序是空操作IO调度程序，之所以这样命名是因为他基本上是一个空操作，不做多少事情。空操作IO调度程序不进行排序，或者也不进行什么其他形式的预寻址操作。唯一做的事情就是执行合并。以近乎FIFO的顺序排列，块设备驱动程序便可以从这种队列中摘取请求。

#### 14.5.7IO调度程序的选择

可以看到下面2.6内核中四种不同的IO调度程序。每一种调度程序都可以被弃用并内置在内核中。默认是CFQ。

| 参数     | IO调度程序     |
| -------- | -------------- |
| as       | 预测           |
| cfq      | 完全公正的排队 |
| deadline | 最终期限       |
| noop     | 空操作         |

例如，内核命令行选项elevator = as会启用预测IO调度程序给所有的块设备，从而覆盖默认的完全公平调度程序。

### 14.6小结

在本章，讨论了块设备的基本知识，并考察了块IO层所用的数据结构；bio，表示活动的IO操作；buffer_head，表示块到页的映射；还有请求结构，表示具体的IO请求。我们追寻了IO请求简单但重要的生命历程，其生命的重点是IO调度程序。讨论了IO调度程序锁涉及到的困惑问题，同时仔细推敲了当前内核的4中IO调度程序。以及2.4版本原有的linus电梯调度。

将在第十五章讨论进程地址空间。

## 第十五章进程地址空间

---

严格来说，我学的ARM架构就是这里才是发挥真正的作用。

----

第十二章介绍了内存如何管理物理内存。其实内核除了管理本身的内存外，**还必须管理用户空间中进程的内存**。我们称这个内存为进程地址空间，也就是系统中每个用户空间进程所看到的的内存。Linux操作系统采用虚拟内存技术，（所以说管理的并不是内核进程，这个属于内核内部的），因此，系统中的所有进程之间以虚拟方式共享内存。对一个进程而言，他好像可以访问整个系统的所有物理内存。这里的虚拟方式是个啥，更重要的是，即使单独一个进程，它拥有的地址空间也可以远远大于系统物理内存。对，我在进程中访问任意路径的文件都可以，但是内核进程中因为有命名空间限制，进程是不可以访问整体内核的。本章将集中讨论内核如何管理进程地址空间。

### 15.1地址空间

进程地址空间由**进程可寻址的虚拟内存**组成，而且更为重要的特点是内核允许进程使用这种虚拟内存中的地址。每个进程都有一个32位或者64位的平坦地址空间，空间的具体大小取决于体系结构，术语平坦指的是地址空间范围是一个独立的连续区间。一些操作系统提供了段地址空间，这种地址空间并非是一个独立的线性区域，而是被分段的，但现代采用虚拟内存的操作系统通常都使用平坦地址空间而不是分段式的内存模式。通常情况下，每个进程都有唯一的这种平坦地址空间。一个进程的地址空间与另一个进程的地址空间即使有相同的内存地址，实际上也彼此互不相干。称这样的进程为线程。（共享主进程的地址空间，其实如果考虑到fcse的话，其实所有的进程虚拟地址都相同，根据特殊重定位寄存器映射不同的二级页表）

内存地址是一个给定的值，他要在地址空间范围之内，比如4021f000，这个值表示的是进程32位地址空间中的一个特定的字节。尽管一个进程可以寻址4GB的虚拟内存，但这并不代表他有权访问所有的虚拟地址。在地址空间中，更关心的是一些虚拟内存的地址区间，他们可被进程访问。（这里所谓的被进程访问，其实指的是处理器模式，确实有一部分区域被访问会发生fault异常，这也是和MPU区别，没有所谓背景区域的概念）这些可被访问的合法地址空间称为内存区域。通过内核，进程可以给自己的地址空间动态地添加或减少内存区域。

进程只能访问有效内存区域内的内存地址。每个内存区域也具有相关权限，比如对相关进程有可读、可写、可执行属性。如果一个进程访问了不在有效范围中的内存区域，或者以不正确的方式访问了有效地址，那么内核就会终止该进程，并返回段错误信息。（段错误包括栈溢出、预取指中止、fault等）

内存区域可以包含各种内存对象，比如：

- 可执行文件代码的内存映射，称为代码段
- 可执行文件的已初始化全局变量的内存映射，称为数据段。（这两个太眼熟了，不就是进程的组成方式吗，但是学内核进程的时候好像没有这些事吧）
- 包含未初始化全局变量，也就是bss段的零页（页面中的信息全部为零，所以可用于映射bss段等目的）的内存映射。
- 用于进程用户空间栈（不要和进程**内核栈混淆**，进程的内核栈独立存在并由内核维护）的零页的内存映射。（明明这里都说了，当初就是没学明白）
- 每一个诸如C库或动态连接程序等共享库的代码段、数据段和bss也会被载入进程的地址空间。
- 任何内存映射文件。
- 任何共享内存段。
- 任何匿名的内存映射，比如由malloc()分配的内存。

进程地址空间中的任何有效地址都只能位于唯一的区域，这些内存区域不能相互**覆盖**。（MMU和MPU的区别）可以看到，在执行的进程中，每个不同的内存片段都对应一个独立的内存区域：栈、对象代码、全局变量、被映射的文件等。（进程地址空间中的区域都是该进程0的区域，栈是进程0的栈、以及全局可视的内容）

### 15.2内存描述符

内核使用**内存描述符结构体表示进程的地址空间**，之前的bio结构体表示块IO分布在内核上的内存片段。该结构体包含了和进程地址空间有关的全部信息。内存描述符由mm_struct结构体表示，定义在文件<linux/sched.h>中。下面给出内存描述符的结构体和各个域的描述：这个mm_struct就在task_struct中，之前提到过，我就感觉眼熟。（mm_struct结构体描述每个内存段起始地址、内存段采用链表维护的。）

术语BSS已经有些年头了，他是block started by symbol的缩写。因为未初始化的变量没有对应的值，所以并不需要存放在可执行对象中。但是有因为C标准强制规定未初始化的全局变量要被赋予特殊的默认值也就是0，所以内核要将未赋值的变量从可执行代码加载到内存中，然后将零页映射到该片内存上。于是这些未初始化的变量就被赋予了0值，这样做避免了在目标文件中显式地进行初始化，减少了空间浪费。

mm_users域记录正在使用该地址的进程数目。比如如果两个线程（在内核中进程和线程没有区别）共享该地址空间，那么mm_users的值便等于2；（线程、vfork创建的子进程都共享mm_struct、fork创建的子进程因为拷贝页表所以不共享、内核线程没有进程地址空间）

mm_count域是mm_struct结构体的主引用计数。所有的mm_users都等于mm_count的增加量。当mm_count的指等于0，说明已经没有任何指向该mm_struct的引用了，此时该结构体会被撤销。当内核在一个地址空间上操作，并需要使用该地址相关联的引用计数时，内核便增加mm_count。内核同时使用这两个计数器是为了区分主使用计数器和使用该地址空间的进程的数目。

---

map_count是进程地址空间中的VMA个数

---

mmap和mm_rb这两个不同数据结构描述的对象是相同的：该地址空间中的全部内存区域。但是前者以链表形式存放（块IO操作在内存中也是以链表形式存放的）而后者以红黑树形式存放，（CFS就绪队列就是由红黑树管理的），搜索他的时间复杂度为O(log n)。（这里就是内存区域的维护方式两种）

内核通常会避免使用两种数据结构组织同一种数据，但此处内核这样的冗余的确能排上用场，mmap结构体作为链表，利于简单、高效地遍历所有元素；而mm_rb结构体作为红黑树，更适合搜索指定元素。红黑树特点是时间复杂度成对数。内核并没有复制mm_struct结构体，而仅仅被包含其中。覆盖树上的链表并用这两个结构体同时访问相同的数据集，有时候称此操作为线索树。不理解

所有的mm_struct结构体都通过自身的mmlist域连接在一个双向链表中（每个进程地址空间链接起来），该链表的首元素是init_mm内存描述符，他代表init进程的地址空间。另外要注意操作该链表的时候需要使用mmlist_lock锁来防止并发访问，该锁定义在文件kernel/fork.c中。mm_struct原来也是一个包含链表的父结构。联系方式是mmlist。（因为所有进程都有可能访问链表，所以需要上个防止并发）

start_code和end_code：用于存放进程代码段开始和结束的地址。

start_data和end_data：存放初始化数据开始和结束地址

start_brk和end_brk：堆栈的开始和结束地址

start_stack：进程栈的开始地址。

arg_start和arg_end：指向传递给进程的参数的开始和结束地址

env_start和env_end：存放环境的开始和结束地址。

#### 15.2.1分配内存描述符

在进程的进程描述符中，mm域存放着该进程使用的内存描述符，所有current->mm便指向当前进程的内存描述符。（之前的知识了）fork()函数利用**copy_mm**()函数复制父进程的内存描述符，也就是把mm域给其子进程，（实际上是子进程从mm_cachep 高速缓存中分配一个对象，而fork函数因为拷贝页表，并不是共享进程地址空间，这里是复制父进程的mm_struct，而不是共享，两个mm_struct值一样，但各自独立）

子进程中的mm_struct结构体实际是通过文件kernel/fork.c中的**allocate_mm**()

通常，每个进程都有唯一的mm_struct结构体，即唯一的进程地址空间。

如果父进程希望与其子进程共享地址空间，可以在调用clone()时，设置CLONE_VM标志，把这样的进程称作线程。是否共享地址空间几乎是进程和线程之间本质上的唯一区别（这么说的话vfork以及用户线程都属于线程）。除此之外，Linux内核并不区分对待他们，线程对内核来说是仅仅是一个共享特定资源的进程而已。

当CLONE_WM被指定后，内核就不再需要调用allocate_mm()函数了，而仅仅需要在调用copy_mm()函数将mm域指向其父进程的内存描述符即可。（这里说的很明白了，因为不拷贝页表，也就不复制进程地址空间，仅共享即可）

```c
if(clone_flags & CLONE_VM){
	atomic_inc(&current->mm->mm_users);
    tsk->mm = current ->mm;//将父进程的内存描述符交给子进程tsk的内存描述符
}
```

#### 15.2.2撤销内存描述符

当进程退出时，内核会调用定义在kernel/exit.c中的exit_mm()函数，该函数执行一些常规的撤销工作，同时更新一些统计量。其中，该函数会调用mmput()函数减少内存描述符中的mm_users用户计数，如果用户计数降到零，将调用个mmdrop()函数，减少count_count使用计数。如果使用计数也等于零了，说明该内存描述符不再有任何使用者了，那么调用free_mm()宏通过kmem_cache_free()函数将mm_struct结构体归还给mm_cachep slab缓存中。这是内存管理的内容了。（既然是从slab分配的，就不能直接销毁，需要使用kmem_cache_free()函数将mm_struct结构体归还到mm_cachep slab缓存中，我怀疑这个函数主要工作就是清空对象内容，而不是释放内存）

#### 15.2.3mm_struct与内核线程

内核线程没有进程物理地址空间。（所谓的没有用户上下文，指的是处理器模式不会切换到user模式而是只能在管理模式下运行在内核栈中）

- 寄存器包括：31个通用寄存器、cpsr、栈指针esp、程序计数器pc。
- 用户级上下文：正文、数据、共享存储区、用户堆栈。
- 系统级上下文：内核栈、内存管理信息(mm_struct/vm_area_struct/gde顶级页表、gte二级页表)、进程控制块PCB。

省了进程地址空间再好不过了，因为内核线程并不需要访问任何用户空间的内存（那他们访问谁的呢）而且因为内核线程在用户空间中没有任何页，所以实际上他们并不需要有自己的内存描述符和页表（内核空间实际上的虚拟地址==物理地址，所以对于所有进程而言内核空间部分的**映射关系是相同**的，所以每次切换上下文需要在内核空间进行。防止映射出错，所以内核线程不需要自己的页表，使用任意进程的页表即可。）。尽管如此，**即使访问内核内存，内核线程也还是需要使用一些数据的**。比如页表，为了避免内核线程为内存描述符和页表浪费内存，也为了当新内核线程运行时，**避免浪费处理器周期向新地址空间进行切换，内核线程将直接使用前一个进程的内存描述符**。（使用前一个进程的内存描述符中的内核空间是可以的，因为内核空间全进程共享，至于说使用前一个进程的东西，这是老传统了，中断也是如此，现在是每个进程都有一个中断栈好一些了）

当一个进程被调度时，**该进程的mm域指向的地址空间被装载到内存**（这句话将虚拟存储器讲明白了），进程描述符中的active_mm域会被更新，指向新的地址空间。内核线程没有地址空间，所以mm域为NULL，就会**保留前一个进程的地址空间**（不会装载新的地址空间到内存了），随后内核更新内核线程对应的进程描述符中的active_mm域（这个域表示的是使用哪个域，mm表示装载哪个域），使其指向前一个进程的内存描述符。所以在需要时，内核线程便可以使用前一个进程的页表。因为内核线程不访问用户空间的内存，所以他们仅仅使用地址空间中和内核内存相关的信息，这些信息的含义和普通进程完全相同。（指的就是内核空间的页表完全相同，内核线程虽然只使用内核空间，但依旧需要页表进行）

### 15.3虚拟内存区域

内存区域由vm_area_struct结构体描述，定义在文件<linux/mm_types.h>中。**内存区域在Linux内核中也经常称作虚拟内存区域**（virtual memory Areas VMAs）。

vm_area_struct结构体描述了指定地址空间内连续区间上的一个独立内存范围。内核将每个内存区域作为一个单独的内存对象管理，每个内存区域都拥有一致的属性（这里一致的属性是因为region区域导致的，我认为VMA结构体和region结构体是新旧区别，后者在系统启动期间划分内存，前者是描述内存段），比如**访问权限**等，另外，相应的操作也都一致。按照这样的仿古式，每一个VMA都可以代表不同类型的内存区域，这种管理方式类似于使用VFS层的面向对象方法，下面给出该结构定义和各个域的描述：uboot因为关闭MMU可能只是划分模式堆栈，而不提供受保护的各个区域。（VMA结构体是访问权限、区间起始结束地址）

每个内存描述符都对应于进程地址空间中的唯一区间。vm_start域指向区间的首地址（最低地址），vm_end域指向区间的尾地址（最高地址）之后的第一个字节，（内存描述符是用来描述进程地址空间的，而地址空间所对应的内存由虚拟内存区域描述）。

也就是说，vm_start是内存区间的开始地址（本身在区间内），而vm_end是内存区间的结束地址（本身在区间外），因此vm_end-vm_start的大小就是内存区间的长度，内存区域的位置在[vm_start,vm_end]之中，注意，在同一个地址空间内的不同内存区间 不能重叠。

vm_mm域指向和VMA相关的mm_struct结构体，注意，**每个VMA对其相关的mm_struct结构体来说都是唯一的**，所以即使两个独立的进程将同一个文件映射到各自的地址空间，他们分别都会有一个vm_area_struct结构体来标志自己的内存区域；反过来，如果两个线程共享一个地址空间，那么他们也同时共享其中的所有vm_area_struct结构体。

#### 15.3.1VMA标志

VMA标志是一个位标志，其定义在<linux/mm.h>，它包含在vm_flags域内，标志了内存区域所包含的页面的行为和信息。和**物理页的访问权限**不同（物理页的访问权限指的是页表项中定义的AP吧，这里更全面一些），VMA标志反应了内核处理页面所需要**遵守的行为准则**，而不是硬件要求。

而且vm_flags同时也包含了内存区域中每个页面的信息，或**内存区域的整体信息**，而不是具体的独立页面。

让我们进一步看看其中有趣和重要的几种标志，VM_READ、VM_WRITE和VM_EXEC标志了内存区域中页面的读、写和执行权限。这些标志根据要求组合构成VMA的访问控制权限，当访问VMA时，需要查看其访问权限。比如进程的对象代码映射区域可能会标志为VM_READ和VM_EXEC，而没有标志为VM_WRITE；另一方面，可执行对象数据段的映射区域标志为VM_READ和VM_WRITE。而VM_EXEC标志对它就毫无意义。也就是说只读文件数据段的映射区域仅可被标志为VM_READ。

VM_SHARD指明了内存区域包含的映射是否可以在多进程间共享，如果该标志被设置，则我们成其为共享映射；如果未被设置，而仅仅只有一个进程可以使用该映射内容，成它为私有映射。

VM_IO标志内存区域中包含对设备IO空间的映射。该标志通常在设备驱动程序执行mmap()函数进行IO空间映射时才被设置，同时该标志也表示该内存区域不能被包含在任何进程的存放转存中。剩下的不介绍了。

#### 15.3.2VMA操作

vm_area_struct结构体中的vm_ops域指向与指定内存区域相关的操作函数表，内核使用表中的方法操作VMA。vm_area_struct作为通用对象代表了任何类型的内存区域，而操作表描述针对特定的对象实例的特定方法。（每个内存段都有对应的操作函数表，这么说段错误处理程序实际上是每个内存段中操作函数表中的函数，确实对每个段都需要一个表，以便处理段错误，之前以为是内核检测VMA标志异常后触发异常处理，过于模糊了，实际上就是函数操作表中的函数实现的，但是如何被调用还是不清楚具体流程，**页面故障处理**来调用函数）

操作函数表由vm_operations_struct结构体表示，定义在文件<linux/mm.h>中。

下面介绍具体方法：

```c
void open(struct vm_area_struct *area);//当指定的内存区被加入到一个地址空间时，该函数被调用
void close(struct vm_area_struct *area);//当指定的内存区域从地址空间删除时，该函数被调用。
int fault(struct vm_area_struct *area,struct vm_fault *vmf);//当没有出现在物理内存中的页面被访问时，该函数被页面故障处理调用。
int page_mkwrite(struct vm_area_struct *area,struct vm_fault *vmf);//当某个页面为只读页面时，该函数被页面故障处理调用。
int access(struct vm_area_struct *vma,unsigned long address,void *buf,int len,int write)//当get_user_pages()函数调用失败时，该函数被access_process_vm()函数调用。
```

---

因为内核没提到page_mkwrite的具体工作，只是说内存段只读，写入时调用，具体工作是用于将只读页面复制为可写页面并建立映射关系。

流程应该是写入只读内存段后，MMU抛出缺页中断，在缺页中断中判断当前内存段的引用次数，如果大于1说明是写时拷贝，然后在缺页中断中调用page_mkwrite函数实现拷贝并建立映射给新的进程。

---

#### 15.3.3内存区域的树形结构和内存区域的链表结构

上文讨论过，可以通过内存描述符中的mmap和mm_rb域之一访问内存区域。这两个域各自独立地指向与内存描述符相关的全体内存区域对象。其实他们包含完全相同的vm_area_struct结构体的指针，仅仅组织方法不同。

mmap域使用单独链表 连接所有的内存区域对象。每一个vm_area_struct结构体通过自身的vm_next域被连入链表，所有的区域按地址增长的方向排序，mmap域指向链表中第一个内存区域，链中最后一个结构体指针指向空。

mm_rb域使用 红黑树连接所有的内存区域对象 。mm_rb域指向红黑树的根节点，地址空间中每一个vm_area_struct结构体通过自身的vm_rb域连接到树中。

红黑树中的每个节点都被配以红色或者黑色。分配规则是红节点的子节点是黑色，并且树中任何一条从节点到叶子的路径必须包含同样数目的黑色节点。根节点总为红色。红黑树的搜索、插入、删除等操作的复杂度都是O(long(n))。

链表用于需要遍历全部节点的时候，而红黑树适用于在地址空间中定位特定内存区域的时候。内核为了内存区域上的各种不同操作都能获得高性能，所以同时使用了这两种数据结构。

#### 15.3.4实际使用中的内存区域

可以使用/proc文件系统和pmap(1)工具查看给定进程的内存空间和其中所含的内存区域。看一个简单的例子。

![1689125381219](C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1689125381219.png)

列出该进程地址空间中包含的内存区域，其中有代码段、数据段和bss段等。假设该进程与C库动态连接，那么地址空间中还将分别包含libc.so和ld.so对应的上述中内存区域。此外，地址空间中还要包含进程栈对应的内存区域。

---

之前学ARM架构的时候，我就经常好奇进程地址空间中到底包含什么

这次看到了，c库的代码段、数据段、bss段。（很合理，区域就分为数据和代码两种），然后是可执行文件也就是本身的内容代码段、数据段，因为没有未初始化的变量所以没有bss段，之后是动态连接我理解为链接其他文件的代码段、数据段、bss段，最后是进程栈组成。很合理。

----

该进程的全部地址空间大约为1340KB，但是只有大约40KB的内存区域是可写和私有的。如果一片内存范围是共享的或不可写的，那么内核只需要在内存中 为文件保留一份映射。对于共享映射来说，这么做没什么特别的，但是对于不可写内存区域为什么也这么做。如果考虑到映射区域不可写意味着该区域不可被改变，就应该清楚只把该映射读入一次是很安全的，所以C库在物理内存中 仅仅需要占用1212KB，而不需要为每个使用C库的进程在内存中都**保存一个1212KB的空间**。进程访问了1340KB的数据和代码空间，然而**仅仅消耗了40KB的物理内存**，可以看出利用这种共享不可写内存的方法节约了大量的内存空间。（凡是共享和不可写的内存段，只需要为每个进程保留一份映射，也就是多个页表项对应一个页帧，而不为每个进程单独拷贝一份页帧+页表，让所有进程映射到同一个内存段上）

注意，没有映射文件的内存区域的设备标志为00:00，索引借点标志也为0，这个区域就是零页。如果将 零页映射到可写的内存区域，那么该区域将全被初始化为0。这是零页的一个重要用处，而bss段需要的就是全零的内存区域。由于内存未被共享，所以只要一有进程写该处数据，那么该处数据就将就被拷贝出来，然后才被更新。

**每个和进程相关的内存区域都对应于一个vm_area_struct结构体**。另外进程不同于线程，进程结构体task_struct包含唯一的mm_struct结构体 引用。

### 15.4操作内存区域

内核时常需要在某个内存区域上执行一些操作，比如某个指定地址是否包含在某个内存区域中 。这类操作非常频繁，另外他们也是mmap例程的基础，为了方便执行这类对内存区域的操作，内核定义了许多的辅助函数。

都声明在文件<linux/mm.h>中。

#### 15.4.1find_vma()

为了找到一个给定的内存地址属于那个内存区域，内核提供了find_vma()函数。该函数定义在文件<mm/mmap.c>中：

```c
//为了找到一个给定的内存区域属于哪一个内存区域，内核提供了find_vma()函数，该函数定义在文件<mm/mmap.c>中
struct vm_area_struct *find_vma(struct mm_struct *mm,unsigned long addr);
```

该函数在**指定的地址空间**（指定的进程地址空间）中搜索**第一个**vm_end大于addr的内存区域（因为只是给出一个addr地址，要保证返回的内存段包含这个地址）。如果没有发现这样的区域，该函数返回NULL；否则返回指向匹配的内存区域的vm_area_struct结构体指针（返回第一个内存段结构体指针）。

注意，由于返回的VMA首地址可能大于addr，所以指定的地址并不一定就包含 在返回的VMA中 。因为 很有可能在对某个VMA执行操作之后，还有其他更多的操作会对该VMA接着进行操作，**所以find_vma()函数返回的结果 被缓存在内存描述符mmap_cache域中**（这里是mm_struct结构体的域中）。实践证明，被缓存的VMA会有相当好的命中率，而且检查被缓存的VMA速度会很快，如果指定的地址不再缓存中 ，那么必须搜索和内存描述符相关的所有内存区域。这种搜索通过红黑树进行：（内存段vma可以由链表或红黑树维护，mm_struct只采用链表维护）

```c
struct vm_area_struct *find_vma(struct mm_struct *mm,unsigned long addr)
{
    struct vm_area_struct *vma = NULL;
    if(mm){//如果提供的进程地址空间结构体存在的话
        vma = mm->mmap_cache;//获取缓存中的内存段
        if(!(vma && vma->vm_end > addr && vma->vm_start <=addr))//如果内存段并不是要找的内存段，需要使用红黑树维护内存段来进行查找，这样比链表快
        {
            struct rb_node *rb_node;
            vma = NULL;
            while(rb_node){
                struct vm_area_struct *vma_tmp;
                vma_tmp = rb_entry(rb_node,struct vm_area_struct,vm_rb);
                if(vma_tmp->vm_end>addr){//如果当前vma的结束地址大于addr
                    vma = vma_tmp;
                    if(vma_tmp->vm_start <=addr)
                        break;
                    rb_node = rb_node->rb_left;
                }else{
                    rb_node = rb_node ->rb_right;
                }
            }
            if(vma)
                mm->mmap_cache = vma;//将找到的vma送入缓存
        }
    }
    return vma;
}
```

首先，该函数检查mmap_cache，看看缓存的VMA是否包含了所需地址。注意简单地检查VMA的vm_end是否大于addr，并不能 保证该 vma是第一个大于addr的内存区域，所以缓存要想发挥作用，就要求指定的地址必须包含在被缓存的VMA中 ，幸好，这也正是连续操作同一VMA必然发生的情况。

如果缓存中并未包含希望的VMA，那么该函数必须搜索红黑树。如果当前 VMA的vm_end大于addr，进入左子节点接续搜索；否则沿着右边子节点搜索，直到找到包含addr的VMA位置。（除了红黑树的操作没看懂之外都可以）

#### 15.4.2find_vma_prev()

次函数和find_vma工作方式相同，但他**返回第一个小于addr的VMA**。该函数声明和定义分别在文件mm/mmap.c中和文件<linux/mm.h>中。

```c
struct vm_area_struct *find_vma_prev(struct mm_struct *mm,unsigned long addr,struct vm_area_struct **pprev)
```

pprev参数存放指向先于addr的VMA指针。

#### 15.4.3find_vma_intersection()

此函数返回第一个和**指定地址区间相交的VMA**。因为该函数是内联函数，所以定义在文件<linux/mm.h>中。

### 15.5mmap()和do_mmap()创建地址区间

内核使用do_mmap()函数**创建一个新的线性地址区间**。但是说该函数创建了一个新VMA并不非常准确，因为如果创建的地址区间和一个已经存在的地址区间相邻，并且具有相同的访问权限的话，两个区间将合并为一个。

---

只有在无上也无下邻空闲区时才不会合并，导致空闲区的数量+1

---

如果不能合并，就确实需要创建一个新的VMA了。但无论哪种情况，do_mmap()函数都会将一个地址区间 加入到进程的地址空间之中 ，无论是**扩展**已存在的内存区间还是**创建**一个新的区域。（此函数就是创建一段虚拟空间，那映射怎么解决，这段虚拟内存对应于提供的文件内容物理地址，如果没有指定文件，此时虚拟空间可能映射个进程栈这种和文件页无关的内存段映射给虚拟空间，文件映射就是提供一段文件的物理空间给虚拟空间）

do_mmap()函数定义在文件<linux/mm.h>中。该函数映射由file指定的文件，具体映射的是文件中从偏移offset处开始，长度为len字节的范围内的数据。如果file参数是NULL并且offset参数也是0，那么就代表这次映射没有和文件相关，该情况称为匿名映射。如果指定了文件名和偏移量就是文件映射。

addr是可选参数，他指定搜索空闲区域的起始位置。

prot参数指定内存区域中页面的访问权限。访问权限标志定义在文件<asm/mman.h>中，不同体系结构标志的定义有所不同，但是对所有体系结构来说都会包含下表的标志，不展开说了。

---

之前考过这道题，prot是访问权限，fd才是文件句柄

---

flags参数指定了VMA标志，这些标志指定 类型并改变映射的行为。他们也在文件<asm/mman.h>中定义。

如果系统调用do_mmap()的参数中有无效参数，那么他返回一个负值；否则会在虚拟内存中分配一个合适的新内存区域。如果有可能的话，将新区域和邻近区域进行合并（能合并就合并），否则内存从vm_area_cachep（vma也有自己的高速缓存，分配一个对象作为新区域的结构体）长字节缓存中分配一个vm_area_struct结构体，并且使用vma_link()函数将新分配的内存区域添加到地址空间的内存区域链表和红黑树中，随后还要更新内存描述符中的total_vm域，然后才返回新分配的地址区间的初始地址。

在用户空间可以通过mmap()系统调用获取内核函数do_mmap()的功能。

---

mmap函数系统调用过程如下：

1. 先通过文件系统定位到映射的文件；
2. 权限检查，映射的权限不会超过文件打开的方式；
3. 创建一个vma对象，对其初始化；
4. 调用映射文件的mmap函数，给vm_ops操作函数结构体（也称为向量表，很贴切）赋值。这个向量表是当错误访问该内存段时调用，比如写时拷贝；
5. 将该vma链入进程的vma链表中，如果可以和前后的vma合并则合并，相邻且访问权限相同；
6. 如果VM_LOCKED映射区不被换出的方式映射，需要发出缺页请求，将映射页面读入内存中。





### 15.6mummap()和do_mummap()删除地址区间

do_mummap()函数从特定的进程地址空间中删除指定地址区间，该函数定义在文件<linux/mm.h>中。

```c
int do_mummap(struct mm_struct *mm,unsigned long start,size_t len)
```

### 15.7页表

虽然应用程序操作的对象是映射到物理内存之上的虚拟内存，但是处理器直接操作打断是物理内存，所以当用程序访问一个虚拟地址时，首先必须将虚拟地址转换为物理地址，然后处理器才能解析地址访问请求。地址的转换工作需要通过查询页表才能完成，概括的说，地址转换需要将虚拟地址分段(这里的将虚拟地址分段，指的就是主页表中的页表项或段项是具有逻辑意义的)，使每段虚拟地址都作为一个索引指向页表（这就是主页表和二级页表的意义），而页表项则指向下一级别的页表或者指向最终的物理页面。

Linux中使用三级页表完成地址转换（三级页表的话就是主页表，二级页表，三级页表，比ARM架构介绍好多了一级）。利用多级页表能够节约地址转换所需占用的存放空间。这个计组中讲过了。如果利用三级页表转换地址，即使是64位机器，占用的空间也很有限。（哪怕不支持三级页表的体系结构也采用三级页表管理，因为最大公约数思想）

顶级页表是页全局目录PGD，它包含了pgd_t类型**数组**（采用数组来维护页表），多数体系结构中pgd_t等同于无符号长整形类型，PGD中的表项指向二级页目录中的表项PMD。

二级页表是中间页目录PMD，他是个pmd_t类型的页表项，该页表项指向三级页表PTE，PTE中指向物理页。

多数体系结构中，搜索页表的工作是由硬件完成的，虽然是硬件执行，但是只有在内核正确设置页表的前提下，硬件才能方便地操控他们。

每个进程都有自己的页表（这个就是学完ARM架构才能理解的了）。内存描述符的pgd域指向的就是进程的页全局目录。注意，操作和检索页表时必须使用page_table_lock锁，该锁在相应的进程的内存描述符中以防止竞争。（共享了进程地址空间就意味着共享了页表，不然只有虚拟地址无法找到物理内存）

页表对应的结构体依赖于具体的体系结构（三级页表的结构体和硬件有关了），所以定义在文件<asm/page.h>中。

之前得到的是虚拟内存而已，还需要通过页表转换为物理地址

![1671512512612](C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1671512512612.png)

由于几乎每次对虚拟内存中的页面访问都必须先解析它，从而得到物理内存中的对应地址，所以页表操作的性能非常关键。但不幸的是，搜索内存中的物理地址速度很有限，因此为了加快搜索，多数体系结构都实现了一个翻译后缓冲器（translate lookaside buffer,TLB，这不就是快表吗）TLB作为一个将虚拟地址映射到物理地址的硬件缓存，当请求访问一个虚拟地址时，处理器将首先检查TLB中是否存储了该虚拟地址到物理地址的映射（其实这个就是页表项，之前的dcache和icache存放的是dentry目录项对象和索引节点对象，这个是还属于虚拟文件系统中的）。如果在缓存中直接命中，物理地址立刻返回；否则，就需要再通过页表搜索需要的物理地址。

虽然硬件完成了有关页表的部分工作，但是页表的管理仍然是内核的关键部分，而且在不断改进。

主要改进是：从高端内存分配部分页表。（这算什么改进，使用完页表后释放），今后可能的改进包括用写时拷贝的方式共享页表。这样fork函数不需要拷贝页表和页帧给子进程，只有当修改页表项时，才创建该页表项的拷贝。

阀值是分配内存的值。

这三个阀值的关系是：min阀值 < low阀值 < high阀值。

low阀值 = min阀值 + (min阀值 / 4)，high阀值 = min阀值 + (min阀值 / 2)

较小的内存系统中，slab分配器过于复杂于是采用slob分配器采用最先适配 算法。在内存很大的系统中，slab分配器本身占用的内存空间过大，采用slub进行优化。（内核中只提到了slab）

**伙伴算法**

可以分配11个块链表，分别是从2^0到2^10个页面。当想要分配的页面数量过少时，就将块链表分割两半分别使用和空闲，作为伙伴，当被分配的伙伴空闲时再合并。这就导致合并条件太严苛，只有伙伴才能合并，并且如果伙伴中只有一个页面被占用，就不能合并，只能分配2的幂，如果是基数会导致浪费空间，用链表管理页面会导致合并又立刻拆分。

因为各部分的虚存空间的访问属性不同，需要多个vma_area_struct结构体描述，使用升序排序，以单链表组织数据，也可以采用AVL树。（红黑树管理进程空间中的多个vma结构体）

---

Linux内核没提过内存回收，分为三种，快速、直接、kswapd内存回收。空闲内存数量足够，就使用快速内存回收，直接内存回收：是当每个zone以最低阀值还是无法回收时，对所有的zone进行一次直接回收。

kswapd内存回收发生在kswapd内核线程中。内核确实提到了zone结构体，这是根据物理内存属性不同划分的区域。zone_high/zone_normal/zone_DMA。

---



### 15.8小结

看到了抽象出来的进程虚拟内存（进程地址空间），看到了内核中如何表示进程空间(通过mm_struct)以及内核如何表示该空间中的内存区域（通过结构体vm_area_struct）。除此之外，还了解了内核如何创建（mmap）和撤销（munmap()）这些内存区域，最后还讨论了页表。因为Linux是一个基于虚拟内存的操作系统，所以这些概念对于系统运行来说非常基础。

第十六章讨论页缓存，一种用于所有页IO操作的内存数据缓存，而且还要涵盖内核执行基于页的数据回写。

## 第十六章页高速缓存和页回写

页高速缓存是Linux内核实现磁盘缓存。主要用来减少对磁盘的IO操作。具体地将，是通过把磁盘中的数据缓存到物理内存中，把对磁盘的访问变为对物理内存的访问。这一章将讨论页高速缓存与页回写（将页高速缓存中的变更数据刷新回磁盘的操作）。（TLB是MMU提供页表项，而cache提供数据和指令、这里是将内存分出一部分存储磁盘中的数据）

磁盘高速缓存之所以在任何现代操作系统中显得尤为重要源自两个因素：第一，访问磁盘的速度太低，第二数据一旦被访问，短时间内再次被访问。

### 16.1缓存手段

页高速缓存是由内存中的物理页面组成的，其内容对应磁盘上的物理块。页高速缓存大小能动态调整，可以通过占用空闲内存以扩张大小，也可以自我收缩以缓解内存使用压力。我们称 正被缓存的存储设备为‘后备存储，因为缓存背后的磁盘无疑才是所有缓存数据的归属，当内核开始一个读操作，首先会检查需要的数据是否在页高速缓存中。如果在，则放弃访问磁盘，而是直接从内存中读取，这个行为称为缓存命中。如果不再就是未命中，那么内核必须调度块IO操作从磁盘去读取数据。然后内核将读来的数据放入页缓存中，于是任何后续相同的数据读取都可命中缓存了。缓存可以持有某个文件的全部内容，也可以存储另一些文件的一页或者几页。（这里理解为存储文件更好一些，不可能所有的文件都放到内存中，也不可能只把用到的文件放到内存中，那么剩余的空间就用来存放剩余的文件，以页为单位）

#### 16.1.1写缓存

在进程写磁盘时，比如执行write()系统调用，缓存如何被使用呢，通常来讲，缓存一般被实现成下面三种策略之一：第一种策略称为不缓存，也就是高速缓存不去缓存任何写操作。当对一个缓存中的数据片进行写时，将直接跳过缓存，直接写入磁盘中。同时也使缓存中的数据失效。后续读操作进行时，需要再重新从磁盘中读取数据。不过这种策略很少使用，因为该策略不但不去缓存写操作，而且需要额外费力去使缓存数据失效。（这里的策略和cache的回写策略一样）

第二种策略，写操作将自动更新内存缓存，同时也更新磁盘文件。这成为写透缓存。

第三种策略，也是Linux所采用的称为回写，在这种策略下，程序执行写操作直接写到缓存中，后端存储不会立刻直接更新，而是将页高速缓存中被写入的页面标记为脏，并且被加入到脏页链表中。然后由一个进程周期性将脏页链表中的页写回到磁盘中。（这里的脏页标识实际上就是cache中的脏位cache行，cache中的脏指的是检索cache行对应的虚拟地址对应存储单元需要被修改，这里就是磁盘中的页需要被修改）

#### 16.1.2缓存回收

缓存算法最后涉及的重要内容是缓存中的数据如何清除；或者是为更重要的缓存项腾出位置；这个 工作，也就是决定缓存中什么内容将被清除的策略，称为缓存回收策略。Linux的缓存回收是通过选择干净页进行简单替换。如果缓存中没有足够的干净页面，内核将强制地进行回写操作，以腾出更多的干净可用页。最难的事情在于决定什么页应该回收。理想的回收策略应该是回收那些以后最不可能使用的页面。当然要知道以后的事情你必须是先知。也正是这个原因，理想的回收策略称为预测算法。但这种策略太理想了。

##### 1.最近最少使用

LRU回收策略需要跟踪每个页面的访问踪迹，以便能回收最老时间戳的页面。该策略的良好效果源自于缓存的数据越久未被访问，则越不大可能近期再被访问，而最近被访问的最有可能被再次访问。LRU策略并非是放之四海而皆准的法则，对于许多文件被访问一次，再不被访问的情景，LRU尤其失败。将这些页面放在LRU链的顶端显然不是最优。

##### 2.双链策略

Linux实现的是一个修改过的LRU，也称为双链策略。和以前不同，Linux维护的不再是一个LRU链表，而是维护两个链表：活跃链表和非活跃链表。处于活跃链表上的页面被认为是热的且不会被换出的。而非活跃链表上的页面则是可以被换出的。两个链表都被伪LRU规则维护：页面从尾部加入，从头部移除，如同队列。两个链表需要维持平衡，如果活跃链表变得过多而超过了非活跃链表，那么活跃链表的头页面将被移除到非活跃链表中，以便能再被回收。双链表策略解决了传统LRU算法中对仅一次访问的窘境。这种双链表方式页称为LRU/2.更普遍的是n个链表，故称为LRU/n。

现在知道页缓存如何构建（通过读和写），如何在写时被同步以及就数据如何被回收来容纳新数据（通过双链表）。

### 16.2Linux页高速缓存

从名字可以看出，页高速缓存的是内存页面，缓存中的页来自对正规文件、块设备文件和内存映射文件的读写。如此一来，页高速缓存就包含了最近被访问过的文件的数据块。在执行一个IO操作前，内核会检查数据是否已经在页高速缓存中了，如果所需要的数据的确在高速缓存中，那么内核可以从内存中迅速地返回需要的页，而不再需要从相对较慢的磁盘上读取数据。接下来的章节中，将剖析具体的数据结构以及内核如何使用他们管理缓存。

#### 16.2.1address_space对象

在页高速缓存中的页可能包含了多个不连续的物理磁盘块。也正是由于页面中映射的磁盘块不一定连续，所以在页高速缓存中检查特定数据是否已经被缓存是件颇为困难的工作。因为不能用设备名称和块号来做页高速缓存中的数据的索引，不然这将是最简单的定位方法。Linux页高速缓存的目标是缓存任何基于页的对象，这包含各种类型的文件和各种类型的内存映射。

虽然Linux页高速缓存可以通过扩展inode结构体支持页IO操作，但这种做法会将页高速缓存局限于文件。为了维持页高速缓存的**普遍性**，Linux页高速缓存使用了一个新对象管理缓存项和页IO操作。这个对象就是**address_space**结构体。该结构体是第十五章介绍的虚拟地址vm_area_struct的**物理地址对等体**。（前者表示内存段的虚拟空间，后者表示一个文件的物理内存段，逻辑性是有的）。（我前两遍完全没有这个概念）

当一个文件可以被10个vm_area_struct结构体标识，那么这个**文件只能有一个address_space数据结构**，也就是文件可以有多个虚拟地址，但是物理内存只有一份。映射关系，多个虚拟地址对应一个线性地址，多个线性地址对应一个物理地址。

<img src="C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1689143303929.png" alt="1689143303929" style="zoom:25%;" />

其中，i_mmap字段是一个优先搜索树，他的搜索范围包含了在address_space中所有共享的与私有（共享和私有都是虚拟空间的概念无非是一对一，多对一的情况）的映射页面（之前搜索是进程地址空间中搜索内存段，这里是在物理内存段中搜索页面）。优先搜索树是一种巧妙地将堆和radix树结合的快速检索树。这个搜索树目的是为了**搜索**与物理页面有关的**映射**。

adress space页总数由nrpages字段描述。

address_space结构往往会和某些内核对象关联。毕竟描述的是文件，会和一个索引节点inode关联，此时host域就会指向该索引节点；如果和swapper关联，host被指为NULL。



#### 16.2.2address_space操作

a_ops域指向地址空间对象中的操作函数表，这与VFS对象及其操作表关系类似 ，操作函数表定义在文件<linux/fs.h>中。由address_space_operations结构体表示

这些方法指针指向那些为指定缓存对象实现的页I/O操作。每个后备存储都通过自己的address_space_operation描述与页高速缓存交互。这些方法提供了管理页高速缓存的各种行为，包括最常用的读页到缓存、更新缓存数据。

这里面readpage()和writepage()这两个方法最为重要。接下来就看看一个页面的读操作会包含哪些步骤。首先Linux内核试图在页高速缓存中找到需要的数据；find_get_page()方法负责完成这个检查动作。一个address_space对象和一个偏移量会传给find_get_page()方法，用于在页高速缓存中搜索需要的数据。

```c
page = find_get_page(mapping,index);//前者是传入的物理内存段对象，后者是偏移量，目的是找到物理内存段中的该页
```

这里mapping是指定的地址空间，index是文件中的指定位置，以页面为单位。如果搜索的页并没在高速缓存中，find_get_page()将会返回一个NULL，并且内核将分配一个新页面，然后将之前搜索的页加入到页高速缓存中。

最后，需要的数据从磁盘被读入，再被加入页高速缓存，然后返回给用户。

内核会在晚些时候通过writepage()方法把页写出，对特定文件的写操作比较复杂，他的代码在文件mm/filemap.c中。

#### 16.2.3基树

因为在任何页IO操作前内核都要检查页是否已经在页高速缓存中了，所以这种频繁进行的检查必须迅速、高效否则搜索和检查页高速缓存的开销可能抵消页高速缓存带来的好处。

正如之前看到的，页高速缓存通过两个参数address_space对象加上一个**偏移量**进行搜索。每个address_space对象都有唯一的基树（radix tee）他保存在page_tree结构体中。基树是一个二叉树，**只要指定了文件偏移量，就可以在基树中迅速检索到希望的页**。页高速缓存的搜索函数find_get_page()要调用函数radix_tree_lookup()，该函数会在该函数会在指定基树中搜索指定页面。

基树核心代码的通用形式可以在文件lib/radix-tree.c中找到。另外，要想使用基树，需要包含头文件<linux/radix-tree.h>

#### 16.2.4以前的页散列表

在2.6版本以前，内核页高速缓存并不是通过基树检索，而是通过一个维护了系统中所有页的全局散列表进行检索。对于给定的一个键值，该散列表会返回一个双向链表的入口对应于这个所给定的值。如果需要的页存储在缓存中，那么链表中的一项就会与其对应。否则，页就不在页面高速缓存中，散列函数返回NULL。

全局散列表主要存在四个问题：

- 由于使用单个的全局锁保护散列表，所以即使在中等规模的机器中，锁的争用情况也会相当严重，造成性能受损。
- 由于散列表需要包含所有页高速缓存中的页，可是搜索需要的只是和当前文件相关的哪些页，所以散列表包含的页面相比搜索需要的页面要大得多。
- 如果散列搜索失败（也就是给定的页不在页高速缓存中），执行速度比希望的要慢得多，这时因为检索必须遍历指定散列键值对应的整个链表
- 散列表比其他方法会消耗更多的内存。

2.6版本内核中引入基于基树的页高速缓存来解决问题。

### 16.3缓冲区高速缓存

独立的磁盘块通过块IO缓冲也要被存入页高速缓存。一个缓冲是一个物理磁盘块在内存里的表示。缓冲的作用就是映射内存中的页面到磁盘块，这样一来页高速缓存在块IO操作时也减少了磁盘访问，因为它缓存磁盘块和减少块IO操作。这个缓存通常称为缓冲区高速缓存，作为页高速缓存的一部分。

块IO操作一次操作一个单独的磁盘块。普遍的块IO操作是读写i节点。内核提供bread函数实现从磁盘读一个块的底层操作。通过缓存，磁盘块映射到他们相关的内存页，并缓存到页高速缓存中。

缓冲和页高速缓存并非统一的。一个磁盘块可以同时存于两个缓存中，这导致必须同步操作两个缓存中的数据。如今只有一个磁盘缓存就是页高速缓存，内核仍然需要在内存中使用缓冲来表示磁盘块，缓冲是用页映射块的，所以缓冲也在页高速缓存中。

### 16.4flusher线程

#### 16.4.1膝上型计算机模式

将硬盘转动的机械行为最小化，允许硬盘尽可能长时间地停滞，以此延长电池供电时间。

#### 16.4.2历史上的bdflush、kupdated和pdflush

#### 16.4.3避免拥塞的方法：使用多线程

---

Linux系统中，内存对匿名页和文件缓存用了四条链表进行组织，回收过程主要是针对这四条链表进行扫描：（这里的匿名页是与文件无关的映射页，文件缓存就是文件相关的页）

1. 首先扫描每个zone上的cgroup组；（整个内存只有三个zone结构体，DMA、noraml、highmem）每个zone上将进程进行分组
2. 然后以cgroup的内存为单位进行page链表的扫描；（page页面由链表管理）
3. 内核会先扫描anon的活跃链表。
4. 然后扫描非活跃链表。（这是收回页缓存的两个链表）

覆盖技术是小内存运行大程序，与内存存储管理方式无关，是分区方式。

----



### 16.5小结

本章中看到了Linux的页高速缓存和页回写。了解了内核如何通过页缓存执行页IO操作以及这些页高速缓存可以利用减少磁盘IO，从而极大地提升系统的性能。我们讨论了通过称为回写缓存的进程维护在缓存中的更新页面，具体做法是标记内存中的页面为脏，（改良版时钟算法就是给页面增加了一个修改位也称为脏位）然后找时机延迟写到磁盘中。Flusher内核线程将负责处理这些最终的页回写操作。

通过最近几章的学习，你应该已经对内存与文件系统有了深刻认识，那么接下来我们将进入模块专题 ，去学习Linux的设备驱动以及内核如何被模块化、在运行时插入和删除。

## 第十八章调试

调试工作艰难是内核级开发区别于用户级开发的一个显著特点。相比于用户级开发，内核调试的难度大得多，可能一个错误立刻就能让系统崩溃。因为本章其实对我帮助不大，所以直接快速通过了。

### 18.1准备开始

精确地重现一个bug的时候已经成功一半了。

### 18.2内核中的bug

### 18.3通过打印来调试

printk()和printf功能几乎相同。

#### 18.3.1健壮性

printk可以在任何情况下工作，除了启动初期。

#### 18.3.2日志等级

printk可以指定日志级别。

#### 18.3.3记录缓冲区

内核消息都被保存在一个LOG_BUF_LEN大小的环形队列中。

#### 18.3.4syslogd和klogd

在标准的Linux系统上，用户空间的守护进程klogd从记录缓冲区中获取内核信息，再通过syslogd守护进程将他们保存在系统日志文件中。

#### 18.3.5从printf到printk的转换

### 18.4oops

内核发布oops来告知用户有不幸发生这是最常用的方式。

#### 18.4.1ksymoops

前面列举的oops可以说是一个经过解码的oops。

#### 18.4.2kallsyms

这已经无须使用了

### 18.5内核调试配置选项

### 18.6引发bug并打印信息

### 18.7神奇的系统请求键

### 18.8内核调试器的传奇

内核源代码树中没有一个调试器。

#### 18.8.1gdb

使用标准的GNU调试器gdb对正在运行的内核进行查看。

#### 18.8.2kgdb

kgdb是一个补丁，他可以让我们在远端主机上通过串口利用gdb的所有功能对内核进行调试。

### 18.9探测系统

#### 18.9.1用UID作为选择条件

可以利用用户id作为选择条件来实现这种功能。

#### 18.9.2使用条件变量

如果代码与进程无关，或者希望有一个针对所有情况都能使用的机制来控制某个特性，可以使用条件变量。

#### 18.9.3使用统计量

#### 18.9.4重复频率限制

### 18.10用二分法找出变更

### 18.11使用Git进行二分搜索

### 18.12当所有的努力都失败时，社区

### 18.13小结

本章讨论了内核的调试，调试过程其实是一种寻求实现与目标偏差的行为，考察了几种技术：从内核内置的调试架构到调试程序，从记录日志到git二分法查找，因为调试Linux内核困难重重，非调试用户程序能比，因此不要止步。





## 第十七章设备与模块

讨论四种内核成分：

- 设备类型：为了统一普通设备的操作所采用的分类。
- 模块：Linux内核中用于按需加载和卸载驱动模块的机制
- 内核对象：内核数据结构中支持面向对象的简单操作，支持维护对象之间的父子关系
- sysfs：系统中设备树的一个文件系统，现在知道了这是文件系统，但是最终是在VFS层体现出来的。

### 17.1设备类型

- 块设备
- 字符设备
- 网络设备

块设备缩写blkdev，可寻址的，寻址以块为单位，块大小随设备不同而不同，支持重定位操作，对数据的随机访问。块设备是通过称为块设备节点的特殊文件访问的。通常被挂载为文件系统。

字符设备通常缩写为cdev，它是不可寻址的，仅提供数据的流式访问，通过字符设备节点的特殊文件进行访问的。

网络设备也叫做以太网设备来称呼，提供对网络的访问，这是通过一个物理适配器和一种特定的协议进行的，打破了所有东西都是文件的设计原则，通过套接字API特殊接口访问的。

Linux提供了一些杂项设备，miscdev，实际上就是简化的字符设备

并不是所有设备驱动都表示物理设备。有些是虚拟的，称为伪设备。比如内核随机数发生器、空设备/dev/null这个是文件黑洞、零设备/dev/zero访问、满设备/dev/full访问，还有内存设备/dev/mem访问的。

### 17.2模块

Linux是单内核的操作系统，整个系统内核运行在一个单独的保护域中，但是Linux内核是模块化组成的，允许内核在运行时动态地向其中插入或从中删除代码。这些代码被一并组合在一个单独的二进制镜像中，也就是说所谓的可装载内核模块中，或简称为模块。支持模块的好处是基本内核镜像可以尽可能地小，因为可选的功能和驱动程序可以利用模块形式再提供。而且当热插拔新设备时，可通过命令载入新的驱动程序。

#### 17.2.1hello world

与开发的大多数内核核心子系统不同，模块开发类似于编写一个新的应用系统，因为至少在模块文件中具有入口和出口点。

hello_init()函数是模块的入口点，通过module_init()例程注册到系统中，在内核装载时被调用。调用module_init()实际上不是真正的函数调用，而是宏调用。

因为init函数函数通常不会被外部函数直接调用，所以不必导出该函数，可标记为static类型。

init函数会返回一个int类型数值，如果初始化顺利完成，返回值为零。

init函数还会注册资源、初始化硬件、分配数据结构等。如果被静态编译进内核映像中，其init函数将存放在内核映像中，并在内核启动时运行。

exit函数是模块的出口函数，它由module_exit()例程注册到系统。

如果上书文件被静态编译进内核了，退出函数不会被包含，而且永远不会被调用。

MODULE_LICENSE()宏用于指定模块的版权，如果载入非GPL模块到系统内存中，内核会被标记为被污染标识，之前用过。

最后还要说明MODULE_AUTHOR()宏和MODULE_DESCRIPTION()宏指定代码的作者和模块的简要描述。

#### 17.2.2构建模块

##### 1.放在内核源代码树中

代码正确置于内核中。

下一步要清楚代码应放在何处，设备驱动程序存放在内核源码树根目录下/drivers的子目录中，在其内部，设备驱动文件被进一步的按类别组织起来。

比如一个字符设备添加到/drivers/char/下，然后在该目录下的Makefile添加一行，

```makefile
obj -m += fishing/
```

这行编译指令告诉模块构建系统，在编译模块时需要进入fishing/子目录中。

最后要在fishing/目录下的makfile中添加一行指令：

```makefile
obj -m += fishing.o
```

到时候就会被编译成fishing.ko了。

最后一个注意事项是，在构建文件时，可能需要额外的编译标记，如果这样，只需在makefile中添加指令。

##### 2.放在内核代码外

如果喜欢脱离内核代码树来维护和构建你的模块，把自己作为一个圈外人，那需要做的就是在Makefile文件中添加一行指令：

```makefile
obj-m :=fishing.o
```

同样是把fishing.c编译成.ko文件。

模块在内核内核在内核外构建的最大区别在于构建过程。当模块在内核源代码树外围时，你必须告诉make如何找到内核源代码文件和基础Makefile文件的。

#### 17.2.3安装模块

编译后的模块将被装入到目录/lib/modules/version/kernel下，在kernel/目录下的每一个目录都对应着内核源码树中的模块位置。

#### 17.2.4产生模块依赖性

Linux模块之间存在依赖性，这个在出厂系统中就体现出来了，depmod不需要这个。当载入钓鱼模块时，鱼饵模块会被自动载入。这里需要的依赖信息必须事先生成。多数Linux发布版都能自动产生这些依赖关系信息，而且在每次启动时更新。若想产生内核依赖关系的信息，运行命令depmod

如果是只为新模块生成依赖信息，而不是生成所有的依赖关系，

```
depmod -A
```

模块依赖关系信息存放在/lib/modules/version/modules.dep文件中

#### 17.2.5载入模块

载入模块最简单的方法是通过insmod命令，这是个功能很有限的命令，能做的就是请求呢诶和载入指定的模块。insmod程序不执行任何依赖性分析或进一步的错误检查。

```
insmod module.ko
rmmod module
```

这两个命令是很简单，但是他们一点都不智能，先进工具modprobe提供了模块依赖性分析、错误只能检查等等功能。为了在内核中插入模块

```
modprobe module
```

modprobe命令不但会加载指定的模块，而且会自动加载任何它所依赖的有关模块所以他是当无依赖的时候要使用insmod

从内核中卸载模块

```
modprobe -r modules
```

参数modules指定一个或多个需要卸载的模块。与rmmod命令不同，modprobe也会卸载给定模块所依赖的相关模块。前提是这些模块没有被使用

#### 17.2.6管理配置选项

只要设置了CONFIG_FISHING_POLE配置选项，钓鱼模块就将被自动编译。

由于2.6内核中新引入了kbuild系统，因此加入一个新配置选项。这里的kbuild就是图形化配置界面。

#### 17.2.7模块参数

Linux提供了这样一个简单框架，允许驱动程序声明参数，从而用户可以在系统启动或者模块装载时再指定参数值，这些参数对于驱动程序属于全局变量。值得一提的是模块参数同时也将出现在sysfs文件系统中，无论生成模块参数，还是管理模块参数的方式都变得灵活多样了。

定义一个模块参数可通过宏module_param()完成：

```c
module_param(name,type,perm);
```

参数name是用户可见的参数名，也是模块中存放模块参数的变量名。参数type则存放了参数的类型，他可以是byte、short、ushort、int、long、ulong、charp、bool或invbool。

最后一个参数perm指定了模块在sysfs文件系统下对应文件的权限，该值可以是八进制的格式，比如0644；或是S_Ifoo的定义形式。如果该值是零则表示禁止所有的sysfs项。

这个函数没用过，也是在驱动模块中使用的么。

#### 17.2.8导出符号表

模块被载入后，就会被动态地连接到内核。注意，他与用户空间中的动态链接库类似，只有当被显式导出后的外部函数，才可以被动态库调用。在内核中，导出内核函数需要使用特殊的指令：

```c
EXPORT_SYMBOL()/EXPORT_SYMBOL_GPL()//这个使用过，是在内核中导出函数给另一个内核文件使用的，因为每个进程都只停留在自己的命名空间中，所以这就导致了无关的内核进程无法访问
```

导出的内核函数可以被模块调用，而未导出的函数模块则无法被调用。模块代码的链接和调用规则相比核心内核镜像中的 代码而言，要更加严格。核心代码在内核中可以调用任意非静态接口，因为所有的核心源代码被连接成同一个镜像。

导出的内核符号表被看做导出的内核接口，甚至称为内核API。导出符号相当简单，在声明函数后，紧跟上EXPORT_SYMBOL()指令就搞定了。

假定被导出的函数同时也定义在一个可访问头文件中，那么现在任何模块都可以访问它。有些开发者希望自己的接口仅仅对GPL兼容的模块可见，内核连接器使用MODULE_LICENSE()宏可满足这个要求，如果你希望先前的函数仅仅对标记为GPL协议的模块可见，那么需要用：

```c
EXPORT_SYMBOL_GPL();
```

如果代码被配置成模块，那就必须确保当他被编译成模块时，他所用的全部接口已被导出，否则就会产生连接错误。

### 17.3设备模型

2.6内核增加了一个新特性：统一设备模型(device model)。设备模型提供了一个独立的机制专门来表示设备，并描述其在系统中的拓扑结构，这里指的是设备总线吧，从而使得系统具有以下优点：

- 代码重复最小化
- 提供诸如引用计数这样的机制
- 可以列举系统中所有的设备，观察他们的状态，并且查看他们连接的总线。
- 可以将系统中的全部设备结构以树的形式完整、有效地展现出来，包括所有的总线和内部连接。设备树?
- 可以将设备和其对应的驱动联系起来，probe？
- 可以将设备按照类型加以归类，比如分类为输入设备，而无需理解物理设备的拓扑结构。
- 可以沿设备树的叶子向其根的方向以此遍历，以保证能以正确顺序关闭各设备的电源。

最后一点是实现设备模型的最初动机。若想在内核中实现智能的电源管理，就需要建立表示系统中设备拓扑关系的树结构。当在树上端的设备关闭电源时，内核必须首先关闭该设备节点以下的设备电源。比如内核需要先关闭一个USB鼠标，才能关闭usb控制器。这个电源关闭顺序之前并没有提到过。

#### 17.3.1kobject

设备模型的核心部分就是kobject(kernel object)，它由struct kobject结构体表示，定义在文件<linux/kobject.h>中。kobject类似于面向对象语言中的类。

提供了诸如引用计数、名称和父指针等字段，可以创建对象的层次结构。

name指针指向此kobject的名称。

parent指针指向kobject的父对象。这样一俩，kobject就会在内核中构造一个对象层次结构，并且可以将多个对象间的关系表现出来。这便是sysfs的真正面目：一个用户空间的文件系统，用来表示内核中kobject对象的层次结构。

sd指针指向sysfs_dirent结构体，该结构体在sysfs中表示的就是这个kobject。从sysfs文件系统内部看，这个结构体是表示kobject的一个inode结构体。

kref提供引用计数。ktype和kset结构体对kobject对象进行描述和分类，在下面的内容中将详细介绍他们。

kobject通常是嵌入其他结构中的，其单独意义其实不大，相反，哪些更为重要的结构体，才真正需要用到kobject结构。

```c
struct cdev{
	struct kobject kobj;
    ...
}
```

当kobject被嵌入到其他结构中时，该结构便拥有了kobject提供的标准功能。更重要的是，**嵌入kobject的结构体可以称为对象层次架构中的一部分**。比如cdev结构体就可通过其父指针cdev->kobj.parent和链表cdev->kobj.entry插入到对象层次结构中。

#### 17.3.2ktype

kobject对象被关联到一种特殊的类型，即ktype(kernel object type 的缩写)。ktype由kobj_type结构体表示，定义于头文件<linux/kobject.h>中：

```c
struct kobj_type {
	void (*release)(struct kobject *)
	const struct sysfs_ops *sysfs_ops;
	struct attribute **default_attrs;
}
```

ktype的存在是为了描述一族kobject所具有的普遍特性。如此一来，不再需要每个kobject都分别定义自己的特性，而是将这些普遍的特性在ktype结构中一次定义，然后所有同类的kobject都能共享一样的特性。

release指针指向在kobject引用计数减至零时要被调用的析构函数。然后所有同类的kobject都能共享一样的特性。

sysfs_ops变量指向sysfs_ops结构体。该结构体描述了sysfs文件读写时的特性。

最后，default_attrs指向一个attribute结构体数组。这些结构体定义了该kobject相关的默认属性。

#### 17.3.3kset

kset是kobject对象的集合体。把他看成一个容器，可将所有相关的kobject对象，比如全部的块设备置于同一位置。kset可把kobject集中到一个集合中，而ktype描述相关类型kobject所共有的特性，重要区别在于：**具体相同ktype的kobject可以被分组到不同的kset**。也就是说，在Linux内核中，只有少数一些的ktype，却有多个kset。

kobject的kset指针指向相应的kset集合。kset集合由kset结构体表示，定义在头文件<linux/kobject.h>中：

这个结构中，list连接集合中的所有的kobject对象，list_lock是保护这个链表中元素的自旋锁，kobj指向的kobject对象代表了该集合的基类。uevent_ops指向一个结构体，用于处理集合中kobject对象的热插拔操作。uevent就是用户事件的缩写，提供了用户空间热插拔信息信息通信的机制。

#### 17.3.4kobject、ktype、kset的相互关系

上文反复讨论的这一组结构体很容易令人混淆，并不是他们数量多，也不是复杂，而是他们内部相互交织，要了解kobject很难只讨论其中一个结构体而不涉及其他相关结构。

然而在这些结构的相互作用下，会更有助你深刻理解他们之间的关系。

这里最重要的家伙是kobject，她由struct kobject表示。kobject为我们引入了诸如引用计数、父子关系和对象名称等基本对象道具，并且是以一个统一的方式提供这些功能，不过kobject本身意义并不大，**通常情况下他需要被嵌入到其他数据结构中，让那些包含他的结构具有了kobject特性**。

kobject与一个特别的ktype对象关联，ktype由struct kobj_type结构体表示，在kobject中ktype字段指向该对象。**ktype定义了一些kobject相关的默认特性：析构行为，sysfs行为以及别的一些默认属性**。

kobject又归入了称为kset的集合，kset集合由struct kset结构体表示，提供了两个功能。

**第一是其中 嵌入的kobject作为kobject组的基类。第二是kset将相关的kobject集合在一起。**在sysfs中，这些相关的kobject将以独立的目录出现在文件系统中。这些相关的目录，也许是给定目录的所有子目录，可能处于同一个kset。

<img src="C:\Users\MACHENIKE\AppData\Roaming\Typora\typora-user-images\1671538549239.png" alt="1671538549239" style="zoom:25%;" />

确实kset将kobj集合在一起了。

#### 17.3.5管理和操作kobject

当了解了kobject的内部基本细节后，看下管理和操作他的外部接口。多数时候，驱动程序开发者不比直接处理kobject，因为kobject是被嵌入到一些特殊类型结构体中的，而且会有相关的设备驱动程序在幕后管理，但kobject可以出现在驱动代码中。

使用kobject的第一步需要先来声明和初始化，kobject通过函数kobject_init进行初始化，该函数定义在文件<linux/kobject.h>中

```c
void kobject_init(struct kobject *kobj,struct kobj_type *ktype)
```

该函数的第一个参数往往会在kobject所在的上层结构体初始化时完成。如果kobject未被清空，那么只需要调用memset()即可。

```c
memset(kobj,0,sizeof(*kobj));
```

在清零后，就可以安全的初始化parent和kset字段。

```c
struct kobject *kobj;
kobj = kmalloc(sizeof(*kobj),GFP_KERNEL);
if(!kobj)
    return -ENOMEM;
memset(kobj,0,sizeof(*kobj));
kobj->kset = my_kset;
kobject_init(kobj,my_ktype);
```

看得出来申请内存，清零，初始化kset和ktype。

这多部操作也可以由kobject_create()来自动处理，它返回一个新分配的kobject;

大多数情况下，应该调用kobject_create()创建kobject，或者是调用相关的辅助函数，而不是直接操作这个结构体。

#### 17.3.6引用计数

kobject的主要功能之一就是为我们提供一个统一的引用计数系统。初始化后，kobject的引用计数设置为1。只要引用计数不为零，那么该对象就会继续保留在内存中，也可以说是被定住了，任何包含对象引用的代码都要先增加该对象的引用计数。这不就是智能指针么

##### 1.递增和递减引用计数

增加一个引用计数

```c
struct kobject *kobject_get(struct kobject *kobj);
void kobject_put(struct kobject *kobj);
```

如果对应的kobject的引用计数减少到零，则与该kobject关联的ktype中的析构函数被调用。

##### 2.kref

深入到引用计数系统的内部看，会发现kobject的引用计数是通过kref结构体实现的，该结构体定义在头文件<linux/kref.h>中。其中唯一的字段就是用来存放引用计数的原子变量。使用kref之前，必须先通过kref_init()函数来初始化它。这个函数就是将原子变量置1。

要获得对kref的引用，需要调用kref_get()函数，这个函数声明在<linux/kref.h>中。

增加引用计数值，减少对kref的引用，调用函数kref_put()该函数使得引用计数减1，如果减到零，调用release函数。

提供的release()函数不能简单地采用kfree()，他必须是一个仅接收一个kref结构体作为参数的特有函数，而且还没有返回值。

开发者现在不必在内核代码中实现自己的引用计数。对开发者肉眼，最好的方法是利用 kref类型。

### 17.4sysfs

sysfs文件系统是一个**处于内存中的虚拟文件系统**，他为我们提供了kobject对象层次结构的视图。帮助用户能以简单文件系统的方式来观察系统中各种设备的拓扑结构。（sysfs原来最后还是一个虚拟文件系统），借助属性对像，kobject可以用导出文件的方式，将内核变量提供给用户读取或写入。

虽然设备模型的初衷是为了方便电源管理而提出的一种 设备拓扑结构 ，但是sysfs是颇为意外的收获。为了方便调试，设备模型的开发者决定将设备结构树导出为一个文件系统。这个举措是很名字的，首先sysfs代替了先前处于/proc下的设备相关文件；另外它为系统 对象提供了一个很有效的视图。实际上 sysfs初期被称为driverfs，它早于kobject出现。最终sysfs使得我们认识到一个全新的对象模型非常有利于系统，于是kobject出现了。如今都拥有sysfs系统，而且都挂载在sys目录下。

**sysfs的诀窍是把kobject对象域目录项紧密相连接**，这点是通过kobject对象中的dentry字段实现的。通过连接kobject到指定的目录项上，无疑方便地将kobject映射到该目录上。从此把kobject导出形成文件系统就变得如同在内存中构建目录项一样简单。

sysfs的根目录下包含了至少是个目录，block是块设备，bus提供系统总线，class包含以高层功能逻辑组织起来的系统设备视图。dev目录是已注册设备节点视图。devices目录是系统中设备拓扑结构视图直接映射出了内核中设备结构体的组织层次。firmware目录包含了底层子系统。fs目录是已注册文件系统的视图。kernel目录包含内核配置项和状态信息。module目录则包含系统已加载模块的信息。power目录包含系统范围的电源管理数据。

最重要的目录是devices，该目录将设备模型导出到用户空间。目录结构就是系统中实际的设备拓扑。其他目录中的很多数据都是将devices目录下的数据加以转换加工而得。

#### 17.4.1sysfs中添加和删除kobject

仅仅初始化kobject是不能自动将其导出到sysfs中的，想要把kobject导入sysfs，你需要用到函数kobject_add()

kobject在sysfs中的位置取决于kobject在对象层次结构中的位置。如果kobject的父指针被设置，那么在sysfs中kobject将被映射到其父目录下的子目录；如果parent没有设置，那么kobject将被映射为kset->kobj中的子目录。如果给定 的kobject中parent或kset字段都没有被设置，那么就认为kobject没有父对象，所以就被映射成sysfs下的根据目录。所以在调用kobject_add函数前parent或kset字段应该进行适当的设置。不管怎么样，sysfs中 代表kobject的目录名字是由fmt指定的。

辅助函数kobject_create_and_add把kobject_create和kobject_add函数的工作都完成了。

从sysfs中删除一个kobject对应的文件目录，需要使用函数kobject_del()

#### 17.4.2向sysfs中添加文件

我们已经看到kobject被映射为文件目录，而且所有的对象层次结构都映射成sys下的目录结构。

##### 1.默认属性

默认的文件集合 是通过kobject和kset中的ktype字段提供的。因此所有具有相同类型的kobject在他们对应的sysfs目录下都拥有相同的默认文件集合。kobj_type字段含有一个字段是default_attrs，他是一个attribute结构体数组。这些属性负责将内核数据映射成sysfs中的文件。

其中名称字段提供了该属性的名称，最终出现在sysfs中的文件名就是它。owner字段在存在所属模块的情况下指向其所属的module结构体。如果一个模块没有该属性，那么该字段为NULL。mode字段类型表示sysfs中该文件的权限。对于只读属性而言，如果是所有人都可读它，那么该字段被设为S_IRUGO

虽然default_attr列出了默认的属性，sysfs_ops字段则描述了如何使用他们。sysfs_ops字段指向了一个定义于文件<linux/sysfs.h>的同名的结构体

这里所谓的描述了如何使用他们，其实就是当你访问读取写入这些属性的时候实际上调用的是什么函数。

##### 2.创建新属性

通常来讲，由kobject相关ktype所提供的默认属性 是充足的，事实上因为所有具有相关ktype的kobject，在本质上区别不大的情况下，都应是相互接近的。但有些时候会遇到特殊的kobject实例。遇到了再说

##### 3.删除新属性

通过函数sysfs_remove_file()完成。

##### 4.sysfs约定

当前sysfs文件系统代替了以前需要有ioctl()和procfs文件系统完成的工作。但是为了保证sysfs干净和直观，开发者必须遵从以下约定。

首先sysfs属性应该保证每个文件指导处一个值。

其次，在sysfs中要以一个清晰的层次组织数据。

最后记住sysfs提供内核到用户空间的服务。

这些简单的约定保证sysfs可为用户空间提供丰富和直观的接口。

#### 17.4.3内核事件层

内核事件层实现了内核到用户的消息通知系统，就是建立在kobject基础上的。系统确实需要一种机制来帮助将事件传出内核输送到用户空间。

早期的事件层没有采用kobject和sysfs，现在就比较理性了，内核事件层把时间模拟为信号，从明确的kobject对象发出，所以每个事件源都是sysfs路径。在内核中，我们认为事件都是从幕后的kobject对象产生 的。

每个事件都被赋予了一个动词或动作字符串表示信号，该字符串会以被修改过，或者未挂载等词语来描述事件。

最后每个事件都有一个可选的负载。相比传递任意一个表示负载的字符串到用户空间而言，内核事件层使用sysfs属性代表负载。

从呢诶和实现来讲，内核事件由内核空间传递到用户空间需要经过 netlink。netlink是一个传送网络信息的多点传送套接字。不看了

使用kobject和属性不但有利于很好的实现基于sysfs的事件，同时也有利于创建新kobject对象和属性来表示新对象和数据，他们尚未出现在sysfs中。

这两个函数分别定义和声明于文件lib/kobject_uevent.c和文件<linux/kobject.h>中。

### 17.5小结

本章中，我们考察的内核功能涉及到设备 驱动 的实现和设备数的管理，包括模块、kobject和sysfs。这些功能对于设备驱动程序的开发者来说至关重要，因为这能够让他们写出更为模块化、更为高级的驱动 程序。

本章讨论了内核中我们要学习的最后一个子系统。

## 第十九章可移植性

Linux是一个可移植性非常好的操作系统，他广泛支持许多不同体系结构的计算机。可移植性是指代码从一种体系结构移植到另外一种不同的体系结构上的方便程序。都知道Linux是可移植的，因为他已经能够在各种不同的体系结构上运行了。但这种可移植性不是凭空得来的，需要在编写可移植代码的时候就为此付出努力并坚持不懈。现在，这种努力已经开始得到回报了，移植Linux到新的系统上很容易完成。本章将讨论如何编写可移植的代码，编写内核代码和驱动程序时，必须时刻牢记这个问题。

### 19.1可移植操作系统

有些操作系统在设计时把可移植性作为头等大事，尽可能少地涉及与机器相关的代码。汇编代码用的少之又少，为了支持各种不同类型的体系结构，界面和功能在定义时尽最大可能具有普适性和抽象性。这么做最显著的回报就是需要支持新的体系结构时，所需完成的工作相对容易许多。一些移植性非常高而本身又比较简单的操作系统在支持新的体系结构时可能只需要为此体系结构编写几百行专门的代码就行了。问题在于，体系结构相关的一些特性往往无法被支持，也不能对特定的机器进行手动优化。选择这种设计，就是利用代码的性能优化能力换取代码的可移植性。

与之相反，还有一种操作系统完全不顾及可移植性，他们尽最大的可能追求代码的性能表现，尽可能地使用汇编代码，压根就是只为在一种体系结构使用。内核的特性都是围绕硬件提供的特性设计的。因此将其移植到其他体系结构就等于再重新编写一种新的操作系统，而且也不实用。选择这种设计就是用代码的可移植性换取代码的性能优化能力。这样的系统往往比移植性好的系统更难维护。DOS和windows 95就是这种设计方案的最好例证。

Linux在可移植性这方面走的是中间路线。差不多所有的接口和核心代码都是独立于硬件体系结构的C语言代码。但是，在对性能要求很严格的部分，内核的特性会根据不同的硬件体系结构进行调整。

一般来说，暴露在外面的内核接口往往是与硬件体系结构无关的。如果函数的任何部分需要针对特殊的体系结构提供支持的话，这些部分会被安置在独立的函数中，等待调用。每种被支持的体系结构都实现了一个与体系结构相关的函数，而且会链接到内核映射中。

调度程序是一个好例子，用C语言编写，与体系结构无关，可是调度程序需要进行的一些工作，比如切换处理器上下文和切换地址空间等却不得不依靠相应的体系结构完成。于是，内核用C语言编写了函数。

而对于Linux支持的每种体系结构，他们的具体实现函数各不相同。所以，当Linux需要移植到新的体系结构上的时候，只需要重新编写和提供这样的函数就可以了。

与体系结构相关的代码都存放在arch/architecture/目录中，这是Linux所支持的体系结构的简称。比如说Intel x86体系结构对应的简称是x86。

### 19.2Linux移植史

Linux移植到了21种基本体系结构上，但是实际上可以运行他的机器数目要大得多。

### 19.3字长和数据类型

能够由机器一次完成处理的数据称为字，字是指位的整数数目。但人们说某个机器多少位的时候，指的是字长。比如说Intel i7是64位芯片时，字长为64位，也就是8字节。

处理器通用寄存器的大小和他的字长是相同的，一般来说，对于一个体系结构，他各个部件的宽度最少要和它的字长一样大。虽然物理地址空间有时候会比字长小，但虚拟地址空间的大小等于字长。此外C语言定义的long类型总是对等于机器的字长，而int类型有时候会比字长小，比如说Alpha是64位机器，所以他的寄存器、指针和long类型都是64位的，而int类型是32位的。

##### 字、双字以及混合

有些操作系统和处理器不把他们的标准字长称为字，相反，他们用字代表一些固定长度的数据类型，字节byte8位，字word16位，双字double words32位，和四字quad words64位，而实际上该机是32位的。

对于支持的每一种体系结构Linux都要将<asm/types.h>中的BITS_PER_LONG定义为C long类型的长度，也就是系统的字长。

一般来说，Linux对于一种体系结构都会分别实现32位和64位的不同版本。

#### 19.3.1不透明类型

开发者们利用typedef声明一种类型叫做不透明类型，通常在定义一套特别的接口时才会用到他们。

处理不透明类型的原则是：

- 不要假设该类型的长度
- 不要将该类型转换回对应的C标准类型使用。
- 称为一个大小不可知论者。

#### 19.3.2指定数据类型

#### 19.3.3长度明确的类型

#### 19.3.4char型的符号问题

C标准表示char类型可以带符号也可以不带

### 19.4数据对齐

对齐是跟数据块在内存中的位置相关的话题。如果一个变量的内存地址正好是它长度的整数倍，称为自然对齐。

#### 19.4.1避免对齐引发的问题

内核开发在对齐上不用花费心思，编译器会帮我们实现的。但是当使用的指针过多，对数据的访问方式超出编译器的预期时，就会引发问题了。

#### 19.4.2非标准类型的对齐

C数据类型按照下列原则对齐：

- 对于数组，只要按照基本数据类型进行对齐就可以了，随后的所有元素自然能够对齐。
- 对于联合体，包含的长度最大的数据类型能够对齐就可以了。
- 对于结构体，只要结构体中每个元素能够正确地对齐就可以了。

#### 19.4.3结构体填补

为了保证结构体中每一个成员都能自然对齐，结构体要被填补。

### 19.5字节顺序

字节顺序是指在一个字中各个字节的顺序，就是大小端问题。

##### 高位优先和低位优先的历史

这是一个政治问题。对于Linux支持的每一种体系结构，相应的内核都会根据机器使用的字节顺序在他的<asm/byteorder.h>中定义。

最常用的宏命令有：

```c
u32 __cpu_to_be32(u32)
u32 __cpu_to_le32(u32);
u32 __le32_to_cpu(u32);
u32 __be32_to_cpu(u32);
```

### 19.6时间

时间测量是另一个内核概念，随着体系结构甚至内核版本的不同而不同。绝对不要假定时钟中断发生的频率，也就是每秒产生的jiffies数目。

### 19.7页长度

当处理用页管理的内存时，不要假设页的长度。

### 19.8处理器排序

指令排序就是内存屏障问题，要保证正确顺序提交装载和存储指令。

### 19.9SMP/内核抢占、高端内存

- 假设你的代码会在SMP系统上运行，要正确地选择和使用锁。
- 假设你的代码会在支持内核抢占的情况下运行，要正确地选择和使用锁和内核抢占语句。
- 假设你的代码会运行在使用高端内存的系统上，必要时使用kmap()。

### 19.10小结

要想写出可移植性好、简洁、合适的内核代码，要注意以下两点：

- 编码尽量选取最大公因子：假定任何事情都可能发生。
- 编码尽量选取最小公约数：仅仅需要最小的体系结构功能

在内核开发过程中时刻注意编写简洁、可移植的代码是非常重要的。

## 第二十章补丁、开发和社区

Linux的最大优势是他有一个紧密团结了众多使用者和开发者的社区。社区能帮你检查代码，社区中的转接给你提出忠告，社区中的用户能帮你进行测试，用户还能反馈问题，最主要的是，什么样的代码可以加入Linus的官方内核树也是由社区做出的决定的。因此了解系统到底是怎么运作的就显得很重要了。

### 20.1社区

内核邮件列表之家。内核邮件列表是对内核进行发布，讨论、争辩和打口水战的主要战场。在做任何实际的动作之前，新特性会在此处被讨论，新代码的大部分也会在此处张贴。这个列表每天发布的消息超过300条，任何想踏踏实实研究、认认真真开发内核的人都应该订阅它。单单看看这些奇才们使出的一招一式，也能受益匪浅了。

### 20.2Linux编码风格

像所有大型软件项目一样，Linux指定了一套编码风格，编码风格的主要规范伴随着linus一罐的幽默，都记录在内核源代码树的Documentation/CodingStyle中了。

#### 20.2.1缩进

缩进风格每次缩进tab

#### 20.2.2switch语句

Switch语句下属的case标记缩进到和switch对齐。

#### 20.2.3空格

给符号和关键字加空格

#### 20.2.4花括号

左括号紧跟在语句的最后，与语句在相同的一行，有括号重新起一行。

#### 20.2.5每行代码的长度

源代码保证不超过80个字符

#### 20.2.6命名规范

名称中不允许使用骆驼拼写法或者其他混合的大小写字符。匈牙利命名法是不必要的。

#### 20.2.7函数

代码长度不应该超过两屏，局部变量不超过10个。可以使用inline关键字

#### 20.2.8注释

描述代码要做什么和为什么要做，怎么实现是代码本身展示的。

#### 20.2.9typedef

内核开发者强烈反对使用typedef语句，因为掩盖了数据真实的类型

#### 20.2.10多用现成的东西

直接使用内核提供的接口。

#### 20.2.11在源码中减少使用ifdef

#### 20.2.12结构初始化

结构初始化的时候在他的成员前加上结构标识符“.”

#### 20.2.13代码的事后修正

indent工具就能帮你解决。他可以按照指定的方式对源代码进行格式化，默认情况下按照不怎么好看的GNU编码风格格式化代码。

### 20.3管理系统

所有人都是为了从中获得快乐，所有做出卓越贡献的黑客都能在源代码树根目录的CREDITS文件中留名。

### 20.4提交错误报告

### 20.5补丁

对内核的任何修改都是以补丁的形式发布的，其实是GNU diff(1)程序的一种特定格式的输出，该格式的信息能够被patch(1)程序接受。

#### 20.5.1创建补丁

#### 20.5.2用Git创建补丁

#### 20.5.3提交补丁

### 20.6小结

对于黑客而言，渴望是最可贵的品质，本书讲述了Linux内核的主要部分，讨论了接口、数据结构、算法和原理，从实践出发，以内在的视角洞悉内核，既可以满足你的好奇心，也可以帮助你开始学习内核。DMA